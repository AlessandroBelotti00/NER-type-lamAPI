{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f58a58-df3c-4e43-9f78-8a69659f0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SPARQLWrapper\n",
      "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
      "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
      "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: isodate, rdflib, SPARQLWrapper\n",
      "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-7.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "526459a4-54c5-4f69-a417-4492b7392b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from requests import get\n",
    "import re\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a33d9-bf6e-41f1-ae65-ddb6c5f86138",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7cd55-2fba-45fd-98bf-0ed53cd092b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PERS\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'))\n",
    "cta_keys[\"category\"] = df[\"category\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bdb738-bcb1-4703-899e-e97a7c629407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cta_keys)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('./data/GT/Round1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1e33b-8ee7-4397-b56f-0054f3c079f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898fb70-80b4-4116-bce9-a4b5a774917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_file = './data/Dataset/Dataset/Round3_2019/gt/CTA_Round3_gt.csv'\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1] \n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PER\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = category_list\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'))\n",
    "cta_keys[\"category\"] = df[\"category\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26ef88-7dcc-45a7-bba4-b9c68eeb8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cta_keys)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('./data/GT/Round3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55cfb5-1db2-4bde-bb7f-214ec951bf0f",
   "metadata": {},
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba6342f3-0752-4eea-ad86-6a3fb79ae226",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/output_gemma2/output/classified_2T.txt'\n",
    "macroType_mapping = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d3d6979-6ae1-4978-9e98-60d3cf8d9c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Root Classes: 100%|██████████| 540/540 [00:03<00:00, 143.96it/s]\n"
     ]
    }
   ],
   "source": [
    "cta_file = './data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Asynchronous function to get item root class\n",
    "async def get_item_root(session, id_list):\n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        match = re.search(r'Q(\\d+)$', el)\n",
    "        \n",
    "        if match:\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "        \n",
    "            json_data = json.dumps(data)\n",
    "            \n",
    "            # Send POST request asynchronously\n",
    "            async with session.post(url, headers=headers, data=json_data) as response:\n",
    "                try:\n",
    "                    result = await response.json()\n",
    "                    root_class = result[match[0]]['labels']['en']\n",
    "                    return root_class\n",
    "                except:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "# Function to map categories to macroTypes\n",
    "def map_macroTypes(category, macroType_mapping_df):\n",
    "    # Find the macroType corresponding to the category (label)\n",
    "    row = macroType_mapping_df[macroType_mapping_df['label'] == category]\n",
    "    if not row.empty:\n",
    "        return row['class'].values[0]\n",
    "    return 'UNKNOWN'  # In case category does not match any\n",
    "\n",
    "# Function to process the dataframe asynchronously with progress bar\n",
    "async def process_dataframe(macroType_mapping_df):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(cta_file, header=None)\n",
    "    \n",
    "    root_categories = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create a list of tasks to fetch root classes\n",
    "        tasks = []\n",
    "        for urls in df[2]:\n",
    "            tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "            tasks.append(get_item_root(session, tmp))\n",
    "        \n",
    "        # Use tqdm_asyncio to track the progress of tasks\n",
    "        root_categories = await tqdm_asyncio.gather(*tasks, desc=\"Fetching Root Classes\", total=len(tasks))\n",
    "    \n",
    "    # Assign the root categories to the dataframe\n",
    "    df[\"category\"] = root_categories\n",
    "    \n",
    "    # Map the categories to macroTypes and add it as a new column\n",
    "    df['macroTypes'] = df['category'].apply(lambda category: map_macroTypes(category, macroType_mapping_df))\n",
    "     \n",
    "    # Creating the cta_keys structure\n",
    "    cta_keys = pd.DataFrame()\n",
    "    cta_keys[\"key\"] = df[0].astype(str) + \" \" + df[1].astype(str)\n",
    "    cta_keys[\"category\"] = df[\"category\"]\n",
    "    cta_keys[\"macroTypes\"] = df[\"macroTypes\"]\n",
    "    \n",
    "    return cta_keys\n",
    "\n",
    "# Entry point to run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    df = await(process_dataframe(macroType_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc3d88e9-5878-4129-8a13-b04a010fb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('./data/GT/2T_Round4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607aa12-6ed2-4698-80dc-358324afc8be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397df599-0314-4014-9bed-a26ff5d999c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined macroType mapping\n",
    "macroType_mapping = {\n",
    "    'LOC': [\n",
    "        'neighborhood of Brazil', 'provincial park in Saskatchewan', 'airport railway station',\n",
    "        'city of regional significance of Ukraine', 'national wildlife area of Canada',\n",
    "        'California state park', 'city palace', 'municipality of Sweden', 'department of El Salvador',\n",
    "        'township of Illinois', 'village of New York', 'borough of Alaska', 'town in Romania',\n",
    "        'city district of Brazil', 'township of New Jersey', 'county of Missouri', \n",
    "        'county of South Carolina', 'county of Florida', 'county of North Carolina', \n",
    "        'municipality of Latvia', 'town of Virginia', 'township of Missouri', \n",
    "        'town divided by border', 'department of transportation', 'historic county of England',\n",
    "        'natural region of France', 'region of Italy', 'province of Papua New Guinea', \n",
    "        'district of Nepal', 'district of the German Democratic Republic', 'autonomous prefecture',\n",
    "        'township of New Jersey', 'municipality of Guatemala', 'municipality of Switzerland',\n",
    "        'region of Kazakhstan', 'town', 'lake', 'district of Iran', 'commune of Benin',\n",
    "        'hamlet in Alberta', 'municipality in Germany', 'municipality of Puerto Rico', \n",
    "        'county of Oregon', 'administrative territorial entity of Russia', 'national park of Quebec', \n",
    "        'county of Kansas', 'county of Indiana', 'town of Newfoundland and Labrador', \n",
    "        'district of Malawi', 'village in Indonesia', 'region of the Philippines',\n",
    "        'townland', 'rural area', 'village of Poland', 'district of New Zealand', \n",
    "        'village/town/city in Lebanon', 'provincial capital', 'local municipality',\n",
    "        'Commonwealth War Graves Commission maintained memorial', 'roadside station in Japan', 'township of Kansas',\n",
    "        'cemetery cross', 'viaduct', 'space station', 'bullring', 'Lok Sabha constituency',\n",
    "        'flag of a country subdivision', 'mountain pass', 'coffeehouse', 'scale model', \n",
    "        'Maryland state park', 'city block', 'river basin district', 'municipality of Cuba',\n",
    "        'oasis', 'township municipality', 'urban park', 'national park of Brazil', \n",
    "        'national preserve', 'geographical feature', 'village in Belarus', 'city municipality',\n",
    "        'commune of Chile', 'township of China', 'district capital', 'region of Brazil',\n",
    "        'local government area of Queensland', 'city or town in Armenia', 'village of Japan',\n",
    "        'township of North Dakota', 'town of Pennsylvania', 'highly urbanized city', \n",
    "        'neighbourhood of Helsinki', 'urban commune of Morocco', 'historic district',\n",
    "        'municipality of Greece', 'capital city', 'district of Japan', 'community area in Chicago',\n",
    "        'provincial city', 'township of Nebraska', 'region of Madagascar', 'rural municipality of Poland',\n",
    "        'county of California', 'region of France', 'associated commune', 'rural settlement in Russia',\n",
    "        'avenue', 'urban-type settlement in Russia', 'moon of Uranus', 'warehouse', 'city walls',\n",
    "        'location of burial', 'district of Mongolia', 'parish municipality', 'town of Portugal',\n",
    "        'town of Japan', 'city in Colombia', 'region of Peru', 'county of Alabama', \n",
    "        'province of Morocco', 'natural monument', 'capital of prefecture', 'fjord', 'village municipality of Quebec',\n",
    "        'census-designated place in the United States', 'township of Indiana', 'township of Arkansas', \n",
    "        'district municipality', 'district of India', 'village in India', 'municipality of Niger', \n",
    "        'province of Sweden', 'parish of Louisiana', 'urban council of Ukraine', 'central business district',\n",
    "        'island of Portugal', 'lower-tier municipality', 'city of Portugal', 'city of the Philippines',\n",
    "        'community school', 'municipality of the Philippines', 'community development block in India',\n",
    "        'administrative territorial entity of Canada', 'natural park', 'community of Cyprus Republic',\n",
    "        'exclave', 'harbor', 'sconce', 'city in the state of New York', 'city of Indonesia', \n",
    "        'district of Costa Rica', 'county of Kentucky', 'municipality of Japan', 'cultural heritage', \n",
    "        'historical country', 'political territorial entity', 'unitary authority area in England', 'state of Nigeria',\n",
    "        'district of Uganda', 'district of Belarus', 'district of Serbia', 'municipality of Mexico', \n",
    "        'urban municipality in Germany', 'village municipality', 'city in Armenia'\n",
    "    ],\n",
    "    'ORG': [\n",
    "        'bar association', 'film production company', 'film studio', 'heavy metal band',\n",
    "        'auto racing team', 'professional sports team', 'political party in Germany', \n",
    "        'dance', 'research council', 'foundation school', 'commercial organization', \n",
    "        'national badminton team', 'medical association', 'federal electoral district of Germany', \n",
    "        'public library', 'badminton association', 'sports club', 'television series season',\n",
    "        'university press', 'television series episode', 'association football team',\n",
    "        'weekly newspaper', 'cultural center', 'academy', 'university and college sports club', \n",
    "        'programming tool', 'conference paper', 'professional association', 'sports competition',\n",
    "        'carnival', 'federation', 'national library',\n",
    "        'piano sonata', 'film award', 'music museum', 'music venue', 'restaurant chain',\n",
    "        'educational institution', 'medical society', 'academic library', 'government building',\n",
    "        'advocacy group', 'commercial bank', 'university in France', 'association football team season',\n",
    "        'musical release', 'professional sports team', 'multinational corporation', 'television network',\n",
    "        'social media platform', 'music festival', 'research university', 'sports league',\n",
    "        'international organization', 'cultural institution', 'nonprofit organization',\n",
    "        'news agency', 'concert hall', 'Lyceum', 'youth organization', 'music organization', \n",
    "        'political party', 'theatre company', 'football club', 'futsal team', 'diplomatic mission', \n",
    "        'armed forces', 'sports venue', 'association under the French law of 1901', \n",
    "        'military occupation', 'commercial organization', 'historical motorcycle manufacturer',\n",
    "        'cable channel', 'Boston Open Badminton Championships', 'LEN European Aquatics Championships',\n",
    "        'Czech Open', 'Italian Grand Prix', 'Baltic state', 'Christian denomination', 'political party',\n",
    "        'Nobel family', 'music school', 'college', 'restaurant chain', 'film festival', \n",
    "        'nonprofit organization', 'sports club', 'federal agency of Germany', 'sports park', \n",
    "        'broadcaster', 'association football league', 'music genre', 'local government area of Nigeria', \n",
    "        'political organization', 'national trade union center', 'international organization', \n",
    "        'media company', 'government', 'department of Ivory Coast', 'academic journal article',\n",
    "        'sports league', 'fictional organization', 'regional council of Israel', 'academic department',\n",
    "        'sports team', 'amateur sports organization', 'musical ensemble', 'noble family', \n",
    "        'historically black colleges and universities', 'historical organization', 'police station',\n",
    "        'university building', 'group of sculptures', 'transport service itinerary', 'community of Cyprus Republic'\n",
    "    ],\n",
    "    'PERS': [\n",
    "        'family', 'profession', 'married couple', 'female given name',\n",
    "        'Japanese television series', 'fictional detective', 'anime film', 'superhero film character',\n",
    "        'boy band', 'mythological character', 'famous artist', 'author', 'historical figure',\n",
    "        'young stellar object candidate', 'individual time trial', 'Homo sapiens',\n",
    "        'corporate title', 'human biblical figure', 'fictional deity', 'political organization',\n",
    "        'sports discipline', 'art movement', 'musical profession', 'military rank'\n",
    "    ],\n",
    "    'OTHERS': [\n",
    "        'flammable solid', 'globular cluster', 'emakimono', 'air-to-air missile', \n",
    "        'report', 'fragment', 'stream', 'prestressed concrete bridge', 'show cave', \n",
    "        'fictional humanoid species', 'imperative programming language', 'weapon family', \n",
    "        'transcript fusion', 'productivity software', 'photograph album', 'specialty channel', \n",
    "        'chemical compound', 'flying-type Pokémon', 'bug-type Pokémon', 'artificial lake', \n",
    "        'designated intractable/rare disease', 'region', 'urban area', 'suburb of Perth',\n",
    "        'trunk road', 'drama television series', 'supervillain', 'comic book', \n",
    "        'characteristic', 'courage award', 'space program', 'film festival', \n",
    "        'tapestry', 'medication', 'economic sector', 'color', 'metaphor',\n",
    "        'LP record', 'short story collection', 'rock formation', 'explosion', 'bus type', \n",
    "        'letter', 'compilation soundtrack album', 'isotope', 'cartoonist', 'bridge',\n",
    "        'mushroom', 'commercially significant film', 'nonprofit sector', 'political region', \n",
    "        'marathon', 'figure of speech', 'poetry collection', 'historical period', \n",
    "        'fountain', 'glacier', 'religious text', 'set of historical events', 'political figure',\n",
    "        'public park', 'forest', 'city park', 'recreational vehicle', 'pet species', \n",
    "        'rural town', 'historic place', 'historical event', 'historic region', \n",
    "        'religious building', 'current country', 'floral arrangement', 'character type',\n",
    "        'theoretical concept', 'local newspaper', 'acronym', 'neighborhood of Sweden', \n",
    "        'national sporting event', 'social construct', 'floral species', 'national park',\n",
    "        'climate zone', 'emotional concept', 'artistic movement', 'theory', \n",
    "        'mechanical system', 'expression', 'collection', 'statistical population',\n",
    "        'partnership', 'outdoor venue', 'social organization', 'assembly', \n",
    "        'food item', 'fusion cuisine', 'economic classification', 'massive urban area',\n",
    "        'species', 'goddess', 'natural disaster', 'cultural movement', 'fire prevention method',\n",
    "        'historical location', 'classic rock song', 'classical music composition',\n",
    "        'historical region', 'role-playing game', 'physical feature', 'hot air balloon',\n",
    "        'supernatural entity', 'piece of writing', 'documentary film', 'discussion forum',\n",
    "        'individual film', 'software project', 'mountain', 'bicycle category', \n",
    "        'scientific research', 'radio show', 'book genre', 'religious movement', \n",
    "        'common area', 'university publication', 'mountain range', 'urban area', \n",
    "        'dispute', 'artistic period', 'mode of transportation', 'road', 'venue',\n",
    "        'election campaign', 'artwork', 'journal article', 'acquired trait', \n",
    "        'scientific publication', 'form of government', 'literary genre'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed800bf-2919-4904-948c-668e5135ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_file = './data/Dataset/Dataset/Round4_2020/gt/cta.csv'\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Asynchronous function to get item root class\n",
    "async def get_item_root(session, id_list):\n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        match = re.search(r'Q(\\d+)$', el)\n",
    "        \n",
    "        if match:\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "        \n",
    "            json_data = json.dumps(data)\n",
    "            \n",
    "            # Send POST request asynchronously\n",
    "            async with session.post(url, headers=headers, data=json_data) as response:\n",
    "                try:\n",
    "                    result = await response.json()\n",
    "                    root_class = result[match[0]]['labels']['en']\n",
    "                    return root_class\n",
    "                except:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "# Function to map categories to macroTypes\n",
    "def map_macroTypes(category):\n",
    "    for macroType, categories in macroType_mapping.items():\n",
    "        if category in categories:\n",
    "            return macroType\n",
    "\n",
    "    return 'UNKNOWN'  # In case category does not match any\n",
    "\n",
    "# Function to process the dataframe asynchronously with progress bar\n",
    "async def process_dataframe():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(cta_file, header=None)\n",
    "    \n",
    "    root_categories = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create a list of tasks to fetch root classes\n",
    "        tasks = []\n",
    "        for urls in df[2]:\n",
    "            tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "            tasks.append(get_item_root(session, tmp))\n",
    "        \n",
    "        # Use tqdm_asyncio to track the progress of tasks\n",
    "        root_categories = await tqdm_asyncio.gather(*tasks, desc=\"Fetching Root Classes\", total=len(tasks))\n",
    "    \n",
    "    # Assign the root categories to the dataframe\n",
    "    df[\"category\"] = root_categories\n",
    "    \n",
    "    # Map the categories to macroTypes and add it as a new column\n",
    "    df['macroTypes'] = df['category'].apply(map_macroTypes)\n",
    "    \n",
    "    # Creating the cta_keys structure\n",
    "    cta_keys = pd.DataFrame()\n",
    "    cta_keys[\"key\"] = df[0].astype(str) + \" \" + df[1].astype(str)\n",
    "    cta_keys[\"category\"] = df[\"category\"]\n",
    "    cta_keys[\"macroTypes\"] = df[\"macroTypes\"]\n",
    "    \n",
    "    return cta_keys\n",
    "\n",
    "# Entry point to run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    df = await(process_dataframe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd1f3b-a94f-471d-8a1e-e852e71cc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef5496-1337-4198-9e87-b22dd11ee75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13094021-4e23-446f-8255-ef864a5bd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "macro_type_counts = Counter(df['macroTypes'])\n",
    "\n",
    "# Print the counts\n",
    "print(macro_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416bfa05-4cb0-40e9-9161-7c2cc5ce4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cta_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88414a-cbf3-4e81-81f7-bffbdc31aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cta_file = './data/Dataset/Dataset/Round4_2020/gt/cta.csv'\n",
    "\n",
    "def get_item_root(id_list):    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    id_to_root_class = {}\n",
    "    \n",
    "    for el in tqdm(id_list, desc=\"Processing IDs\"):\n",
    "        if el not in id_to_root_class:\n",
    "            query = f\"\"\"\n",
    "            SELECT ?instanceClass ?instanceClassLabel WHERE {{\n",
    "              wd:{el} wdt:P31 ?instanceClass .\n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\" }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Set the query and request JSON response\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            #time.sleep(0.5)\n",
    "            \n",
    "            try:\n",
    "                results = sparql.query().convert()\n",
    "                if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "                    inst_item = int(results[\"results\"][\"bindings\"][0]['instanceClassLabel']['value'][1:])\n",
    "                    if inst_item in geolocation_subclass:\n",
    "                        id_to_root_class[el] = \"LOC\"\n",
    "                    elif inst_item in organization_subclass:\n",
    "                        id_to_root_class[el] = \"ORG\"\n",
    "                    elif inst_item == 5 or el == \"Q5\":\n",
    "                        id_to_root_class[el] = \"PERS\"\n",
    "                    else:\n",
    "                        id_to_root_class[el] = \"OTHERS\"\n",
    "                else:\n",
    "                    id_to_root_class[el] = \"None\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {el}: {e}\")\n",
    "                time.sleep(0.5)\n",
    "                id_to_root_class[el] = \"None\"          \n",
    "    \n",
    "    return id_to_root_class\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "ids = [url.split('/')[-1] for url in df[2]]\n",
    "\n",
    "root_classes = get_item_root(ids)\n",
    "\n",
    "# Map root classes to categories\n",
    "root_categories = []\n",
    "for el in ids:\n",
    "    try:\n",
    "        root_categories.append(root_classes[el])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'))\n",
    "cta_keys[\"category\"] = df[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d5c10-f877-43d7-8978-3a65b026682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cta_keys)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('./data/GT/Round4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29e636-94b2-4d2a-831b-d7e422bdca4f",
   "metadata": {},
   "source": [
    "# HardTableR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53b2b51f-8258-4acd-84ce-2fbdbe44d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/output_gemma2/output/classified_HardTableR2.txt'\n",
    "macroType_mapping = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d0ad9ad0-f6b8-4ea4-a6b9-c9c83ed5f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Root Classes: 100%|██████████| 2191/2191 [00:03<00:00, 572.17it/s]\n"
     ]
    }
   ],
   "source": [
    "cta_file = './data/Dataset/Dataset/HardTablesR2/gt/cta.csv'\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Asynchronous function to get item root class\n",
    "async def get_item_root(session, id_list):\n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        match = re.search(r'Q(\\d+)$', el)\n",
    "        \n",
    "        if match:\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "        \n",
    "            json_data = json.dumps(data)\n",
    "            \n",
    "            # Send POST request asynchronously\n",
    "            async with session.post(url, headers=headers, data=json_data) as response:\n",
    "                try:\n",
    "                    result = await response.json()\n",
    "                    root_class = result[match[0]]['labels']['en']\n",
    "                    return root_class\n",
    "                except:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "# Function to map categories to macroTypes\n",
    "def map_macroTypes(category, macroType_mapping_df):\n",
    "    # Find the macroType corresponding to the category (label)\n",
    "    row = macroType_mapping_df[macroType_mapping_df['label'] == category]\n",
    "    if not row.empty:\n",
    "        return row['class'].values[0]\n",
    "    return 'UNKNOWN'  # In case category does not match any\n",
    "\n",
    "# Function to process the dataframe asynchronously with progress bar\n",
    "async def process_dataframe(macroType_mapping_df):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(cta_file, header=None)\n",
    "    \n",
    "    root_categories = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create a list of tasks to fetch root classes\n",
    "        tasks = []\n",
    "        for urls in df[2]:\n",
    "            tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "            tasks.append(get_item_root(session, tmp))\n",
    "        \n",
    "        # Use tqdm_asyncio to track the progress of tasks\n",
    "        root_categories = await tqdm_asyncio.gather(*tasks, desc=\"Fetching Root Classes\", total=len(tasks))\n",
    "    \n",
    "    # Assign the root categories to the dataframe\n",
    "    df[\"category\"] = root_categories\n",
    "    \n",
    "    # Map the categories to macroTypes and add it as a new column\n",
    "    df['macroTypes'] = df['category'].apply(lambda category: map_macroTypes(category, macroType_mapping_df))\n",
    "     \n",
    "    # Creating the cta_keys structure\n",
    "    cta_keys = pd.DataFrame()\n",
    "    cta_keys[\"key\"] = df[0].astype(str) + \" \" + df[1].astype(str)\n",
    "    cta_keys[\"category\"] = df[\"category\"]\n",
    "    cta_keys[\"macroTypes\"] = df[\"macroTypes\"]\n",
    "    \n",
    "    return cta_keys\n",
    "\n",
    "# Entry point to run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    df = await(process_dataframe(macroType_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c371837-0e96-49ba-bd86-c1add50496eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'OTHER': 1054, 'LOCATION': 710, 'ORGANIZATION': 189, 'UNKNOWN': 123, 'PERSON': 115})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "macro_type_counts = Counter(df['macroTypes'])\n",
    "\n",
    "# Print the counts\n",
    "print(macro_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f39f5a79-17a1-4b6a-81a6-0294153973bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('./data/GT/HTR2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d821f-62cd-4a40-8723-77a6e77773ff",
   "metadata": {},
   "source": [
    "# HardTableR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27616d60-95e9-4bf5-96fc-3fffa9330719",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/output_gemma2/output/classified_HardTableR3.txt'\n",
    "macroType_mapping = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b65595d0-c562-40bd-bc2a-784359088247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Root Classes: 100%|██████████| 7207/7207 [00:10<00:00, 662.70it/s] \n"
     ]
    }
   ],
   "source": [
    "cta_file = './data/Dataset/Dataset/HardTablesR3/gt/cta.csv'\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Asynchronous function to get item root class\n",
    "async def get_item_root(session, id_list):\n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        match = re.search(r'Q(\\d+)$', el)\n",
    "        \n",
    "        if match:\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "        \n",
    "            json_data = json.dumps(data)\n",
    "            \n",
    "            # Send POST request asynchronously\n",
    "            async with session.post(url, headers=headers, data=json_data) as response:\n",
    "                try:\n",
    "                    result = await response.json()\n",
    "                    root_class = result[match[0]]['labels']['en']\n",
    "                    return root_class\n",
    "                except:\n",
    "                    pass\n",
    "    return None\n",
    "\n",
    "# Function to map categories to macroTypes\n",
    "def map_macroTypes(category, macroType_mapping_df):\n",
    "    # Find the macroType corresponding to the category (label)\n",
    "    row = macroType_mapping_df[macroType_mapping_df['label'] == category]\n",
    "    if not row.empty:\n",
    "        return row['class'].values[0]\n",
    "    return 'UNKNOWN'  # In case category does not match any\n",
    "\n",
    "# Function to process the dataframe asynchronously with progress bar\n",
    "async def process_dataframe(macroType_mapping_df):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(cta_file, header=None)\n",
    "    \n",
    "    root_categories = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Create a list of tasks to fetch root classes\n",
    "        tasks = []\n",
    "        for urls in df[2]:\n",
    "            tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "            tasks.append(get_item_root(session, tmp))\n",
    "        \n",
    "        # Use tqdm_asyncio to track the progress of tasks\n",
    "        root_categories = await tqdm_asyncio.gather(*tasks, desc=\"Fetching Root Classes\", total=len(tasks))\n",
    "    \n",
    "    # Assign the root categories to the dataframe\n",
    "    df[\"category\"] = root_categories\n",
    "    \n",
    "    # Map the categories to macroTypes and add it as a new column\n",
    "    df['macroTypes'] = df['category'].apply(lambda category: map_macroTypes(category, macroType_mapping_df))\n",
    "     \n",
    "    # Creating the cta_keys structure\n",
    "    cta_keys = pd.DataFrame()\n",
    "    cta_keys[\"key\"] = df[0].astype(str) + \" \" + df[1].astype(str)\n",
    "    cta_keys[\"category\"] = df[\"category\"]\n",
    "    cta_keys[\"macroTypes\"] = df[\"macroTypes\"]\n",
    "    \n",
    "    return cta_keys\n",
    "\n",
    "# Entry point to run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    df = await(process_dataframe(macroType_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "82baa837-eae7-41b7-9bd2-47fecab519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv('./data/GT/HTR3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ebf60-a2c4-4630-a1a6-9dee4bf467f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Promt LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bcc533-896b-468c-9a49-12994abcee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_promt = \"classify each of the items according to the following macro types ORG, LOC, PERS, OTHERS. an item can belong only to one class with the following python dictionary format CSV with columns (label_name, macroTypes) es. Docklands Light Railway station, LOC\"\n",
    "\n",
    "# Read a txt file where items are separated by commas\n",
    "file_path = './data/GT/types_list/Round4.txt'\n",
    "\n",
    "# Read the file and split the line into a list of items\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the entire content as a single line\n",
    "    content = file.read()\n",
    "    \n",
    "    # Remove single quotes from the content\n",
    "    content = content.replace(\"'\", \"\")\n",
    "    \n",
    "    # Split the content by commas and strip any extra spaces\n",
    "    items = [item.strip() for item in content.split(',')]\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(0, len(items), batch_size):\n",
    "    batch = items[i:i + batch_size]\n",
    "    prompt = f\"{string_prompt} {'; '.join(batch)}.\"\n",
    "    \n",
    "    ## .... LLM\n",
    "    # format output = [(), (), ...]\n",
    "    \n",
    "    data = data + output\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['type', 'macroType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add63e23-1649-42b5-b3f3-6eee7f2c239e",
   "metadata": {},
   "source": [
    "# LLM macroType evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "feff7381-5cf7-46d6-8b7f-8503e489b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Class Labels:\n"
     ]
    }
   ],
   "source": [
    "gemma_file_path = './data/output_gemma2/output/classified_Round4.txt'\n",
    "mistral_file_path = './data/output_mistral/output2/classified_Round4.txt'\n",
    "\n",
    "gemma = pd.read_csv(gemma_file_path)\n",
    "mistral = pd.read_csv(mistral_file_path)\n",
    "\n",
    "# Assume the column containing entities is named 'entity'; adjust as necessary\n",
    "merged_df = pd.merge(gemma, mistral, on='label', suffixes=('_gemma', '_mistral'))\n",
    "\n",
    "# Step 2: Find where classes are different\n",
    "differences = merged_df[merged_df['class_gemma'] != merged_df['class_mistral'].str.split().str[0]]\n",
    "\n",
    "# Display the differences\n",
    "print(\"\\nDifferences in Class Labels:\")\n",
    "(differences)\n",
    "differences.to_csv('differences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79305b4c-e93e-4112-84f5-56e4a3599087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Class Labels:\n"
     ]
    }
   ],
   "source": [
    "gemma_file_path = './data/output_gemma2/output/classified_2T.txt'\n",
    "mistral_file_path = './data/output_mistral/output2/classified_2T.txt'\n",
    "\n",
    "gemma = pd.read_csv(gemma_file_path)\n",
    "mistral = pd.read_csv(mistral_file_path)\n",
    "\n",
    "# Assume the column containing entities is named 'entity'; adjust as necessary\n",
    "merged_df = pd.merge(gemma, mistral, on='label', suffixes=('_gemma', '_mistral'))\n",
    "\n",
    "# Step 2: Find where classes are different\n",
    "differences = merged_df[merged_df['class_gemma'] != merged_df['class_mistral'].str.split().str[0]]\n",
    "\n",
    "# Display the differences\n",
    "print(\"\\nDifferences in Class Labels:\")\n",
    "(differences)\n",
    "differences.to_csv('./data/output_gemma2/differences_2T.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0af29ce0-506c-4f9c-a0cb-7d052ae23d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Class Labels:\n"
     ]
    }
   ],
   "source": [
    "gemma_file_path = './data/output_gemma2/output/classified_HardTableR2.txt'\n",
    "mistral_file_path = './data/output_mistral/output2/classified_HardTableR2.txt'\n",
    "\n",
    "gemma = pd.read_csv(gemma_file_path)\n",
    "mistral = pd.read_csv(mistral_file_path)\n",
    "\n",
    "# Assume the column containing entities is named 'entity'; adjust as necessary\n",
    "merged_df = pd.merge(gemma, mistral, on='label', suffixes=('_gemma', '_mistral'))\n",
    "\n",
    "# Step 2: Find where classes are different\n",
    "differences = merged_df[merged_df['class_gemma'] != merged_df['class_mistral'].str.split().str[0]]\n",
    "\n",
    "# Display the differences\n",
    "print(\"\\nDifferences in Class Labels:\")\n",
    "(differences)\n",
    "differences.to_csv('./data/output_gemma2/differences_HardTableR2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a148c158-a533-4905-97d4-cba40244da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Differences in Class Labels:\n"
     ]
    }
   ],
   "source": [
    "gemma_file_path = './data/output_gemma2/output/classified_HardTableR3.txt'\n",
    "mistral_file_path = './data/output_mistral/output2/classified_HardTableR3.txt'\n",
    "\n",
    "gemma = pd.read_csv(gemma_file_path)\n",
    "mistral = pd.read_csv(mistral_file_path)\n",
    "\n",
    "# Assume the column containing entities is named 'entity'; adjust as necessary\n",
    "merged_df = pd.merge(gemma, mistral, on='label', suffixes=('_gemma', '_mistral'))\n",
    "\n",
    "# Step 2: Find where classes are different\n",
    "differences = merged_df[merged_df['class_gemma'] != merged_df['class_mistral'].str.split().str[0]]\n",
    "\n",
    "# Display the differences\n",
    "print(\"\\nDifferences in Class Labels:\")\n",
    "(differences)\n",
    "differences.to_csv('./data/output_gemma2/differences_HardTableR3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
