{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937bad8e-c05a-48f7-bee4-e06aef31f1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a517463-bba5-43d5-a083-bc18ab0726a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.asyncio import tqdm\n",
    "import re\n",
    "import json\n",
    "import backoff  # Ensure this import is included for the decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe0f1d7-a613-4432-9cd9-66abc23b2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping definition\n",
    "\n",
    "PERS = 0\n",
    "LOC = 1\n",
    "ORG = 2\n",
    "OTHERS = 3\n",
    "none = 4\n",
    "\n",
    "entity_to_id = {\n",
    "    'PERS': PERS,\n",
    "    'LOC': LOC,\n",
    "    'ORG': ORG,\n",
    "    'OTHERS': OTHERS,\n",
    "    None: none\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b1c92-c95f-4907-894e-9eda00d2dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/datasets_alligator_completi'\n",
    "\n",
    "# Initialize an empty set to hold all unique ids\n",
    "all_unique_ids = set()\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000  # Adjust this value as needed\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [filename for filename in os.listdir(directory) if filename.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over all files in the directory with a progress bar\n",
    "for filename in tqdm(csv_files, desc=\"Processing files\"):\n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    # Initialize a set for unique ids in the current file\n",
    "    unique_ids_in_file = set()\n",
    "    \n",
    "    # Read the DataFrame in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Get the unique ids from the current chunk\n",
    "        unique_ids = set(chunk['id'])\n",
    "        \n",
    "        # Update the set of unique ids in the current file\n",
    "        unique_ids_in_file.update(unique_ids)\n",
    "        \n",
    "        # Update the set of all unique ids\n",
    "        all_unique_ids.update(unique_ids)\n",
    "    \n",
    "    # Print the number of unique ids in the current file and total accumulated unique ids\n",
    "    print(f\"File: {filename}, Unique IDs in File: {len(unique_ids_in_file)}, Total Unique IDs: {len(all_unique_ids)}\")\n",
    "\n",
    "# Print the final total number of unique ids across all files\n",
    "print(f\"Final Total unique IDs across all files: {len(all_unique_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28f6f0-2f69-4dcc-a2fd-b5e6e222a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels'\n",
    "params = {\n",
    "    'lang': 'en',\n",
    "    'token': 'lamapi_demo_2023'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "id_to_ner = {}\n",
    "\n",
    "# Semaphore to limit concurrent requests\n",
    "semaphore = asyncio.Semaphore(70)\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch_with_backoff(session, url, payload, id):\n",
    "    async with semaphore:\n",
    "        async with session.post(url, params=params, headers=headers, json=payload) as response:\n",
    "            return id, await response.json()\n",
    "\n",
    "async def process_chunk(session, chunk):\n",
    "    tasks = []\n",
    "    for unique_id in chunk:\n",
    "        payload = {\n",
    "            \"json\": [\n",
    "                f\"{unique_id}\"\n",
    "            ]\n",
    "        }\n",
    "        tasks.append(fetch_with_backoff(session, url, payload, unique_id))\n",
    "    \n",
    "    responses = []\n",
    "    for result in asyncio.as_completed(tasks):\n",
    "        response = await result\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "async def main():\n",
    "\n",
    "    all_unique_ids_list = list(all_unique_ids)\n",
    "    \n",
    "    # Chunk size\n",
    "    chunk_size = 10000\n",
    "\n",
    "    # Split IDs into chunks\n",
    "    chunks = [all_unique_ids_list[i:i + chunk_size] for i in range(0, len(all_unique_ids_list), chunk_size)]\n",
    "\n",
    "    progress_bar = tqdm(total=len(chunks), desc=\"Processing Chunks\")\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            responses = await process_chunk(session, chunk)\n",
    "            for id, response in responses:\n",
    "                try:\n",
    "                    id_to_ner[id] = response[id]['NERtype']\n",
    "                except:\n",
    "                    id_to_ner[id] = None                  \n",
    "\n",
    "        \n",
    "        # Save dictionary to a JSON file after each chunk\n",
    "        with open('id_to_ner.json', 'w') as json_file:\n",
    "            json.dump(id_to_ner, json_file)\n",
    "        \n",
    "        progress_bar.update(1)  # Update progress bar\n",
    "    \n",
    "    progress_bar.close()  # Close progress bar once done\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94bbbe6-3e86-4ee5-89cf-d7d8331f619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./id_to_ner.json\"\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    id_to_ner = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e1500-d15d-4714-9f81-cc04ed2fa1e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833f5a6b-bc99-4861-b5a4-4454d1d10d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 21.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PERS\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb2d285-3048-4cb7-a502-9828d569a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79274/79274 [00:11<00:00, 6710.90it/s] \n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/Round1_T2D.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/Round1_T2D_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f779644-057b-43bd-9035-9f5008aef22d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ab04ed-7c19-4866-b485-0fe82ddf6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2161/2161 [06:34<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round3_2019/gt/CTA_Round3_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1]            \n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PERS\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = category_list\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6c8f61-a4f2-49d9-ad41-bf2cd79ce49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3818812 [00:41<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|          | 10000/3818812 [00:01<10:20, 6139.06it/s]\u001b[A\n",
      "  1%|          | 20000/3818812 [00:08<30:37, 2067.11it/s]\u001b[A\n",
      "  1%|          | 30000/3818812 [00:14<31:58, 1975.27it/s]\u001b[A\n",
      "  1%|          | 40000/3818812 [00:17<27:40, 2276.23it/s]\u001b[A\n",
      "  1%|▏         | 50000/3818812 [00:20<23:53, 2628.20it/s]\u001b[A\n",
      "  2%|▏         | 60000/3818812 [00:23<22:50, 2742.60it/s]\u001b[A\n",
      "  2%|▏         | 70000/3818812 [00:26<20:59, 2976.72it/s]\u001b[A\n",
      "  2%|▏         | 80000/3818812 [00:29<20:03, 3107.21it/s]\u001b[A\n",
      "  2%|▏         | 90000/3818812 [00:31<18:26, 3370.31it/s]\u001b[A\n",
      "  3%|▎         | 100000/3818812 [00:33<17:10, 3609.31it/s]\u001b[A\n",
      "  3%|▎         | 110000/3818812 [00:35<15:12, 4066.57it/s]\u001b[A\n",
      "  3%|▎         | 120000/3818812 [00:37<14:29, 4254.90it/s]\u001b[A\n",
      "  3%|▎         | 130000/3818812 [00:40<14:18, 4298.99it/s]\u001b[A\n",
      "  4%|▎         | 140000/3818812 [00:42<14:01, 4372.80it/s]\u001b[A\n",
      "  4%|▍         | 150000/3818812 [00:44<14:00, 4363.30it/s]\u001b[A\n",
      "  4%|▍         | 160000/3818812 [00:46<12:34, 4847.94it/s]\u001b[A\n",
      "  4%|▍         | 170000/3818812 [00:47<11:55, 5097.86it/s]\u001b[A\n",
      "  5%|▍         | 180000/3818812 [00:49<12:09, 4988.87it/s]\u001b[A\n",
      "  5%|▍         | 190000/3818812 [00:51<11:28, 5271.68it/s]\u001b[A\n",
      "  5%|▌         | 200000/3818812 [00:53<11:15, 5359.36it/s]\u001b[A\n",
      "  5%|▌         | 210000/3818812 [00:55<11:07, 5406.51it/s]\u001b[A\n",
      "  6%|▌         | 220000/3818812 [00:56<10:31, 5698.31it/s]\u001b[A\n",
      "  6%|▌         | 230000/3818812 [00:58<09:50, 6076.32it/s]\u001b[A\n",
      "  6%|▋         | 240000/3818812 [00:59<09:55, 6011.47it/s]\u001b[A\n",
      "  7%|▋         | 250000/3818812 [01:01<09:21, 6357.89it/s]\u001b[A\n",
      "  7%|▋         | 260000/3818812 [01:02<08:35, 6907.01it/s]\u001b[A\n",
      "  7%|▋         | 270000/3818812 [01:03<08:25, 7016.75it/s]\u001b[A\n",
      "  7%|▋         | 280000/3818812 [01:05<09:00, 6552.08it/s]\u001b[A\n",
      "  8%|▊         | 290000/3818812 [01:06<08:42, 6754.68it/s]\u001b[A\n",
      "  8%|▊         | 300000/3818812 [01:08<09:00, 6511.77it/s]\u001b[A\n",
      "  8%|▊         | 310000/3818812 [01:10<08:55, 6547.17it/s]\u001b[A\n",
      "  8%|▊         | 320000/3818812 [01:11<08:48, 6624.62it/s]\u001b[A\n",
      "  9%|▊         | 330000/3818812 [01:13<09:21, 6218.26it/s]\u001b[A\n",
      "  9%|▉         | 340000/3818812 [01:14<09:06, 6370.50it/s]\u001b[A\n",
      "  9%|▉         | 350000/3818812 [01:16<09:42, 5954.97it/s]\u001b[A\n",
      "  9%|▉         | 360000/3818812 [01:18<09:28, 6084.52it/s]\u001b[A\n",
      " 10%|▉         | 370000/3818812 [01:19<08:50, 6502.12it/s]\u001b[A\n",
      " 10%|▉         | 380000/3818812 [01:20<08:35, 6666.42it/s]\u001b[A\n",
      " 10%|█         | 390000/3818812 [01:22<07:59, 7155.71it/s]\u001b[A\n",
      " 10%|█         | 400000/3818812 [01:23<07:30, 7588.87it/s]\u001b[A\n",
      " 11%|█         | 410000/3818812 [01:24<07:19, 7751.93it/s]\u001b[A\n",
      " 11%|█         | 420000/3818812 [01:25<06:58, 8116.69it/s]\u001b[A\n",
      " 11%|█▏        | 430000/3818812 [01:26<06:45, 8361.06it/s]\u001b[A\n",
      " 12%|█▏        | 440000/3818812 [01:27<06:32, 8597.74it/s]\u001b[A\n",
      " 12%|█▏        | 450000/3818812 [01:28<06:27, 8686.83it/s]\u001b[A\n",
      " 12%|█▏        | 460000/3818812 [01:30<06:35, 8500.02it/s]\u001b[A\n",
      " 12%|█▏        | 470000/3818812 [01:31<06:51, 8130.34it/s]\u001b[A\n",
      " 13%|█▎        | 480000/3818812 [01:32<06:37, 8408.26it/s]\u001b[A\n",
      " 13%|█▎        | 490000/3818812 [01:33<06:40, 8316.44it/s]\u001b[A\n",
      " 13%|█▎        | 500000/3818812 [01:34<06:21, 8691.08it/s]\u001b[A\n",
      " 13%|█▎        | 510000/3818812 [01:36<06:18, 8730.45it/s]\u001b[A\n",
      " 14%|█▎        | 520000/3818812 [01:37<06:23, 8610.70it/s]\u001b[A\n",
      " 14%|█▍        | 530000/3818812 [01:38<06:18, 8698.58it/s]\u001b[A\n",
      " 14%|█▍        | 540000/3818812 [01:39<06:02, 9039.05it/s]\u001b[A\n",
      " 14%|█▍        | 550000/3818812 [01:40<06:03, 8980.78it/s]\u001b[A\n",
      " 15%|█▍        | 560000/3818812 [01:41<05:56, 9137.21it/s]\u001b[A\n",
      " 15%|█▍        | 570000/3818812 [01:42<05:40, 9546.28it/s]\u001b[A\n",
      " 15%|█▌        | 580000/3818812 [01:43<05:42, 9462.06it/s]\u001b[A\n",
      " 15%|█▌        | 590000/3818812 [01:44<05:44, 9373.19it/s]\u001b[A\n",
      " 16%|█▌        | 600000/3818812 [01:45<05:42, 9400.27it/s]\u001b[A\n",
      " 16%|█▌        | 610000/3818812 [01:46<05:38, 9471.38it/s]\u001b[A\n",
      " 16%|█▌        | 620000/3818812 [01:47<05:28, 9738.12it/s]\u001b[A\n",
      " 16%|█▋        | 630000/3818812 [01:48<05:43, 9275.78it/s]\u001b[A\n",
      " 17%|█▋        | 640000/3818812 [01:50<05:51, 9055.06it/s]\u001b[A\n",
      " 17%|█▋        | 650000/3818812 [01:51<06:22, 8283.20it/s]\u001b[A\n",
      " 17%|█▋        | 660000/3818812 [01:52<06:05, 8630.69it/s]\u001b[A\n",
      " 18%|█▊        | 670000/3818812 [01:53<05:57, 8800.16it/s]\u001b[A\n",
      " 18%|█▊        | 680000/3818812 [01:54<05:50, 8952.34it/s]\u001b[A\n",
      " 18%|█▊        | 690000/3818812 [01:55<05:40, 9187.12it/s]\u001b[A\n",
      " 18%|█▊        | 700000/3818812 [01:56<05:42, 9102.40it/s]\u001b[A\n",
      " 19%|█▊        | 710000/3818812 [01:57<05:30, 9410.72it/s]\u001b[A\n",
      " 19%|█▉        | 720000/3818812 [01:58<05:32, 9316.16it/s]\u001b[A\n",
      " 19%|█▉        | 730000/3818812 [02:00<05:33, 9254.64it/s]\u001b[A\n",
      " 19%|█▉        | 740000/3818812 [02:01<05:27, 9395.60it/s]\u001b[A\n",
      " 20%|█▉        | 750000/3818812 [02:02<05:28, 9335.32it/s]\u001b[A\n",
      " 20%|█▉        | 760000/3818812 [02:03<05:26, 9376.20it/s]\u001b[A\n",
      " 20%|██        | 770000/3818812 [02:04<05:21, 9470.55it/s]\u001b[A\n",
      " 20%|██        | 780000/3818812 [02:05<05:32, 9149.12it/s]\u001b[A\n",
      " 21%|██        | 790000/3818812 [02:06<05:23, 9376.26it/s]\u001b[A\n",
      " 21%|██        | 800000/3818812 [02:07<05:18, 9467.51it/s]\u001b[A\n",
      " 21%|██        | 810000/3818812 [02:08<05:17, 9465.59it/s]\u001b[A\n",
      " 21%|██▏       | 820000/3818812 [02:09<05:10, 9670.97it/s]\u001b[A\n",
      " 22%|██▏       | 830000/3818812 [02:10<05:12, 9556.25it/s]\u001b[A\n",
      " 22%|██▏       | 840000/3818812 [02:11<05:16, 9410.66it/s]\u001b[A\n",
      " 22%|██▏       | 850000/3818812 [02:12<05:14, 9453.08it/s]\u001b[A\n",
      " 23%|██▎       | 860000/3818812 [02:13<05:21, 9208.83it/s]\u001b[A\n",
      " 23%|██▎       | 870000/3818812 [02:14<05:11, 9481.19it/s]\u001b[A\n",
      " 23%|██▎       | 880000/3818812 [02:15<05:05, 9621.85it/s]\u001b[A\n",
      " 23%|██▎       | 890000/3818812 [02:16<05:12, 9367.34it/s]\u001b[A\n",
      " 24%|██▎       | 900000/3818812 [02:18<05:27, 8915.35it/s]\u001b[A\n",
      " 24%|██▍       | 910000/3818812 [02:19<05:15, 9211.75it/s]\u001b[A\n",
      " 24%|██▍       | 920000/3818812 [02:20<05:09, 9356.21it/s]\u001b[A\n",
      " 24%|██▍       | 930000/3818812 [02:21<05:02, 9548.92it/s]\u001b[A\n",
      " 25%|██▍       | 940000/3818812 [02:22<05:00, 9568.71it/s]\u001b[A\n",
      " 25%|██▍       | 950000/3818812 [02:23<05:06, 9352.70it/s]\u001b[A\n",
      " 25%|██▌       | 960000/3818812 [02:24<05:07, 9286.58it/s]\u001b[A\n",
      " 25%|██▌       | 970000/3818812 [02:25<05:04, 9363.86it/s]\u001b[A\n",
      " 26%|██▌       | 980000/3818812 [02:26<04:50, 9760.57it/s]\u001b[A\n",
      " 26%|██▌       | 990000/3818812 [02:27<04:43, 9993.80it/s]\u001b[A\n",
      " 26%|██▌       | 1000000/3818812 [02:28<04:46, 9833.14it/s]\u001b[A\n",
      " 26%|██▋       | 1010000/3818812 [02:29<04:40, 10030.12it/s]\u001b[A\n",
      " 27%|██▋       | 1020000/3818812 [02:30<04:48, 9711.83it/s] \u001b[A\n",
      " 27%|██▋       | 1030000/3818812 [02:31<04:45, 9774.78it/s]\u001b[A\n",
      " 27%|██▋       | 1040000/3818812 [02:32<04:40, 9902.55it/s]\u001b[A\n",
      " 27%|██▋       | 1050000/3818812 [02:33<04:47, 9633.49it/s]\u001b[A\n",
      " 28%|██▊       | 1060000/3818812 [02:34<04:40, 9831.00it/s]\u001b[A\n",
      " 28%|██▊       | 1070000/3818812 [02:35<04:32, 10079.93it/s]\u001b[A\n",
      " 28%|██▊       | 1080000/3818812 [02:36<04:31, 10100.50it/s]\u001b[A\n",
      " 29%|██▊       | 1090000/3818812 [02:37<04:30, 10092.25it/s]\u001b[A\n",
      " 29%|██▉       | 1100000/3818812 [02:38<04:40, 9690.35it/s] \u001b[A\n",
      " 29%|██▉       | 1110000/3818812 [02:39<04:42, 9572.20it/s]\u001b[A\n",
      " 29%|██▉       | 1120000/3818812 [02:40<04:38, 9706.94it/s]\u001b[A\n",
      " 30%|██▉       | 1130000/3818812 [02:41<04:35, 9771.30it/s]\u001b[A\n",
      " 30%|██▉       | 1140000/3818812 [02:42<04:44, 9419.65it/s]\u001b[A\n",
      " 30%|███       | 1150000/3818812 [02:43<04:48, 9245.79it/s]\u001b[A\n",
      " 30%|███       | 1160000/3818812 [02:44<04:38, 9556.71it/s]\u001b[A\n",
      " 31%|███       | 1170000/3818812 [02:45<04:33, 9672.38it/s]\u001b[A\n",
      " 31%|███       | 1180000/3818812 [02:46<04:28, 9825.16it/s]\u001b[A\n",
      " 31%|███       | 1190000/3818812 [02:48<04:32, 9638.70it/s]\u001b[A\n",
      " 31%|███▏      | 1200000/3818812 [02:49<04:42, 9274.78it/s]\u001b[A\n",
      " 32%|███▏      | 1210000/3818812 [02:50<04:34, 9516.97it/s]\u001b[A\n",
      " 32%|███▏      | 1220000/3818812 [02:51<04:30, 9604.69it/s]\u001b[A\n",
      " 32%|███▏      | 1230000/3818812 [02:52<04:31, 9527.42it/s]\u001b[A\n",
      " 32%|███▏      | 1240000/3818812 [02:53<04:30, 9520.74it/s]\u001b[A\n",
      " 33%|███▎      | 1250000/3818812 [02:54<04:29, 9520.22it/s]\u001b[A\n",
      " 33%|███▎      | 1260000/3818812 [02:55<04:27, 9578.93it/s]\u001b[A\n",
      " 33%|███▎      | 1270000/3818812 [02:56<04:21, 9738.25it/s]\u001b[A\n",
      " 34%|███▎      | 1280000/3818812 [02:57<04:17, 9859.50it/s]\u001b[A\n",
      " 34%|███▍      | 1290000/3818812 [02:58<04:19, 9750.53it/s]\u001b[A\n",
      " 34%|███▍      | 1300000/3818812 [02:59<04:15, 9861.77it/s]\u001b[A\n",
      " 34%|███▍      | 1310000/3818812 [03:00<04:20, 9614.05it/s]\u001b[A\n",
      " 35%|███▍      | 1320000/3818812 [03:01<04:19, 9647.03it/s]\u001b[A\n",
      " 35%|███▍      | 1330000/3818812 [03:02<04:18, 9633.55it/s]\u001b[A\n",
      " 35%|███▌      | 1340000/3818812 [03:03<04:21, 9477.62it/s]\u001b[A\n",
      " 35%|███▌      | 1350000/3818812 [03:04<04:28, 9194.66it/s]\u001b[A\n",
      " 36%|███▌      | 1360000/3818812 [03:05<04:21, 9412.10it/s]\u001b[A\n",
      " 36%|███▌      | 1370000/3818812 [03:06<04:13, 9671.85it/s]\u001b[A\n",
      " 36%|███▌      | 1380000/3818812 [03:07<04:14, 9581.50it/s]\u001b[A\n",
      " 36%|███▋      | 1390000/3818812 [03:08<04:17, 9420.22it/s]\u001b[A\n",
      " 37%|███▋      | 1400000/3818812 [03:10<04:27, 9032.51it/s]\u001b[A\n",
      " 37%|███▋      | 1410000/3818812 [03:11<04:14, 9459.98it/s]\u001b[A\n",
      " 37%|███▋      | 1420000/3818812 [03:12<04:08, 9666.83it/s]\u001b[A\n",
      " 37%|███▋      | 1430000/3818812 [03:13<04:16, 9324.29it/s]\u001b[A\n",
      " 38%|███▊      | 1440000/3818812 [03:14<04:24, 8984.78it/s]\u001b[A\n",
      " 38%|███▊      | 1450000/3818812 [03:15<04:24, 8961.06it/s]\u001b[A\n",
      " 38%|███▊      | 1460000/3818812 [03:16<04:18, 9110.93it/s]\u001b[A\n",
      " 38%|███▊      | 1470000/3818812 [03:17<04:12, 9297.25it/s]\u001b[A\n",
      " 39%|███▉      | 1480000/3818812 [03:18<04:08, 9407.22it/s]\u001b[A\n",
      " 39%|███▉      | 1490000/3818812 [03:19<04:10, 9294.14it/s]\u001b[A\n",
      " 39%|███▉      | 1500000/3818812 [03:21<04:19, 8940.60it/s]\u001b[A\n",
      " 40%|███▉      | 1510000/3818812 [03:22<04:13, 9121.01it/s]\u001b[A\n",
      " 40%|███▉      | 1520000/3818812 [03:23<04:20, 8840.41it/s]\u001b[A\n",
      " 40%|████      | 1530000/3818812 [03:24<04:12, 9066.19it/s]\u001b[A\n",
      " 40%|████      | 1540000/3818812 [03:25<04:07, 9197.94it/s]\u001b[A\n",
      " 41%|████      | 1550000/3818812 [03:26<03:58, 9501.05it/s]\u001b[A\n",
      " 41%|████      | 1560000/3818812 [03:27<03:53, 9672.87it/s]\u001b[A\n",
      " 41%|████      | 1570000/3818812 [03:28<03:59, 9371.82it/s]\u001b[A\n",
      " 41%|████▏     | 1580000/3818812 [03:29<03:50, 9716.88it/s]\u001b[A\n",
      " 42%|████▏     | 1590000/3818812 [03:30<03:55, 9447.73it/s]\u001b[A\n",
      " 42%|████▏     | 1600000/3818812 [03:31<04:01, 9188.36it/s]\u001b[A\n",
      " 42%|████▏     | 1610000/3818812 [03:32<03:51, 9531.05it/s]\u001b[A\n",
      " 42%|████▏     | 1620000/3818812 [03:33<03:53, 9426.66it/s]\u001b[A\n",
      " 43%|████▎     | 1630000/3818812 [03:34<03:47, 9602.57it/s]\u001b[A\n",
      " 43%|████▎     | 1640000/3818812 [03:35<03:59, 9096.80it/s]\u001b[A\n",
      " 43%|████▎     | 1650000/3818812 [03:36<03:47, 9540.05it/s]\u001b[A\n",
      " 43%|████▎     | 1660000/3818812 [03:37<03:46, 9512.80it/s]\u001b[A\n",
      " 44%|████▎     | 1670000/3818812 [03:38<03:43, 9595.06it/s]\u001b[A\n",
      " 44%|████▍     | 1680000/3818812 [03:40<03:45, 9474.50it/s]\u001b[A\n",
      " 44%|████▍     | 1690000/3818812 [03:41<03:46, 9400.64it/s]\u001b[A\n",
      " 45%|████▍     | 1700000/3818812 [03:42<03:42, 9534.82it/s]\u001b[A\n",
      " 45%|████▍     | 1710000/3818812 [03:43<03:40, 9542.88it/s]\u001b[A\n",
      " 45%|████▌     | 1720000/3818812 [03:44<03:38, 9609.01it/s]\u001b[A\n",
      " 45%|████▌     | 1730000/3818812 [03:45<03:37, 9588.71it/s]\u001b[A\n",
      " 46%|████▌     | 1740000/3818812 [03:46<03:39, 9460.61it/s]\u001b[A\n",
      " 46%|████▌     | 1750000/3818812 [03:47<03:34, 9652.33it/s]\u001b[A\n",
      " 46%|████▌     | 1760000/3818812 [03:48<03:36, 9515.04it/s]\u001b[A\n",
      " 46%|████▋     | 1770000/3818812 [03:49<03:29, 9788.98it/s]\u001b[A\n",
      " 47%|████▋     | 1780000/3818812 [03:50<03:37, 9370.01it/s]\u001b[A\n",
      " 47%|████▋     | 1790000/3818812 [03:51<03:47, 8917.29it/s]\u001b[A\n",
      " 47%|████▋     | 1800000/3818812 [03:52<03:45, 8946.42it/s]\u001b[A\n",
      " 47%|████▋     | 1810000/3818812 [03:53<03:36, 9290.29it/s]\u001b[A\n",
      " 48%|████▊     | 1820000/3818812 [03:55<03:36, 9247.46it/s]\u001b[A\n",
      " 48%|████▊     | 1830000/3818812 [03:55<03:28, 9539.64it/s]\u001b[A\n",
      " 48%|████▊     | 1840000/3818812 [03:57<03:29, 9425.97it/s]\u001b[A\n",
      " 48%|████▊     | 1850000/3818812 [03:58<03:26, 9550.19it/s]\u001b[A\n",
      " 49%|████▊     | 1860000/3818812 [03:59<03:23, 9626.00it/s]\u001b[A\n",
      " 49%|████▉     | 1870000/3818812 [04:00<03:23, 9572.28it/s]\u001b[A\n",
      " 49%|████▉     | 1880000/3818812 [04:01<03:21, 9642.75it/s]\u001b[A\n",
      " 49%|████▉     | 1890000/3818812 [04:02<03:21, 9583.90it/s]\u001b[A\n",
      " 50%|████▉     | 1900000/3818812 [04:03<03:21, 9540.01it/s]\u001b[A\n",
      " 50%|█████     | 1910000/3818812 [04:04<03:14, 9835.13it/s]\u001b[A\n",
      " 50%|█████     | 1920000/3818812 [04:05<03:10, 9958.10it/s]\u001b[A\n",
      " 51%|█████     | 1930000/3818812 [04:06<03:17, 9549.18it/s]\u001b[A\n",
      " 51%|█████     | 1940000/3818812 [04:07<03:14, 9666.04it/s]\u001b[A\n",
      " 51%|█████     | 1950000/3818812 [04:08<03:18, 9432.29it/s]\u001b[A\n",
      " 51%|█████▏    | 1960000/3818812 [04:09<03:14, 9577.37it/s]\u001b[A\n",
      " 52%|█████▏    | 1970000/3818812 [04:10<03:12, 9616.69it/s]\u001b[A\n",
      " 52%|█████▏    | 1980000/3818812 [04:11<03:10, 9669.06it/s]\u001b[A\n",
      " 52%|█████▏    | 1990000/3818812 [04:12<03:13, 9468.22it/s]\u001b[A\n",
      " 52%|█████▏    | 2000000/3818812 [04:13<03:10, 9570.52it/s]\u001b[A\n",
      " 53%|█████▎    | 2010000/3818812 [04:14<03:16, 9190.22it/s]\u001b[A\n",
      " 53%|█████▎    | 2020000/3818812 [04:15<03:10, 9462.78it/s]\u001b[A\n",
      " 53%|█████▎    | 2030000/3818812 [04:16<03:08, 9495.68it/s]\u001b[A\n",
      " 53%|█████▎    | 2040000/3818812 [04:17<03:03, 9710.81it/s]\u001b[A\n",
      " 54%|█████▎    | 2050000/3818812 [04:18<03:05, 9558.51it/s]\u001b[A\n",
      " 54%|█████▍    | 2060000/3818812 [04:20<03:06, 9437.27it/s]\u001b[A\n",
      " 54%|█████▍    | 2070000/3818812 [04:20<02:59, 9719.63it/s]\u001b[A\n",
      " 54%|█████▍    | 2080000/3818812 [04:22<03:01, 9570.01it/s]\u001b[A\n",
      " 55%|█████▍    | 2090000/3818812 [04:23<03:02, 9467.98it/s]\u001b[A\n",
      " 55%|█████▍    | 2100000/3818812 [04:24<02:56, 9723.97it/s]\u001b[A\n",
      " 55%|█████▌    | 2110000/3818812 [04:25<02:57, 9603.47it/s]\u001b[A\n",
      " 56%|█████▌    | 2120000/3818812 [04:26<03:00, 9436.69it/s]\u001b[A\n",
      " 56%|█████▌    | 2130000/3818812 [04:27<03:01, 9297.90it/s]\u001b[A\n",
      " 56%|█████▌    | 2140000/3818812 [04:28<03:08, 8900.79it/s]\u001b[A\n",
      " 56%|█████▋    | 2150000/3818812 [04:29<02:59, 9272.83it/s]\u001b[A\n",
      " 57%|█████▋    | 2160000/3818812 [04:30<02:50, 9732.03it/s]\u001b[A\n",
      " 57%|█████▋    | 2170000/3818812 [04:31<02:46, 9887.60it/s]\u001b[A\n",
      " 57%|█████▋    | 2180000/3818812 [04:32<02:52, 9488.41it/s]\u001b[A\n",
      " 57%|█████▋    | 2190000/3818812 [04:33<02:52, 9468.18it/s]\u001b[A\n",
      " 58%|█████▊    | 2200000/3818812 [04:34<02:45, 9760.09it/s]\u001b[A\n",
      " 58%|█████▊    | 2210000/3818812 [04:35<02:37, 10212.31it/s]\u001b[A\n",
      " 58%|█████▊    | 2220000/3818812 [04:36<02:34, 10334.87it/s]\u001b[A\n",
      " 58%|█████▊    | 2230000/3818812 [04:37<02:31, 10467.87it/s]\u001b[A\n",
      " 59%|█████▊    | 2240000/3818812 [04:38<02:32, 10328.86it/s]\u001b[A\n",
      " 59%|█████▉    | 2250000/3818812 [04:39<02:29, 10470.05it/s]\u001b[A\n",
      " 59%|█████▉    | 2260000/3818812 [04:40<02:30, 10335.99it/s]\u001b[A\n",
      " 59%|█████▉    | 2270000/3818812 [04:41<02:30, 10265.17it/s]\u001b[A\n",
      " 60%|█████▉    | 2280000/3818812 [04:42<02:26, 10512.26it/s]\u001b[A\n",
      " 60%|█████▉    | 2290000/3818812 [04:43<02:29, 10239.57it/s]\u001b[A\n",
      " 60%|██████    | 2300000/3818812 [04:44<02:30, 10104.52it/s]\u001b[A\n",
      " 60%|██████    | 2310000/3818812 [04:45<02:29, 10083.08it/s]\u001b[A\n",
      " 61%|██████    | 2320000/3818812 [04:46<02:25, 10331.61it/s]\u001b[A\n",
      " 61%|██████    | 2330000/3818812 [04:47<02:27, 10102.58it/s]\u001b[A\n",
      " 61%|██████▏   | 2340000/3818812 [04:48<02:26, 10076.11it/s]\u001b[A\n",
      " 62%|██████▏   | 2350000/3818812 [04:49<02:29, 9799.71it/s] \u001b[A\n",
      " 62%|██████▏   | 2360000/3818812 [04:50<02:27, 9904.19it/s]\u001b[A\n",
      " 62%|██████▏   | 2370000/3818812 [04:51<02:30, 9628.63it/s]\u001b[A\n",
      " 62%|██████▏   | 2380000/3818812 [04:52<02:29, 9654.40it/s]\u001b[A\n",
      " 63%|██████▎   | 2390000/3818812 [04:53<02:28, 9594.77it/s]\u001b[A\n",
      " 63%|██████▎   | 2400000/3818812 [04:54<02:28, 9533.24it/s]\u001b[A\n",
      " 63%|██████▎   | 2410000/3818812 [04:55<02:23, 9809.10it/s]\u001b[A\n",
      " 63%|██████▎   | 2420000/3818812 [04:56<02:21, 9887.90it/s]\u001b[A\n",
      " 64%|██████▎   | 2430000/3818812 [04:57<02:20, 9865.05it/s]\u001b[A\n",
      " 64%|██████▍   | 2440000/3818812 [04:58<02:19, 9869.87it/s]\u001b[A\n",
      " 64%|██████▍   | 2450000/3818812 [04:59<02:21, 9658.86it/s]\u001b[A\n",
      " 64%|██████▍   | 2460000/3818812 [05:00<02:20, 9657.46it/s]\u001b[A\n",
      " 65%|██████▍   | 2470000/3818812 [05:01<02:17, 9788.66it/s]\u001b[A\n",
      " 65%|██████▍   | 2480000/3818812 [05:02<02:16, 9820.90it/s]\u001b[A\n",
      " 65%|██████▌   | 2490000/3818812 [05:03<02:17, 9665.63it/s]\u001b[A\n",
      " 65%|██████▌   | 2500000/3818812 [05:04<02:18, 9517.10it/s]\u001b[A\n",
      " 66%|██████▌   | 2510000/3818812 [05:05<02:14, 9760.31it/s]\u001b[A\n",
      " 66%|██████▌   | 2520000/3818812 [05:06<02:08, 10105.98it/s]\u001b[A\n",
      " 66%|██████▋   | 2530000/3818812 [05:07<02:06, 10188.56it/s]\u001b[A\n",
      " 67%|██████▋   | 2540000/3818812 [05:08<02:04, 10311.51it/s]\u001b[A\n",
      " 67%|██████▋   | 2550000/3818812 [05:09<02:01, 10433.77it/s]\u001b[A\n",
      " 67%|██████▋   | 2560000/3818812 [05:10<02:07, 9882.64it/s] \u001b[A\n",
      " 67%|██████▋   | 2570000/3818812 [05:11<02:05, 9957.55it/s]\u001b[A\n",
      " 68%|██████▊   | 2580000/3818812 [05:12<02:11, 9449.22it/s]\u001b[A\n",
      " 68%|██████▊   | 2590000/3818812 [05:13<02:07, 9634.68it/s]\u001b[A\n",
      " 68%|██████▊   | 2600000/3818812 [05:14<02:08, 9466.30it/s]\u001b[A\n",
      " 68%|██████▊   | 2610000/3818812 [05:15<02:06, 9529.21it/s]\u001b[A\n",
      " 69%|██████▊   | 2620000/3818812 [05:17<02:07, 9394.83it/s]\u001b[A\n",
      " 69%|██████▉   | 2630000/3818812 [05:18<02:04, 9536.85it/s]\u001b[A\n",
      " 69%|██████▉   | 2640000/3818812 [05:19<02:00, 9807.05it/s]\u001b[A\n",
      " 69%|██████▉   | 2650000/3818812 [05:20<02:01, 9648.22it/s]\u001b[A\n",
      " 70%|██████▉   | 2660000/3818812 [05:21<02:01, 9571.75it/s]\u001b[A\n",
      " 70%|██████▉   | 2670000/3818812 [05:22<02:00, 9520.68it/s]\u001b[A\n",
      " 70%|███████   | 2680000/3818812 [05:23<02:01, 9371.43it/s]\u001b[A\n",
      " 70%|███████   | 2690000/3818812 [05:24<02:08, 8778.60it/s]\u001b[A\n",
      " 71%|███████   | 2700000/3818812 [05:25<02:10, 8549.89it/s]\u001b[A\n",
      " 71%|███████   | 2710000/3818812 [05:26<02:05, 8832.47it/s]\u001b[A\n",
      " 71%|███████   | 2720000/3818812 [05:28<02:06, 8673.31it/s]\u001b[A\n",
      " 71%|███████▏  | 2730000/3818812 [05:29<02:14, 8096.63it/s]\u001b[A\n",
      " 72%|███████▏  | 2740000/3818812 [05:31<02:35, 6952.44it/s]\u001b[A\n",
      " 72%|███████▏  | 2750000/3818812 [05:32<02:25, 7329.23it/s]\u001b[A\n",
      " 72%|███████▏  | 2760000/3818812 [05:33<02:17, 7699.40it/s]\u001b[A\n",
      " 73%|███████▎  | 2770000/3818812 [05:34<02:08, 8154.99it/s]\u001b[A\n",
      " 73%|███████▎  | 2780000/3818812 [05:35<02:00, 8634.88it/s]\u001b[A\n",
      " 73%|███████▎  | 2790000/3818812 [05:36<01:56, 8867.01it/s]\u001b[A\n",
      " 73%|███████▎  | 2800000/3818812 [05:37<01:50, 9185.68it/s]\u001b[A\n",
      " 74%|███████▎  | 2810000/3818812 [05:39<02:03, 8189.34it/s]\u001b[A\n",
      " 74%|███████▍  | 2820000/3818812 [05:41<02:16, 7317.06it/s]\u001b[A\n",
      " 74%|███████▍  | 2830000/3818812 [05:42<02:27, 6721.94it/s]\u001b[A\n",
      " 74%|███████▍  | 2840000/3818812 [05:43<02:12, 7383.56it/s]\u001b[A\n",
      " 75%|███████▍  | 2850000/3818812 [05:45<02:09, 7456.60it/s]\u001b[A\n",
      " 75%|███████▍  | 2860000/3818812 [05:46<01:58, 8058.50it/s]\u001b[A\n",
      " 75%|███████▌  | 2870000/3818812 [05:47<01:53, 8324.97it/s]\u001b[A\n",
      " 75%|███████▌  | 2880000/3818812 [05:48<01:52, 8308.71it/s]\u001b[A\n",
      " 76%|███████▌  | 2890000/3818812 [05:49<01:47, 8673.98it/s]\u001b[A\n",
      " 76%|███████▌  | 2900000/3818812 [05:51<01:54, 8034.06it/s]\u001b[A\n",
      " 76%|███████▌  | 2910000/3818812 [05:52<01:54, 7971.89it/s]\u001b[A\n",
      " 76%|███████▋  | 2920000/3818812 [05:53<01:50, 8149.64it/s]\u001b[A\n",
      " 77%|███████▋  | 2930000/3818812 [05:54<01:46, 8343.15it/s]\u001b[A\n",
      " 77%|███████▋  | 2940000/3818812 [05:55<01:43, 8511.69it/s]\u001b[A\n",
      " 77%|███████▋  | 2950000/3818812 [05:56<01:37, 8893.85it/s]\u001b[A\n",
      " 78%|███████▊  | 2960000/3818812 [05:57<01:36, 8915.86it/s]\u001b[A\n",
      " 78%|███████▊  | 2970000/3818812 [05:59<01:36, 8757.00it/s]\u001b[A\n",
      " 78%|███████▊  | 2980000/3818812 [06:00<01:36, 8713.25it/s]\u001b[A\n",
      " 78%|███████▊  | 2990000/3818812 [06:01<01:33, 8895.96it/s]\u001b[A\n",
      " 79%|███████▊  | 3000000/3818812 [06:02<01:32, 8849.52it/s]\u001b[A\n",
      " 79%|███████▉  | 3010000/3818812 [06:03<01:35, 8501.71it/s]\u001b[A\n",
      " 79%|███████▉  | 3020000/3818812 [06:04<01:31, 8697.22it/s]\u001b[A\n",
      " 79%|███████▉  | 3030000/3818812 [06:05<01:28, 8954.12it/s]\u001b[A\n",
      " 80%|███████▉  | 3040000/3818812 [06:06<01:25, 9119.75it/s]\u001b[A\n",
      " 80%|███████▉  | 3050000/3818812 [06:08<01:24, 9071.47it/s]\u001b[A\n",
      " 80%|████████  | 3060000/3818812 [06:09<01:23, 9087.61it/s]\u001b[A\n",
      " 80%|████████  | 3070000/3818812 [06:10<01:24, 8843.11it/s]\u001b[A\n",
      " 81%|████████  | 3080000/3818812 [06:11<01:22, 9000.43it/s]\u001b[A\n",
      " 81%|████████  | 3090000/3818812 [06:12<01:20, 9033.70it/s]\u001b[A\n",
      " 81%|████████  | 3100000/3818812 [06:13<01:20, 8926.74it/s]\u001b[A\n",
      " 81%|████████▏ | 3110000/3818812 [06:14<01:18, 9026.67it/s]\u001b[A\n",
      " 82%|████████▏ | 3120000/3818812 [06:15<01:19, 8792.64it/s]\u001b[A\n",
      " 82%|████████▏ | 3130000/3818812 [06:16<01:16, 8957.41it/s]\u001b[A\n",
      " 82%|████████▏ | 3140000/3818812 [06:18<01:14, 9079.53it/s]\u001b[A\n",
      " 82%|████████▏ | 3150000/3818812 [06:19<01:14, 9000.57it/s]\u001b[A\n",
      " 83%|████████▎ | 3160000/3818812 [06:20<01:13, 8935.14it/s]\u001b[A\n",
      " 83%|████████▎ | 3170000/3818812 [06:21<01:11, 9084.17it/s]\u001b[A\n",
      " 83%|████████▎ | 3180000/3818812 [06:22<01:10, 9056.94it/s]\u001b[A\n",
      " 84%|████████▎ | 3190000/3818812 [06:23<01:11, 8740.49it/s]\u001b[A\n",
      " 84%|████████▍ | 3200000/3818812 [06:24<01:09, 8909.44it/s]\u001b[A\n",
      " 84%|████████▍ | 3210000/3818812 [06:25<01:07, 9080.89it/s]\u001b[A\n",
      " 84%|████████▍ | 3220000/3818812 [06:26<01:06, 9025.52it/s]\u001b[A\n",
      " 85%|████████▍ | 3230000/3818812 [06:28<01:05, 9033.66it/s]\u001b[A\n",
      " 85%|████████▍ | 3240000/3818812 [06:29<01:04, 8919.28it/s]\u001b[A\n",
      " 85%|████████▌ | 3250000/3818812 [06:30<01:03, 8898.58it/s]\u001b[A\n",
      " 85%|████████▌ | 3260000/3818812 [06:31<01:01, 9158.55it/s]\u001b[A\n",
      " 86%|████████▌ | 3270000/3818812 [06:32<01:00, 9094.13it/s]\u001b[A\n",
      " 86%|████████▌ | 3280000/3818812 [06:33<00:59, 9015.48it/s]\u001b[A\n",
      " 86%|████████▌ | 3290000/3818812 [06:34<00:59, 8862.57it/s]\u001b[A\n",
      " 86%|████████▋ | 3300000/3818812 [06:35<00:56, 9107.62it/s]\u001b[A\n",
      " 87%|████████▋ | 3310000/3818812 [06:36<00:55, 9186.42it/s]\u001b[A\n",
      " 87%|████████▋ | 3320000/3818812 [06:38<00:54, 9170.06it/s]\u001b[A\n",
      " 87%|████████▋ | 3330000/3818812 [06:39<00:54, 8964.67it/s]\u001b[A\n",
      " 87%|████████▋ | 3340000/3818812 [06:40<00:53, 8890.60it/s]\u001b[A\n",
      " 88%|████████▊ | 3350000/3818812 [06:41<00:51, 9079.92it/s]\u001b[A\n",
      " 88%|████████▊ | 3360000/3818812 [06:42<00:50, 9082.47it/s]\u001b[A\n",
      " 88%|████████▊ | 3370000/3818812 [06:43<00:50, 8875.56it/s]\u001b[A\n",
      " 89%|████████▊ | 3380000/3818812 [06:44<00:49, 8803.06it/s]\u001b[A\n",
      " 89%|████████▉ | 3390000/3818812 [06:45<00:47, 8970.24it/s]\u001b[A\n",
      " 89%|████████▉ | 3400000/3818812 [06:46<00:45, 9192.16it/s]\u001b[A\n",
      " 89%|████████▉ | 3410000/3818812 [06:48<00:44, 9181.44it/s]\u001b[A\n",
      " 90%|████████▉ | 3420000/3818812 [06:49<00:44, 8961.83it/s]\u001b[A\n",
      " 90%|████████▉ | 3430000/3818812 [06:50<00:44, 8818.35it/s]\u001b[A\n",
      " 90%|█████████ | 3440000/3818812 [06:51<00:41, 9052.61it/s]\u001b[A\n",
      " 90%|█████████ | 3450000/3818812 [06:52<00:41, 8964.86it/s]\u001b[A\n",
      " 91%|█████████ | 3460000/3818812 [06:53<00:40, 8826.88it/s]\u001b[A\n",
      " 91%|█████████ | 3470000/3818812 [06:54<00:40, 8658.50it/s]\u001b[A\n",
      " 91%|█████████ | 3480000/3818812 [06:55<00:37, 9000.84it/s]\u001b[A\n",
      " 91%|█████████▏| 3490000/3818812 [06:56<00:35, 9140.89it/s]\u001b[A\n",
      " 92%|█████████▏| 3500000/3818812 [06:58<00:34, 9303.12it/s]\u001b[A\n",
      " 92%|█████████▏| 3510000/3818812 [06:59<00:33, 9279.85it/s]\u001b[A\n",
      " 92%|█████████▏| 3520000/3818812 [07:00<00:33, 8955.59it/s]\u001b[A\n",
      " 92%|█████████▏| 3530000/3818812 [07:01<00:31, 9148.14it/s]\u001b[A\n",
      " 93%|█████████▎| 3540000/3818812 [07:02<00:30, 9169.31it/s]\u001b[A\n",
      " 93%|█████████▎| 3550000/3818812 [07:03<00:30, 8938.76it/s]\u001b[A\n",
      " 93%|█████████▎| 3560000/3818812 [07:04<00:30, 8619.39it/s]\u001b[A\n",
      " 93%|█████████▎| 3570000/3818812 [07:05<00:28, 8817.70it/s]\u001b[A\n",
      " 94%|█████████▎| 3580000/3818812 [07:06<00:26, 9004.05it/s]\u001b[A\n",
      " 94%|█████████▍| 3590000/3818812 [07:08<00:25, 9152.25it/s]\u001b[A\n",
      " 94%|█████████▍| 3600000/3818812 [07:09<00:24, 9013.45it/s]\u001b[A\n",
      " 95%|█████████▍| 3610000/3818812 [07:10<00:24, 8632.01it/s]\u001b[A\n",
      " 95%|█████████▍| 3620000/3818812 [07:11<00:22, 8650.69it/s]\u001b[A\n",
      " 95%|█████████▌| 3630000/3818812 [07:12<00:21, 8982.48it/s]\u001b[A\n",
      " 95%|█████████▌| 3640000/3818812 [07:13<00:20, 8757.94it/s]\u001b[A\n",
      " 96%|█████████▌| 3650000/3818812 [07:14<00:18, 9026.15it/s]\u001b[A\n",
      " 96%|█████████▌| 3660000/3818812 [07:16<00:17, 8937.55it/s]\u001b[A\n",
      " 96%|█████████▌| 3670000/3818812 [07:17<00:16, 8851.11it/s]\u001b[A\n",
      " 96%|█████████▋| 3680000/3818812 [07:18<00:15, 9019.09it/s]\u001b[A\n",
      " 97%|█████████▋| 3690000/3818812 [07:19<00:14, 9067.74it/s]\u001b[A\n",
      " 97%|█████████▋| 3700000/3818812 [07:20<00:13, 8942.08it/s]\u001b[A\n",
      " 97%|█████████▋| 3710000/3818812 [07:21<00:12, 8488.17it/s]\u001b[A\n",
      " 97%|█████████▋| 3720000/3818812 [07:23<00:11, 8280.16it/s]\u001b[A\n",
      " 98%|█████████▊| 3730000/3818812 [07:24<00:10, 8331.60it/s]\u001b[A\n",
      " 98%|█████████▊| 3740000/3818812 [07:25<00:09, 8581.20it/s]\u001b[A\n",
      " 98%|█████████▊| 3750000/3818812 [07:26<00:07, 8678.69it/s]\u001b[A\n",
      " 98%|█████████▊| 3760000/3818812 [07:27<00:06, 8880.54it/s]\u001b[A\n",
      " 99%|█████████▊| 3770000/3818812 [07:28<00:05, 8944.52it/s]\u001b[A\n",
      " 99%|█████████▉| 3780000/3818812 [07:29<00:04, 8827.37it/s]\u001b[A\n",
      " 99%|█████████▉| 3790000/3818812 [07:30<00:03, 8825.14it/s]\u001b[A\n",
      "100%|█████████▉| 3800000/3818812 [07:32<00:02, 8596.04it/s]\u001b[A\n",
      "100%|█████████▉| 3810000/3818812 [07:33<00:01, 8716.93it/s]\u001b[A\n",
      "100%|██████████| 3818812/3818812 [07:34<00:00, 8404.50it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/Round3.csv'\n",
    "output_file = './data/alligator_training_data/Round3_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6f751-eff3-4365-bd91-5124c111854a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d7aa92-eaef-4d54-86c1-25a191389662",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./R4_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d453dae8-adbf-46d2-a54b-723f31130e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9412429/9412429 [15:21<00:00, 10215.86it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/Round4.csv'\n",
    "output_file = './data/alligator_training_data/Round4_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b19b6f-f789-455f-bedd-cf6ea970b2ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad07b6-9d85-4421-a0a3-5ab58ed7cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_path = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ed510-c006-49cd-b673-4d04751eace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = './data/datasets_alligator_completi/2T-2020.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/2T_2020_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deffda-20b3-424b-a1db-3432871d679c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510be4c9-29b1-4396-904c-dcd612680f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT2_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e84812-71b4-4e80-8846-3cf0894e9eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438801/438801 [01:04<00:00, 6815.35it/s] \n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/HardTableR2-2021.csv'\n",
    "output_file = './data/alligator_training_data/HT2_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6746d-32fe-4cca-b092-432de7ec91c5",
   "metadata": {},
   "source": [
    "# HT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169d9c9-8e35-445a-ae9d-1fa08de630dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT3_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912fe7f-12fb-495b-9b37-dbeae3912b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR3-2021.csv'\n",
    "output_file = '../data/alligator_training_data/HT3_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdb058-ea11-440a-9fee-8b1ef419e321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
