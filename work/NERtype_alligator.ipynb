{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937bad8e-c05a-48f7-bee4-e06aef31f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a517463-bba5-43d5-a083-bc18ab0726a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.asyncio import tqdm\n",
    "import re\n",
    "import json\n",
    "import backoff  # Ensure this import is included for the decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe0f1d7-a613-4432-9cd9-66abc23b2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping definition\n",
    "\n",
    "PERS = 0\n",
    "LOC = 1\n",
    "ORG = 2\n",
    "OTHERS = 4\n",
    "none = 4\n",
    "\n",
    "entity_to_id = {\n",
    "    'PERS': PERS,\n",
    "    'LOC': LOC,\n",
    "    'ORG': ORG,\n",
    "    'OTHERS': OTHERS,\n",
    "    None: none\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b1c92-c95f-4907-894e-9eda00d2dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/datasets_alligator_completi'\n",
    "\n",
    "# Initialize an empty set to hold all unique ids\n",
    "all_unique_ids = set()\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000  # Adjust this value as needed\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [filename for filename in os.listdir(directory) if filename.endswith(\".csv\")]\n",
    "\n",
    "# Iterate over all files in the directory with a progress bar\n",
    "for filename in tqdm(csv_files, desc=\"Processing files\"):\n",
    "    # Construct full file path\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    # Initialize a set for unique ids in the current file\n",
    "    unique_ids_in_file = set()\n",
    "    \n",
    "    # Read the DataFrame in chunks\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        # Get the unique ids from the current chunk\n",
    "        unique_ids = set(chunk['id'])\n",
    "        \n",
    "        # Update the set of unique ids in the current file\n",
    "        unique_ids_in_file.update(unique_ids)\n",
    "        \n",
    "        # Update the set of all unique ids\n",
    "        all_unique_ids.update(unique_ids)\n",
    "    \n",
    "    # Print the number of unique ids in the current file and total accumulated unique ids\n",
    "    print(f\"File: {filename}, Unique IDs in File: {len(unique_ids_in_file)}, Total Unique IDs: {len(all_unique_ids)}\")\n",
    "\n",
    "# Print the final total number of unique ids across all files\n",
    "print(f\"Final Total unique IDs across all files: {len(all_unique_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28f6f0-2f69-4dcc-a2fd-b5e6e222a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels'\n",
    "params = {\n",
    "    'lang': 'en',\n",
    "    'token': 'lamapi_demo_2023'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "id_to_ner = {}\n",
    "\n",
    "# Semaphore to limit concurrent requests\n",
    "semaphore = asyncio.Semaphore(70)\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch_with_backoff(session, url, payload, id):\n",
    "    async with semaphore:\n",
    "        async with session.post(url, params=params, headers=headers, json=payload) as response:\n",
    "            return id, await response.json()\n",
    "\n",
    "async def process_chunk(session, chunk):\n",
    "    tasks = []\n",
    "    for unique_id in chunk:\n",
    "        payload = {\n",
    "            \"json\": [\n",
    "                f\"{unique_id}\"\n",
    "            ]\n",
    "        }\n",
    "        tasks.append(fetch_with_backoff(session, url, payload, unique_id))\n",
    "    \n",
    "    responses = []\n",
    "    for result in asyncio.as_completed(tasks):\n",
    "        response = await result\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "async def main():\n",
    "\n",
    "    all_unique_ids_list = list(all_unique_ids)\n",
    "    \n",
    "    # Chunk size\n",
    "    chunk_size = 10000\n",
    "\n",
    "    # Split IDs into chunks\n",
    "    chunks = [all_unique_ids_list[i:i + chunk_size] for i in range(0, len(all_unique_ids_list), chunk_size)]\n",
    "\n",
    "    progress_bar = tqdm(total=len(chunks), desc=\"Processing Chunks\")\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            responses = await process_chunk(session, chunk)\n",
    "            for id, response in responses:\n",
    "                try:\n",
    "                    id_to_ner[id] = response[id]['NERtype']\n",
    "                except:\n",
    "                    id_to_ner[id] = None                  \n",
    "\n",
    "        \n",
    "        # Save dictionary to a JSON file after each chunk\n",
    "        with open('id_to_ner.json', 'w') as json_file:\n",
    "            json.dump(id_to_ner, json_file)\n",
    "        \n",
    "        progress_bar.update(1)  # Update progress bar\n",
    "    \n",
    "    progress_bar.close()  # Close progress bar once done\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await (main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94bbbe6-3e86-4ee5-89cf-d7d8331f619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./id_to_ner.json\"\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    id_to_ner = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e1500-d15d-4714-9f81-cc04ed2fa1e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833f5a6b-bc99-4861-b5a4-4454d1d10d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 25.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PERS\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eb2d285-3048-4cb7-a502-9828d569a7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79274/79274 [00:07<00:00, 10928.02it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/Round1_T2D.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/Round1_T2D_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f779644-057b-43bd-9035-9f5008aef22d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# R3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7ab04ed-7c19-4866-b485-0fe82ddf6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2161/2161 [09:59<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round3_2019/gt/CTA_Round3_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1]            \n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PERS\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = category_list\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c8f61-a4f2-49d9-ad41-bf2cd79ce49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3818812/3818812 [04:56<00:00, 14807.09it/s]"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/Round3.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/Round3_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6f751-eff3-4365-bd91-5124c111854a",
   "metadata": {},
   "source": [
    "# R4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d7aa92-eaef-4d54-86c1-25a191389662",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./R4_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d453dae8-adbf-46d2-a54b-723f31130e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9412429/9412429 [26:35<00:00, 5899.71it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/Round4.csv'\n",
    "output_file = './data/alligator_training_data/Round4_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b19b6f-f789-455f-bedd-cf6ea970b2ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad07b6-9d85-4421-a0a3-5ab58ed7cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_path = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ed510-c006-49cd-b673-4d04751eace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = './data/datasets_alligator_completi/2T-2020.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/2T_2020_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5deffda-20b3-424b-a1db-3432871d679c",
   "metadata": {},
   "source": [
    "# HT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510be4c9-29b1-4396-904c-dcd612680f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT2_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90e84812-71b4-4e80-8846-3cf0894e9eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438801/438801 [01:04<00:00, 6815.35it/s] \n"
     ]
    }
   ],
   "source": [
    "csv_file = './data/datasets_alligator_completi/HardTableR2-2021.csv'\n",
    "output_file = './data/alligator_training_data/HT2_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6746d-32fe-4cca-b092-432de7ec91c5",
   "metadata": {},
   "source": [
    "# HT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169d9c9-8e35-445a-ae9d-1fa08de630dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT3_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912fe7f-12fb-495b-9b37-dbeae3912b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR3-2021.csv'\n",
    "output_file = '../data/alligator_training_data/HT3_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdb058-ea11-440a-9fee-8b1ef419e321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
