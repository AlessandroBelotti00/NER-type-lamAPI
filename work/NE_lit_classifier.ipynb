{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2be3fbb-0e7a-44c2-8807-381e580db097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 10:57:44.772192: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-05 10:57:44.876632: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-05 10:57:45.320277: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 10:57:47.900107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ec340f-ef96-4940-9e89-a8be6c17a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "columns = []\n",
    "target = []\n",
    "average_numeric_counts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc0c11-e811-4540-9a74-c29dd551b55f",
   "metadata": {},
   "source": [
    "## Round1_T2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06dfe186-559d-4a4b-a568-eacafc41f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "R1_cea = [item[0]for item in R1_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab481172-9273-43e5-b4d2-d1011cec6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 29.24it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R1_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408542e-7a90-4b4b-b1a8-74fc2448eed5",
   "metadata": {},
   "source": [
    "## Round3_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da63443-b6a5-4060-b097-e6e4a2fb9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)\n",
    "\n",
    "R3_cea = [item[0]for item in R3_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081c261e-3ff1-41ca-8cde-34c090d858e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2162/2162 [15:12<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R3_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673eea67-ad8c-4b23-99ef-c5f2dc86b1cc",
   "metadata": {},
   "source": [
    "## 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778c0a1c-045b-4cc6-ac29-d60215b8fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_2T_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dcf378a-2f88-44c0-a551-843d7dc9bed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:39<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_2T_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a4ed1-7939-4dfc-9eb4-9c9c0f102ef4",
   "metadata": {},
   "source": [
    "## Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "208edf55-99eb-4018-a0a7-031dbd192a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b461089-7f54-4ed5-9aef-669855493e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22207/22207 [21:16:10<00:00,  3.45s/it]        \n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af83a2d-7e99-4577-85ec-b297fdf376ad",
   "metadata": {},
   "source": [
    "## DF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1c14bf0-1ea7-4856-90a9-e433c1c807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df_def = pd.DataFrame({\n",
    "    'column names': columns,\n",
    "    'median_lengths': median_lengths,\n",
    "    'median_token_counts': median_token_counts,\n",
    "    'average_numeric_counts': average_numeric_counts,\n",
    "    'target': target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb6c179f-5eb2-4c54-9d77-4639ddb9f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_def.to_csv('./data/NE_lit_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446e1e43-0b44-4204-b52a-80be8cce57fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column names</th>\n",
       "      <th>median_lengths</th>\n",
       "      <th>median_token_counts</th>\n",
       "      <th>average_numeric_counts</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>col0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>col1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>col2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>col0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>col1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>col2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>col3</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column names  median_lengths  median_token_counts  average_numeric_counts  \\\n",
       "0         col0            10.0                  2.0                1.650000   \n",
       "1         col1             5.0                  1.0                2.000000   \n",
       "2         col2             6.0                  1.0                2.000000   \n",
       "3         col0            12.0                  3.0                0.826087   \n",
       "4         col1            16.0                  1.0                2.000000   \n",
       "5         col2             6.0                  1.0                2.000000   \n",
       "6         col3             5.5                  1.0                2.000000   \n",
       "\n",
       "  target  \n",
       "0     NE  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3     NE  \n",
       "4    NaN  \n",
       "5    NaN  \n",
       "6    NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "#   READ DIRECTLY THE DATASET HERE\n",
    "###################################\n",
    "\n",
    "df = pd.read_csv('./data/NE_lit_dataset.csv')\n",
    "filtered_df = df[df['target'].isin(['lit', 'NE'])]\n",
    "\n",
    "# Displaying the filtered DataFrame\n",
    "df[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "001b29d6-834d-47ce-bbf7-eac0d736e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'NE': 51553\n",
      "Count of 'lit': 1293\n",
      "Count of 'NaN': 25904\n"
     ]
    }
   ],
   "source": [
    "target_counts = df['target'].value_counts()\n",
    "\n",
    "# Extract counts for specific values\n",
    "ne_count = target_counts.get(\"NE\", 0)\n",
    "lit_count = target_counts.get(\"lit\", 0)\n",
    "none_count = df.shape[0] - (ne_count+lit_count)\n",
    "\n",
    "print(f\"Count of 'NE': {ne_count}\")\n",
    "print(f\"Count of 'lit': {lit_count}\")\n",
    "print(f\"Count of 'NaN': {none_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b0060-e2de-4d00-a043-56bc1709897d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2256078-4db3-4a4a-98c0-1e49c19b7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11525/2629803127.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['target'] = label_encoder.fit_transform(filtered_df['target'])\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9039 - loss: 0.1970 - val_accuracy: 0.9907 - val_loss: 0.0334\n",
      "Epoch 2/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9924 - loss: 0.0271 - val_accuracy: 0.9889 - val_loss: 0.0285\n",
      "Epoch 3/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9927 - loss: 0.0240 - val_accuracy: 0.9905 - val_loss: 0.0291\n",
      "Epoch 4/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0232 - val_accuracy: 0.9899 - val_loss: 0.0266\n",
      "Epoch 5/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9914 - loss: 0.0265 - val_accuracy: 0.9902 - val_loss: 0.0276\n",
      "Epoch 6/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0254 - val_accuracy: 0.9908 - val_loss: 0.0272\n",
      "Epoch 7/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0232 - val_accuracy: 0.9907 - val_loss: 0.0268\n",
      "Epoch 8/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9927 - loss: 0.0212 - val_accuracy: 0.9908 - val_loss: 0.0264\n",
      "Epoch 9/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9930 - loss: 0.0227 - val_accuracy: 0.9907 - val_loss: 0.0256\n",
      "Epoch 10/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0246 - val_accuracy: 0.9907 - val_loss: 0.0273\n",
      "Epoch 11/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9924 - loss: 0.0233 - val_accuracy: 0.9907 - val_loss: 0.0261\n",
      "Epoch 12/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0239 - val_accuracy: 0.9907 - val_loss: 0.0274\n",
      "Epoch 13/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9938 - loss: 0.0206 - val_accuracy: 0.9901 - val_loss: 0.0273\n",
      "Epoch 14/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0201 - val_accuracy: 0.9907 - val_loss: 0.0267\n",
      "Epoch 15/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0219 - val_accuracy: 0.9907 - val_loss: 0.0266\n",
      "Epoch 16/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0225 - val_accuracy: 0.9904 - val_loss: 0.0266\n",
      "Epoch 17/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9928 - loss: 0.0221 - val_accuracy: 0.9908 - val_loss: 0.0261\n",
      "Epoch 18/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0216 - val_accuracy: 0.9908 - val_loss: 0.0261\n",
      "Epoch 19/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0222 - val_accuracy: 0.9909 - val_loss: 0.0259\n",
      "Epoch 20/20\n",
      "\u001b[1m1057/1057\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9924 - loss: 0.0222 - val_accuracy: 0.9901 - val_loss: 0.0274\n",
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0246\n",
      "Test Accuracy: 99.23%\n"
     ]
    }
   ],
   "source": [
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df['target'] = label_encoder.fit_transform(filtered_df['target'])\n",
    "\n",
    "# One-hot encode the 'column names' column\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = one_hot_encoder.fit_transform(filtered_df[['column names']])\n",
    "\n",
    "# Combine the encoded categorical data with the numeric data\n",
    "numeric_data = filtered_df[['median_lengths', 'median_token_counts', 'average_numeric_counts']].values\n",
    "X = np.hstack([encoded_columns, numeric_data])\n",
    "y = filtered_df['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28204b07-3a07-4b09-8748-ec0d4f4eade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m331/331\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Original label: ['col5'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col4'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col5'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col5'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col5'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col3'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col4'], Predicted: NE\n",
      "Original label: ['col2'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: NE\n",
      "Original label: ['col0'], Predicted: NE\n",
      "Original label: ['col1'], Predicted: lit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "mapped_predictions = [\"lit\" if pred == 1 else \"NE\" for pred in y_pred.ravel()]\n",
    "\n",
    "# Extract the part of X_test that corresponds to the one-hot encoded columns\n",
    "encoded_columns_test = X_test[:, :encoded_columns.shape[1]]\n",
    "\n",
    "# Inverse transform the one-hot encoded columns to get the original categorical labels\n",
    "original_labels = one_hot_encoder.inverse_transform(encoded_columns_test)\n",
    "\n",
    "# Print a few examples to check\n",
    "for i in range(100):\n",
    "    print(f'Original label: {original_labels[i]}, Predicted: {mapped_predictions[i]}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06397d40-ffd2-4fc2-a622-3a3338c45d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column names</th>\n",
       "      <th>median_lengths</th>\n",
       "      <th>median_token_counts</th>\n",
       "      <th>average_numeric_counts</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>col0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>col0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>col5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>col0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>col0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>col1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>col2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>col3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>col0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>col1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column names  median_lengths  median_token_counts  average_numeric_counts  \\\n",
       "0          col0            10.0                  2.0                1.650000   \n",
       "3          col0            12.0                  3.0                0.826087   \n",
       "8          col5             5.0                  1.0                1.000000   \n",
       "9          col0            19.5                  2.0                0.000000   \n",
       "12         col0            16.5                  3.0                1.000000   \n",
       "13         col1            19.0                  3.0                0.000000   \n",
       "14         col2            13.5                  2.0                0.000000   \n",
       "15         col3             7.0                  1.0                0.000000   \n",
       "16         col0            22.0                  3.0                0.000000   \n",
       "17         col1            23.0                  3.0                0.000000   \n",
       "\n",
       "    target  \n",
       "0        0  \n",
       "3        0  \n",
       "8        0  \n",
       "9        0  \n",
       "12       0  \n",
       "13       0  \n",
       "14       0  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
