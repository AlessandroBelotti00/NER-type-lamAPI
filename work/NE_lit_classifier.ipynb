{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4fc3a-0dce-4366-907b-927dac9615af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.41.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install tf-keras\n",
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2be3fbb-0e7a-44c2-8807-381e580db097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57ec340f-ef96-4940-9e89-a8be6c17a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ColumnAnalysis:\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.entity_type_dict = {\n",
    "            \"PERSON\": \"NE\",\n",
    "            \"NORP\": \"NE\",\n",
    "            \"FAC\": \"NE\",\n",
    "            \"ORG\": \"NE\",\n",
    "            \"GPE\": \"NE\",\n",
    "            \"LOC\": \"NE\",\n",
    "            \"PRODUCT\": \"NE\",\n",
    "            \"EVENT\": \"NE\",\n",
    "            \"WORK_OF_ART\": \"NE\",\n",
    "            \"LAW\": \"NE\",\n",
    "            \"LANGUAGE\": \"NE\",\n",
    "            \"DATE\": \"LIT\",\n",
    "            \"TIME\": \"LIT\",\n",
    "            \"PERCENT\": \"LIT\",\n",
    "            \"MONEY\": \"LIT\",\n",
    "            \"QUANTITY\": \"LIT\",\n",
    "            \"ORDINAL\": \"LIT\",\n",
    "            \"CARDINAL\": \"LIT\",\n",
    "            \"URL\": \"LIT\",\n",
    "            \"DESC\": \"LIT\",\n",
    "            \"TOKEN\": \"NE\",\n",
    "            \"INTEGER\": \"LIT\",\n",
    "            \"FLOAT\": \"LIT\",\n",
    "            \"DATETIME\": \"LIT\",\n",
    "            \"ADDRESS\": \"LIT\",\n",
    "            \"EMAIL\": \"LIT\"\n",
    "        }\n",
    "\n",
    "        self.LIT_DATATYPE = {\n",
    "            \"DATE\": \"DATETIME\", \n",
    "            \"TIME\": \"STRING\", \n",
    "            \"PERCENT\": \"STRING\", \n",
    "            \"MONEY\": \"STRING\", \n",
    "            \"QUANTITY\": \"STRING\", \n",
    "            \"ORDINAL\": \"NUMBER\", \n",
    "            \"CARDINAL\": \"NUMBER\", \n",
    "            \"URL\": \"STRING\",\n",
    "            \"DESC\": \"STRING\",\n",
    "            \"TOKEN\": \"STRING\",\n",
    "            \"INTEGER\": \"NUMBER\",\n",
    "            \"FLOAT\": \"NUMBER\",\n",
    "            \"DATETIME\": \"DATETIME\",\n",
    "            \"ADDRESS\": \"STRING\",\n",
    "            \"EMAIL\": \"STRING\",\n",
    "            \"STRING\": \"STRING\"\n",
    "        }\n",
    "\n",
    "        self.NE_DATATYPE = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"]\n",
    "\n",
    "\n",
    "    def most_frequent_element(self, input_list):\n",
    "        # Count occurrences of each element in the list\n",
    "        counter = Counter(input_list)\n",
    "        # Get the most common element and its count\n",
    "        most_common = counter.most_common(1)\n",
    "        # Return the element (or None if the list is empty)\n",
    "        return most_common[0][0] if most_common else None\n",
    "    \n",
    "    def classifiy_columns(self, df):\n",
    "\n",
    "        def most_frequent_element(input_list):\n",
    "            # Count occurrences of each element in the list\n",
    "            counter = Counter(input_list)\n",
    "            \n",
    "            # Get the most common element(s) and their count\n",
    "            most_common = counter.most_common(1)  # Returns a list of (element, count)\n",
    "            \n",
    "            # If there are ties (multiple elements with the same frequency), this will return the first one encountered.\n",
    "            return most_common[0][0]  # Return the element from the first tuple\n",
    "\n",
    "       \n",
    "        def update_dict(dictionary, key, value=1):\n",
    "            if key not in dictionary:\n",
    "                dictionary[key] = 0\n",
    "            dictionary[key] += value    \n",
    "\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        address_pattern = re.compile(\n",
    "            r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        datetime_pattern = re.compile(\n",
    "            r'(?:\\d{4}-\\d{2}-\\d{2})'  # YYYY-MM-DD format\n",
    "            r'|(?:31(?:\\/|-|\\.)0?[13578]|1[02](?:\\/|-|\\.)\\d{4})'  # 31 days months\n",
    "            r'|(?:29|30(?:\\/|-|\\.)0?[1,3-9]|1[0-2](?:\\/|-|\\.)\\d{4})'  # 29/30 days months\n",
    "            r'|(?:0?[1-9]|[12]\\d|3[01])(?:\\/|-|\\.)'  # Day\n",
    "            r'(?:0?[1-9]|1[0-2])(?:\\/|-|\\.)\\d{4}'  # Month\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/(?:\\d{2})'  # MM/DD/YY format\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/\\d{2}'  # MM/DD/YY format\n",
    "            r'\\b\\d{2}/(?:0?[1-9]|[12]\\d|3[01])/(?:0?[1-9]|1[0-2])\\b'  # YY/DD/MM format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d\\.[0-5]\\d'  # HH:MM.SS format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d'  # HH:MM format\n",
    "            r'|(?:[0-5]?\\d):[0-5]\\d(?:\\.\\d{1,2})?'  # H:MM or H:MM.S format\n",
    "            r'|(?:2[0-3]|[01]?\\d)h[0-5]?\\d(?:m[0-5]?\\d(?:\\.\\d{1,2})?s)?',  # HhMMmSSs format\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        #pattern = r'^[\\d,.]+$'  # ^ asserts position at the start, [\\d,.]+ matches one or more digits, commas, or periods, $ asserts position at the end.\n",
    "        #return re.match(pattern, input_string) is not None\n",
    "\n",
    "        col_type = []\n",
    "        final_result = {}\n",
    "        \n",
    "        for col_name, col_data in df.items():\n",
    "            \n",
    "            # Analyze the concatenated text using Spacy\n",
    "            labels = {}\n",
    "            tags = {\"NE\": 0, \"LIT\": 0}\n",
    "            type = []\n",
    "            count_cell = 0\n",
    "            \n",
    "            for cell in col_data:\n",
    "                is_number = False\n",
    "                label = None\n",
    "            \n",
    "                if isinstance(cell, str):\n",
    "                    if cell == \"NaN\":\n",
    "                        label = \"\"\n",
    "                    \n",
    "                    # Check if the cell is a URL\n",
    "                    elif re.match(url_pattern, cell):\n",
    "                        label = \"URL\"\n",
    "                    \n",
    "                    # Check if the cell is an Email\n",
    "                    elif re.match(email_pattern, cell):\n",
    "                        label = \"EMAIL\"\n",
    "                    \n",
    "                    # Check if the cell is an Address\n",
    "                    elif re.match(address_pattern, cell):\n",
    "                        label = \"ADDRESS\"\n",
    "                        \n",
    "                    # Check if the cell matches datetime pattern\n",
    "                    elif re.match(datetime_pattern, cell):\n",
    "                        label = \"DATETIME\"\n",
    "\n",
    "\n",
    "                # Check if the cell is a number\n",
    "                if label is None:\n",
    "                    try:\n",
    "                        cell = str(cell)\n",
    "                        if ',' in cell or '.' in cell or '%' in cell or '$' in cell:\n",
    "                            cell = cell.replace('.', '').replace(',', '').replace('%', '').replace('$', '')\n",
    "                        \n",
    "                        if len(cell) - len(re.findall(r'\\d', cell)) < 5:\n",
    "                            is_number = True\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                \n",
    "                if is_number:\n",
    "                    label = \"NUMBER\"\n",
    "                elif len(cell.split(\" \")) >= 15:   #is the choosen treshold for discriminate descriptions and strings\n",
    "                    label = \"NOA\"\n",
    "                elif len(cell.split(\" \")) == 1 and len(cell) <= 4:\n",
    "                    label = \"STRING\"\n",
    "                    \n",
    "                if label is not None:\n",
    "                    type.append(label)\n",
    "                    break\n",
    "                else:\n",
    "                    if count_cell > 5:\n",
    "                        type.append(f\"NE_OTHERS\")\n",
    "                        break\n",
    "                    else:\n",
    "                        count_cell += 1\n",
    "                        # do the lookup\n",
    "                        url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "                        params = {\n",
    "                            'name': cell,\n",
    "                            'token': 'lamapi_demo_2023',\n",
    "                            'kg': 'wikidata',\n",
    "                            'limit': 10,\n",
    "                            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{cell}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "                            'sort': [\n",
    "                                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                            ]\n",
    "                        }\n",
    "    \n",
    "                        response = requests.get(url, params=params)\n",
    "                        if response.status_code == 200:\n",
    "                            data = response.json()\n",
    "                            if len(data) > 0:\n",
    "                                if data[0]['NERtype'] != \"OTHERS\":\n",
    "                                    type.append(f\"NE_{data[0]['NERtype']}\")\n",
    "                                    break\n",
    "                                    \n",
    "                                #print(f\"{cell} --> NE_{data[0]['NERtype']}\")\n",
    "                                #type.append(f\"NE_{data[0]['NERtype']}\")\n",
    "                                \n",
    "                            else:\n",
    "                                type.append(\"None\")\n",
    "            \n",
    "            most_common_type = self.most_frequent_element(type)\n",
    "            col_type.append(most_common_type)\n",
    "  \n",
    "           \n",
    "        return col_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06b5aa8b-e43e-49c1-9ab2-99df78ebf217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/64 [00:20<21:26, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'NUMBER'), ('Media', 'NE_OTHERS'), ('MIX', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/10579449_0_1681126353774891032.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/64 [01:04<35:14, 34.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NE_OTHERS'), ('Publisher', 'NUMBER'), ('EU Release Date', 'DATETIME'), ('AU Release Date', 'DATETIME'), ('PEGI', 'NUMBER'), ('ACB', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/11833461_1_3811022039809817402.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3/64 [01:47<38:52, 38.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Great white pelican', 'NE_OTHERS'), ('Pelecanus onocrotalus', 'NE_OTHERS'), ('QE', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/13719111_1_5719401842463579519.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 4/64 [01:58<27:28, 27.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OAU State', 'NE_ORG'), ('Official Language', 'NE_PERS'), ('IMF SDRs', 'NUMBER'), ('Date of Independence', 'NUMBER'), ('Head of Central Bank', 'NE_PERS'), ('Capital', 'NE_LOC'), ('IMF #Votes', 'NUMBER'), ('Name of Currency', 'NE_LOC')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/14067031_0_559833072073397908.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/64 [01:58<17:31, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Party', 'NUMBER'), ('Votes 2015', 'NUMBER'), ('% votes 2015', 'NUMBER'), ('Seats', 'NUMBER'), ('Poll standing 16/4/15', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/1438042986423_95_20150728002306-00125-ip-10-236-191-2_88435628_5.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 6/64 [02:00<11:46, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'NUMBER'), ('Name', 'NE_LOC'), ('Age', 'NUMBER'), ('Club', 'NUMBER'), ('LMSC', 'NE_LOC'), ('Time', 'DATETIME')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/1438042986423_95_20150728002306-00329-ip-10-236-191-2_805336391_10.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 7/64 [02:00<07:51,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Worcestershire bowling', 'NE_PERS'), ('Overs', 'NUMBER'), ('Mdns', 'NUMBER'), ('Runs', 'NUMBER'), ('Wkts', 'NUMBER'), ('Wides', 'NUMBER'), ('No-Balls', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/1438042989018_40_20150728002309-00067-ip-10-236-191-2_57714692_2.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 8/64 [02:02<06:01,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Summit', 'NE_LOC'), ('Location', 'NE_OTHERS'), ('Elevation', 'NE_LOC')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/1438042989043_35_20150728002309-00287-ip-10-236-191-2_875026214_2.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 9/64 [02:04<04:32,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Company', 'NUMBER'), ('Headquarters', 'NUMBER'), ('Industry', 'NE_OTHERS'), ('Revenue (billion $)', 'NUMBER'), ('Profits (billion $)', 'NUMBER'), ('Assets (billion $)', 'NUMBER'), ('Market Value (billion $)', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/14380604_4_3329235705746762392.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 10/64 [02:05<03:27,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NUMBER'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/16767252_0_2409448375013995751.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/64 [02:07<02:46,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'None'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/20135078_0_7570343137119682530.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 12/64 [02:07<02:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('State/Province', 'NE_LOC'), ('Lake Name', 'NE_LOC'), ('File Size', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/21245481_0_8730460088443117515.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13/64 [02:09<01:47,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NE_OTHERS'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/21362676_0_6854186738074119688.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 14/64 [02:11<01:40,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NE_OTHERS'), ('Publisher', 'NUMBER'), ('North America', 'DATETIME'), ('Europe', 'DATETIME'), ('Japan', 'NUMBER'), ('Cost', 'NUMBER'), ('Size', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/22864497_0_8632623712684511496.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 15/64 [02:13<01:38,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Country', 'NE_ORG'), ('Land area (km2) [49]', 'NUMBER'), ('GDP nominal (US$M)[50]', 'NUMBER'), ('Per capita GDP (US$)[51]', 'NUMBER'), ('Population[52]', 'NUMBER'), ('Government[53]', 'NE_OTHERS'), ('Conscription[54]', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/24036779_0_5608105867560183058.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 16/64 [02:14<01:23,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Title', 'NE_PERS'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/25404227_0_2240631045609013057.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 17/64 [02:15<01:07,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ENTITY', 'NE_LOC'), ('CURRENCY', 'NUMBER'), ('ALPHABETIC CODE', 'NUMBER'), ('NUMERIC CODE', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/26310680_0_5150772059999313798.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 18/64 [02:15<00:49,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Saint', 'NE_PERS'), ('Anglican', 'NUMBER'), ('Ethiopian', 'NUMBER'), ('Orthodox', 'NUMBER'), ('Catholic', 'NUMBER'), ('Non-historical', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/28086084_0_3127660530989916727.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 19/64 [02:18<01:20,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Year', 'NUMBER'), ('Game', 'NE_OTHERS'), ('Genre', 'NE_OTHERS'), ('Platform(s)', 'NUMBER'), ('Developer(s)', 'NE_ORG')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/29414811_12_251152470253168163.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 20/64 [02:23<01:56,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Year', 'NUMBER'), ('Game', 'NE_OTHERS'), ('Genre', 'NE_OTHERS'), ('Platform(s)', 'NUMBER'), ('Developer(s)', 'NE_ORG')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/29414811_13_8724394428539174350.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 21/64 [02:28<02:19,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Year', 'NUMBER'), ('Game', 'NE_OTHERS'), ('Genre', 'NE_OTHERS'), ('Platform(s)', 'NE_OTHERS'), ('Developer(s)', 'NE_ORG')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/29414811_2_4773219892816395776.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 22/64 [02:31<02:23,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Year', 'NUMBER'), ('Game', 'NE_OTHERS'), ('Genre', 'NUMBER'), ('Platform(s)', 'NE_OTHERS'), ('Developer(s)', 'NE_ORG')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/29414811_6_8221428333921653560.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 23/64 [02:33<02:01,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Adam av Bremen', 'NE_PERS'), ('1040?85', 'NUMBER'), ('Historieskriver', 'NE_PERS')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/33401079_0_9127583903019856402.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 24/64 [02:35<01:37,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Airport', 'NE_LOC'), ('Location', 'NE_ORG'), ('Code (IATA)', 'NUMBER'), ('Total Passengers', 'NUMBER'), ('% Change', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/34041816_1_4749054164534706977.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 25/64 [02:37<01:28,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NE_ORG'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/35188621_0_6058553107571275232.csv\n",
      "____________________\n",
      "[('Platform', 'NUMBER'), ('Developer', 'NUMBER'), ('Game Title', 'NUMBER'), ('Units Sold (in millions)', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/36102169_0_7739454799295072814.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 27/64 [02:38<00:56,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('State, Province, Territory, Country, etc', 'NE_LOC'), ('Area (square miles)', 'NUMBER'), ('Comments', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/37856682_0_6818907050314633217.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 28/64 [02:39<00:55,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NE_OTHERS'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/38428277_0_1311643810102462607.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 29/64 [02:43<01:15,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Airport', 'NE_OTHERS'), ('Location', 'NE_LOC'), ('Code (IATA/ICAO)', 'None'), ('Total Passengers', 'NUMBER'), ('Rank Change', 'NUMBER'), ('% Change', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/39107734_2_2329160387535788734.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 30/64 [02:45<01:09,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Title', 'NE_ORG'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/39173938_0_7916056990138658530.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 31/64 [02:46<01:01,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Brand', 'NUMBER'), ('Value', 'NUMBER'), ('Industry', 'None')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/39650055_5_7135804139753401681.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 32/64 [02:47<00:50,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('N', 'NUMBER'), ('Title', 'NE_PERS'), ('Year', 'NUMBER'), ('Director', 'NE_PERS'), ('IMDB Rating', 'NUMBER'), ('IMDB Votes', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/39759273_0_1427898308030295194.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 33/64 [02:48<00:36,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'NUMBER'), ('Catcheur', 'NE_PERS'), ('Nombre de r?gnes', 'NUMBER'), ('Jours combin?s', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/40534006_0_4617468856744635526.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 34/64 [02:49<00:36,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Company', 'NE_ORG'), ('Country', 'NE_PERS'), ('Industry', 'NE_ORG'), ('Sales ($bil)', 'NUMBER'), ('Profits ($bil)', 'NUMBER'), ('Assets ($bil)', 'NUMBER'), ('Market Value ($bil)', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/41336118_0_4331895026409635103.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 35/64 [02:49<00:26,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Saint', 'NE_PERS'), ('Anglican', 'NUMBER'), ('Ethiopian', 'NUMBER'), ('Orthodox', 'NUMBER'), ('Catholic', 'NUMBER'), ('Non-historical', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/41480166_0_6681239260286218499.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 36/64 [02:53<00:49,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mathematician', 'NE_PERS'), ('Year of PhD', 'NUMBER'), ('Granting Institution', 'NE_ORG'), ('Supervisor', 'NE_PERS'), ('Thesis', 'NE_OTHERS')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/43237185_1_3636357855502246981.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 37/64 [02:54<00:39,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Player (2011 TBs)', 'NE_PERS'), ('TBs', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/45073662_0_3179937335063201739.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 38/64 [02:56<00:45,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Game', 'NE_OTHERS'), ('Genre', 'NUMBER'), ('Platform', 'NUMBER'), ('Release', 'DATETIME')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/46671561_0_6122315295162029872.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 39/64 [03:00<00:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NE_OTHERS'), ('Year', 'NUMBER'), ('Media Type', 'NUMBER'), ('Director', 'NE_PERS'), ('Writer', 'NE_PERS'), ('Length', 'NUMBER'), ('Matting', 'NE_OTHERS')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/50245608_0_871275842592178099.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 40/64 [03:01<00:50,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Player (2011 Ws)', 'NE_PERS'), ('W', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/50270082_0_444360818941411589.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 41/64 [03:02<00:37,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Unnamed: 0', 'NUMBER'), ('1998 Data Country', 'NE_LOC'), ('GDP Per Capita', 'NUMBER'), ('GDP Growth Rate', 'NUMBER'), ('GDP', 'NUMBER'), ('Inflation', 'NUMBER'), ('Population', 'NUMBER'), ('Popul. Growth', 'NUMBER'), ('Infant Mortality', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/52299421_0_4473286348258170200.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 42/64 [03:04<00:36,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Date', 'DATETIME'), ('Movie', 'NE_OTHERS'), ('Year', 'NUMBER'), ('Director', 'NE_PERS')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/53822652_0_5767892317858575530.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 43/64 [03:04<00:27,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lake Name', 'NE_LOC'), ('Access', 'NUMBER'), ('Acres', 'NUMBER'), ('Class', 'NUMBER'), ('Category', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/53989675_0_8697482470743954630.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 44/64 [03:06<00:31,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sequence', 'NUMBER'), ('Date', 'None'), ('New?', 'NUMBER'), ('Title', 'NUMBER'), ('Country', 'NUMBER'), ('Director', 'NE_LOC'), ('Year', 'NUMBER'), ('Grade', 'NUMBER'), ('Theater', 'NUMBER'), ('Shorts', 'NUMBER'), ('Additional Info', 'NUMBER'), ('Ignore This', 'NUMBER'), ('Unnamed: 12', 'NUMBER'), ('Unnamed: 13', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/54719588_0_8417197176086756912.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 45/64 [03:07<00:27,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Title', 'NE_PERS'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/58891288_0_1117541047012405958.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 46/64 [03:12<00:41,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('English name', 'NE_OTHERS'), ('Latin name', 'NE_OTHERS'), ('Hungarian name', 'None')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/60319454_0_3938426910282115527.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 47/64 [03:16<00:47,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abbreviation', 'None'), ('Species', 'NE_OTHERS'), ('Common Name', 'NE_OTHERS')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/63450419_0_8012592961815711786.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 48/64 [03:17<00:36,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('???O', 'NE_PERS'), ('???[?O', 'NUMBER'), ('?`?[??', 'NE_ORG'), ('????', 'NUMBER'), ('?N', 'NUMBER'), ('??', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/66009064_0_9148652238372261251.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 49/64 [03:20<00:36,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rank', 'NUMBER'), ('Country', 'NE_OTHERS'), ('Population', 'NUMBER'), ('Date of Information', 'NE_OTHERS')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/69537082_0_7789694313271016902.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 50/64 [03:21<00:30,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NE_LOC'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/69881946_0_1105130426898457358.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 51/64 [03:21<00:20,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lake', 'NE_LOC'), ('water temperature', 'NUMBER'), ('water gauge', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/71840765_0_6664391841933033844.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 52/64 [03:26<00:31,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'NE_OTHERS'), ('Developer(s)', 'NE_ORG'), ('Publisher(s)', 'NE_OTHERS'), ('Release Date', 'NE_OTHERS'), ('ESRB', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/75367212_2_2745466355267233390.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 53/64 [03:28<00:26,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NE_OTHERS'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/77694908_0_6083291340991074532.csv\n",
      "____________________\n",
      "[('#', 'NUMBER'), ('Media', 'NUMBER'), ('MIX', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/80588006_0_6965325215443683359.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 55/64 [03:30<00:16,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Fans' Rank\", 'NUMBER'), ('Title', 'NE_OTHERS'), ('Year', 'NUMBER'), ('Director(s)', 'NE_PERS'), ('Overall Rank', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/84548468_0_5955155464119382182.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 56/64 [03:31<00:11,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'NUMBER'), ('Citations', 'NUMBER'), ('Journal', 'NE_ORG')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/84575189_0_6365692015941409487.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 57/64 [03:32<00:09,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rang', 'NUMBER'), ('Name', 'NE_LOC'), ('Staat(en)', 'None'), ('Fläche-(km2)', 'NUMBER'), ('Länge-(km)', 'NUMBER'), ('Max.-Tiefe-(m)', 'NUMBER'), ('Volumen-(km?)', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/8468806_0_4382447409703007384.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 58/64 [03:35<00:10,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Scientific Name', 'NE_OTHERS'), ('Common Names (abridged)', 'NOA'), ('Family', 'NE_OTHERS'), ('Habit', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/88523363_0_8180214313099580515.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 59/64 [03:38<00:10,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Common Name', 'NE_OTHERS'), ('Scientific Name', 'NE_OTHERS'), ('Season', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/90196673_0_5458330029110291950.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 60/64 [03:41<00:09,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'NUMBER'), ('Titel', 'NE_OTHERS'), ('System', 'NUMBER'), ('Reviews', 'NUMBER'), ('Durchschnitt', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/91959037_0_7907661684242014480.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 61/64 [03:43<00:07,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('U.S./Canada Release Date', 'NE_OTHERS'), ('Title', 'NUMBER'), ('Season #', 'NUMBER'), ('Notes', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/9475172_1_1023126399856690702.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 62/64 [03:44<00:03,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 'NUMBER'), ('Media', 'NUMBER'), ('MIX', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/9567241_0_5666388268510912770.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 63/64 [03:44<00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nr.', 'NUMBER'), ('Museum', 'NE_LOC'), ('Country', 'NUMBER'), ('Score *', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/9834884_0_3871985887467090123.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [03:46<00:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('No.', 'NUMBER'), ('Name', 'NE_LOC'), ('Feet', 'NUMBER'), ('Met', 'NUMBER'), ('Grid Ref', 'NUMBER'), ('Area', 'NE_OTHERS'), ('?', 'NUMBER'), ('Trips', 'NUMBER')]\n",
      "./data/Dataset/Dataset/Round1_T2D/tables/99070098_0_2074872741302696997.csv\n",
      "____________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "#tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "target = []\n",
    "column_analysis = ColumnAnalysis()\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "\n",
    "    result = column_analysis.classifiy_columns(df.iloc[1:100])\n",
    "\n",
    "    final = list(zip(df.iloc[2], df.columns, result))\n",
    "    filtered_data = [(item, result) for _, item, result in final]\n",
    "    print(filtered_data)\n",
    "    print(table_file)\n",
    "    print(\"____________________\")\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc0c11-e811-4540-9a74-c29dd551b55f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Round1_T2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfe186-559d-4a4b-a568-eacafc41f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "R1_cea = [item[0]for item in R1_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866bea9-b680-4c47-b179-93f270b6bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Place\",\n",
    "    \"PopulatedPlace\",\n",
    "    \"City\",\n",
    "    \"Country\",\n",
    "    \"Region\",\n",
    "    \"Mountain\",\n",
    "    \"Island\",\n",
    "    \"Lake\",\n",
    "    \"River\",\n",
    "    \"Park\",\n",
    "    \"Building\",\n",
    "    \"HistoricPlace\",\n",
    "    \"Monument\",\n",
    "    \"Bridge\",\n",
    "    \"Road\",\n",
    "    \"Airport\",\n",
    "    \"Person\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"Politician\",\n",
    "    \"Scientist\",\n",
    "    \"Writer\",\n",
    "    \"Actor\",\n",
    "    \"Musician\",\n",
    "    \"MilitaryPerson\",\n",
    "    \"Religious\",\n",
    "    \"Royalty\",\n",
    "    \"Criminal\",\n",
    "    \"Organisation\",\n",
    "    \"Company\",\n",
    "    \"EducationalInstitution\",\n",
    "    \"PoliticalParty\",\n",
    "    \"SportsTeam\",\n",
    "    \"Non-ProfitOrganisation\",\n",
    "    \"GovernmentAgency\",\n",
    "    \"ReligiousOrganisation\",\n",
    "    \"Band\",\n",
    "    \"Library\",\n",
    "    \"Museum\",\n",
    "    \"Hospital\",\n",
    "    \"University\",\n",
    "    \"TradeUnion\"\n",
    "]\n",
    "\n",
    "# Mapping of subtypes to macro classes\n",
    "mapping = {\n",
    "    \"Place\": [\"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\", \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"],\n",
    "    \"Person\": [\"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\", \"Religious\", \"Royalty\", \"Criminal\"],\n",
    "    \"Organisation\": [\"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\", \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\"],\n",
    "    \"Institution\": [\"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab481172-9273-43e5-b4d2-d1011cec6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R1_cea).any()\n",
    "        if NE_flag:\n",
    "            joined_cells = column.str.cat(sep='-')        \n",
    "            doc = nlp(joined_cells)\n",
    "            entities = {\"ORG\": [], \"PERS\": [], \"LOC\": [], \"OTHERS\": []}\n",
    "\n",
    "            # Extract entities and classify them\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    entities[\"ORG\"].append(ent.text)\n",
    "                elif ent.label_ == \"PERSON\":\n",
    "                    entities[\"PERS\"].append(ent.text)\n",
    "                elif ent.label_ == \"GPE\" or ent.label_ == \"FAC\":  # GPE (Geopolitical Entity)\n",
    "                    entities[\"LOC\"].append(ent.text)\n",
    "                else:\n",
    "                    entities[\"OTHERS\"].append(ent.text)\n",
    "            \n",
    "            # Find the key of the longest entities list\n",
    "            longest_key = max(entities, key=lambda k: len(entities[k]))\n",
    "            print(f\"{joined_cells} --> The key of the longest list is '{longest_key}'\")\n",
    "                    \n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408542e-7a90-4b4b-b1a8-74fc2448eed5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Round3_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da63443-b6a5-4060-b097-e6e4a2fb9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)\n",
    "\n",
    "R3_cea = [item[0]for item in R3_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c261e-3ff1-41ca-8cde-34c090d858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R3_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673eea67-ad8c-4b23-99ef-c5f2dc86b1cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c0a1c-045b-4cc6-ac29-d60215b8fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_2T_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf378a-2f88-44c0-a551-843d7dc9bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_2T_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a4ed1-7939-4dfc-9eb4-9c9c0f102ef4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208edf55-99eb-4018-a0a7-031dbd192a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b461089-7f54-4ed5-9aef-669855493e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af83a2d-7e99-4577-85ec-b297fdf376ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c14bf0-1ea7-4856-90a9-e433c1c807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df_def = pd.DataFrame({\n",
    "    'column names': columns,\n",
    "    'median_lengths': median_lengths,\n",
    "    'median_token_counts': median_token_counts,\n",
    "    'average_numeric_counts': average_numeric_counts,\n",
    "    'target': target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c179f-5eb2-4c54-9d77-4639ddb9f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_def.to_csv('./data/NE_lit_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e1e43-0b44-4204-b52a-80be8cce57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#   READ DIRECTLY THE DATASET HERE\n",
    "###################################\n",
    "\n",
    "df = pd.read_csv('./data/NE_lit_dataset.csv')\n",
    "filtered_df = df[df['target'].isin(['lit', 'NE'])]\n",
    "\n",
    "# Displaying the filtered DataFrame\n",
    "df[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b29d6-834d-47ce-bbf7-eac0d736e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = df['target'].value_counts()\n",
    "\n",
    "# Extract counts for specific values\n",
    "ne_count = target_counts.get(\"NE\", 0)\n",
    "lit_count = target_counts.get(\"lit\", 0)\n",
    "none_count = df.shape[0] - (ne_count+lit_count)\n",
    "\n",
    "print(f\"Count of 'NE': {ne_count}\")\n",
    "print(f\"Count of 'lit': {lit_count}\")\n",
    "print(f\"Count of 'NaN': {none_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b0060-e2de-4d00-a043-56bc1709897d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2256078-4db3-4a4a-98c0-1e49c19b7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df['target'] = label_encoder.fit_transform(filtered_df['target'])\n",
    "\n",
    "# One-hot encode the 'column names' column\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = one_hot_encoder.fit_transform(filtered_df[['column names']])\n",
    "\n",
    "# Combine the encoded categorical data with the numeric data\n",
    "numeric_data = filtered_df[['median_lengths', 'median_token_counts', 'average_numeric_counts']].values\n",
    "X = np.hstack([encoded_columns, numeric_data])\n",
    "y = filtered_df['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28204b07-3a07-4b09-8748-ec0d4f4eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "mapped_predictions = [\"lit\" if pred == 1 else \"NE\" for pred in y_pred.ravel()]\n",
    "\n",
    "# Extract the part of X_test that corresponds to the one-hot encoded columns\n",
    "encoded_columns_test = X_test[:, :encoded_columns.shape[1]]\n",
    "\n",
    "# Inverse transform the one-hot encoded columns to get the original categorical labels\n",
    "original_labels = one_hot_encoder.inverse_transform(encoded_columns_test)\n",
    "\n",
    "# Print a few examples to check\n",
    "for i in range(100):\n",
    "    print(f'Original label: {original_labels[i]}, Predicted: {mapped_predictions[i]}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06397d40-ffd2-4fc2-a622-3a3338c45d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
