{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38adf943-dbc5-479c-b917-ef8252f9ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import Counter\n",
    "\n",
    "# GLiNER related imports (assuming 'gliner' is a valid package)\n",
    "from gliner import GLiNER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc9fb00-45b6-40a6-8824-ad8edc6735bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnAnalysis:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.entity_type_dict = {\n",
    "            \"PERSON\": \"NE\",\n",
    "            \"NORP\": \"NE\",\n",
    "            \"FAC\": \"NE\",\n",
    "            \"ORG\": \"NE\",\n",
    "            \"GPE\": \"NE\",\n",
    "            \"LOC\": \"NE\",\n",
    "            \"PRODUCT\": \"NE\",\n",
    "            \"EVENT\": \"NE\",\n",
    "            \"WORK_OF_ART\": \"NE\",\n",
    "            \"LAW\": \"NE\",\n",
    "            \"LANGUAGE\": \"NE\",\n",
    "            \"DATE\": \"LIT\",\n",
    "            \"TIME\": \"LIT\",\n",
    "            \"PERCENT\": \"LIT\",\n",
    "            \"MONEY\": \"LIT\",\n",
    "            \"QUANTITY\": \"LIT\",\n",
    "            \"ORDINAL\": \"LIT\",\n",
    "            \"CARDINAL\": \"LIT\",\n",
    "            \"URL\": \"LIT\",\n",
    "            \"DESC\": \"LIT\",\n",
    "            \"TOKEN\": \"NE\",\n",
    "            \"INTEGER\": \"LIT\",\n",
    "            \"FLOAT\": \"LIT\",\n",
    "            \"DATETIME\": \"LIT\",\n",
    "            \"ADDRESS\": \"LIT\",\n",
    "            \"EMAIL\": \"LIT\"\n",
    "        }\n",
    "\n",
    "        self.LIT_DATATYPE = {\n",
    "            \"DATE\": \"DATETIME\", \n",
    "            \"TIME\": \"STRING\", \n",
    "            \"PERCENT\": \"STRING\", \n",
    "            \"MONEY\": \"STRING\", \n",
    "            \"QUANTITY\": \"STRING\", \n",
    "            \"ORDINAL\": \"NUMBER\", \n",
    "            \"CARDINAL\": \"NUMBER\", \n",
    "            \"URL\": \"STRING\",\n",
    "            \"DESC\": \"STRING\",\n",
    "            \"TOKEN\": \"STRING\",\n",
    "            \"INTEGER\": \"NUMBER\",\n",
    "            \"FLOAT\": \"NUMBER\",\n",
    "            \"DATETIME\": \"DATETIME\",\n",
    "            \"ADDRESS\": \"STRING\",\n",
    "            \"EMAIL\": \"STRING\",\n",
    "            \"STRING\": \"STRING\"\n",
    "        }\n",
    "\n",
    "        self.NE_DATATYPE = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"]\n",
    "    \n",
    "    def most_frequent_element(self, input_list):\n",
    "        counter = Counter(input_list)\n",
    "        most_common = counter.most_common(1)\n",
    "        return most_common[0][0] if most_common else None\n",
    "\n",
    "    def extract_number_features(self, column):\n",
    "        try:\n",
    "            col = pd.to_numeric(column, errors='coerce')\n",
    "            return {\n",
    "                'min_value': np.min(col),\n",
    "                'max_value': np.max(col),\n",
    "                'mean_value': np.mean(col),\n",
    "                'std_dev': np.std(col),\n",
    "                'unique_count': len(set(col))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting number features: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def extract_named_entity_features(self, column):\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'all_caps': sum(1 for entry in column if str(entry).isupper()),\n",
    "            'capitalized': sum(1 for entry in column if str(entry).istitle()),\n",
    "            'hyphens': sum(str(entry).count('-') for entry in column),\n",
    "            'periods': sum(str(entry).count('.') for entry in column),\n",
    "            'commas': sum(str(entry).count(',') for entry in column)\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_string_features(self, column):\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'all_caps': sum(1 for entry in column if str(entry).isupper()),\n",
    "            'capitalized': sum(1 for entry in column if str(entry).istitle()),\n",
    "            'alphabetic_chars': sum(char.isalpha() for entry in column for char in str(entry)),\n",
    "            'digit_chars': sum(char.isdigit() for entry in column for char in str(entry)),\n",
    "            'special_chars': sum(not char.isalnum() for entry in column for char in str(entry))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_datetime_features(self, column):\n",
    "        dates = pd.to_datetime(column, errors='coerce')\n",
    "        features = {\n",
    "            'min_date': dates.min(),\n",
    "            'max_date': dates.max(),\n",
    "            'year_counts': dates.dt.year.value_counts().to_dict(),\n",
    "            'month_counts': dates.dt.month.value_counts().to_dict()\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_url_features(self, column):\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'valid_urls': sum(1 for entry in column if re.match(url_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_address_features(self, column):\n",
    "        address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'address_count': sum(1 for entry in column if re.match(address_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_email_features(self, column):\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'valid_emails': sum(1 for entry in column if re.match(email_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    async def fetch_entity(self, session, cell):\n",
    "        if cell is None or pd.isna(cell):\n",
    "            return None\n",
    "        cell = str(cell)\n",
    "        url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "        params = {\n",
    "            'name': cell,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            #'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{cell}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            #'sort': [\n",
    "            #    f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            #]\n",
    "        }\n",
    "        async with session.get(url, params=params, ssl=False, timeout=100) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            return None\n",
    "\n",
    "    async def classify_columns_async(self, df):\n",
    "        def combine_scores(j_score, ed_score, w1=0.5, w2=0.5):\n",
    "            return w1 * j_score + w2 * ed_score\n",
    "\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        datetime_pattern = re.compile(\n",
    "            r'(?:\\d{4}-\\d{2}-\\d{2})'  # YYYY-MM-DD format\n",
    "            r'|(?:31(?:\\/|-|\\.)0?[13578]|1[02](?:\\/|-|\\.)\\d{4})'  # 31 days months\n",
    "            r'|(?:29|30(?:\\/|-|\\.)0?[1,3-9]|1[0-2](?:\\/|-|\\.)\\d{4})'  # 29/30 days months\n",
    "            r'|(?:0?[1-9]|[12]\\d|3[01])(?:\\/|-|\\.)'  # Day\n",
    "            r'(?:0?[1-9]|1[0-2])(?:\\/|-|\\.)\\d{4}'  # Month\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/(?:\\d{2})'  # MM/DD/YY format\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/\\d{2}'  # MM/DD/YY format\n",
    "            r'\\b\\d{2}/(?:0?[1-9]|[12]\\d|3[01])/(?:0?[1-9]|1[0-2])\\b'  # YY/DD/MM format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d\\.[0-5]\\d'  # HH:MM.SS format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d'  # HH:MM format\n",
    "            r'|(?:[0-5]?\\d):[0-5]\\d(?:\\.\\d{1,2})?'  # H:MM or H:MM.S format\n",
    "            r'|(?:2[0-3]|[01]?\\d)h[0-5]?\\d(?:m[0-5]?\\d(?:\\.\\d{1,2})?s)?',  # HhMMmSSs format\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        col_type = []\n",
    "        feature_list = []\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for col_name, col_data in df.items():\n",
    "                type = []\n",
    "                count_cell = 0\n",
    "\n",
    "                for cell in col_data:\n",
    "                    label = None\n",
    "                    is_number = False\n",
    "    \n",
    "                    try:\n",
    "                        if math.isnan(cell):\n",
    "                            label = \"None\"\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                    if isinstance(cell, str):\n",
    "                        if cell == \"NaN\" or cell == \"nan\":\n",
    "                            label = \"None\"\n",
    "                        elif re.match(url_pattern, cell):\n",
    "                            label = \"URL\"\n",
    "                        elif re.match(email_pattern, cell):\n",
    "                            label = \"EMAIL\"\n",
    "                        elif re.match(address_pattern, cell):\n",
    "                            label = \"ADDRESS\"\n",
    "                        elif re.match(datetime_pattern, cell):\n",
    "                            label = \"DATETIME\"\n",
    "                    \n",
    "                    if label is None:  # if it's none of the types below\n",
    "                        try:\n",
    "                            cell_str = str(cell)\n",
    "                            if ',' in cell_str or '.' in cell_str or '%' in cell_str or '$' in cell_str:\n",
    "                                cell_str = cell_str.replace('.', '').replace(',', '').replace('%', '').replace('$', '')\n",
    "                            if len(cell_str) - len(re.findall(r'\\d', cell_str)) < 5 and len(re.findall(r'\\d', cell_str)) != 0:\n",
    "                                is_number = True\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    if is_number:\n",
    "                        label = \"NUMBER\"\n",
    "                    elif isinstance(cell, bool):\n",
    "                        label = \"STRING\"\n",
    "                    elif label != \"None\" and len(cell.split(\" \")) >= 15:\n",
    "                        label = \"NOA\"\n",
    "                    elif label != \"None\" and len(cell.split(\" \")) >= 1 and len(cell) <= 4:\n",
    "                        label = \"STRING\"\n",
    "                    \n",
    "                    if label is not None:\n",
    "                        type.append(label)\n",
    "                    else:\n",
    "                        if count_cell > 5:\n",
    "                            type.append(\"STRING\")\n",
    "                            break  \n",
    "                        else:                \n",
    "                            tasks = [self.fetch_entity(session, cell) for cell in col_data if cell is not None and count_cell <= 5]\n",
    "                            responses = await asyncio.gather(*tasks)\n",
    "                            \n",
    "                            for cell, data in zip(col_data, responses):\n",
    "                                #print(f\"{cell}-->{data[0]}\")\n",
    "                                try:\n",
    "                                    if data and len(data) > 0 and data[0]['NERtype'] != None:\n",
    "                                        if combine_scores(data[0]['jaccard_score'], data[0]['ed_score']) >= 0.7:\n",
    "                                            #type.append(f\"NE_{data[0]['NERtype']}\")\n",
    "                                            type.append(f\"NE\")\n",
    "                                    else:\n",
    "                                        # if you didn't find a NER type for this i2tem\n",
    "                                        type.append(\"STRING\")\n",
    "                                    count_cell += 1\n",
    "                                except:\n",
    "                                    continue\n",
    "\n",
    "                most_common_type = self.most_frequent_element(type)\n",
    "                col_type.append(most_common_type)\n",
    "\n",
    "                if most_common_type == \"NUMBER\":\n",
    "                    features = self.extract_number_features(col_data)\n",
    "                elif most_common_type in ['NE_PERS', 'NE_LOC', 'NE_ORG', 'NE_OTHERS']:\n",
    "                    features = self.extract_named_entity_features(col_data)\n",
    "                elif most_common_type == \"STRING\" or most_common_type == \"NOA\":\n",
    "                    features = self.extract_string_features(col_data)\n",
    "                elif most_common_type == \"DATETIME\":\n",
    "                    features = self.extract_datetime_features(col_data)\n",
    "                elif most_common_type == \"URL\":\n",
    "                    features = self.extract_url_features(col_data)\n",
    "                elif most_common_type == \"ADDRESS\":\n",
    "                    features = self.extract_address_features(col_data)\n",
    "                elif most_common_type == \"EMAIL\":\n",
    "                    features = self.extract_email_features(col_data)\n",
    "                else:\n",
    "                    features = {}\n",
    "\n",
    "                features['column_name'] = col_name\n",
    "                features['column_type'] = most_common_type\n",
    "                return most_common_type\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "    def classify_columns(self, df):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(self.classify_columns_async(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695dac20-8e6b-4871-a7c1-64ed2ca98535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 and df2 are the same\n",
    "\n",
    "df1 = pd.read_csv(\"./R1_train_df.csv\")\n",
    "df2 = pd.read_csv(\"./R3_train_df.csv\")\n",
    "df3 = pd.read_csv(\"./R4_train_df.csv\")\n",
    "df4 = pd.read_csv(\"./HT2_train_df.csv\")\n",
    "\n",
    "# filtering because otherwise the model gets values too high\n",
    "df3 = df3[(df3['max_value'] <= 1.000000e+10) ]\n",
    "\n",
    "result = pd.concat([df3, df4, df1, df2], axis=0)\n",
    "result.drop(['date_range', 'year_counts', 'month_counts'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "formats = ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "for fmt in formats:\n",
    "    result['min_date'] = pd.to_datetime(result['min_date'], format=fmt, errors='coerce')\n",
    "    result['max_date'] = pd.to_datetime(result['max_date'], format=fmt, errors='coerce')\n",
    "\n",
    "\n",
    "result['min_month'] = result['min_date'].dt.month\n",
    "result['min_year'] = result['min_date'].dt.year\n",
    "result['max_month'] = result['max_date'].dt.month\n",
    "result['max_year'] = result['max_date'].dt.year\n",
    "result = result.dropna(subset=['column_type'])\n",
    "\n",
    "\n",
    "result.iloc[:, 2:26] = result.iloc[:, 2:26].fillna(-1) \n",
    "result.iloc[:, 26:30] = result.iloc[:, 26:30].fillna(0)  # fill the ['min_month', 'min_year', 'max_month', 'max_year'] \n",
    "\n",
    "X = result.drop(['max_date', 'min_date', 'column_name', 'column_type'], axis=1)  # Drop the target column from features\n",
    "y = label_encoder.fit_transform(result['column_type'].values)\n",
    "\n",
    "# One-hot encode the target variable for multiclass classification\n",
    "y = to_categorical(y)\n",
    "\n",
    "\n",
    "# see how imbalanced is the dataset\n",
    "result.groupby('column_type').size().reset_index(name='count')\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868eb03-18e7-4ef9-bfcc-7d2cc5daed30",
   "metadata": {},
   "source": [
    "# NE - LIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdbbe0fe-2447-400d-a37d-80f11f2177dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_strings = [\n",
    "    \"2023-02-15 08:30:00\",\n",
    "    \"2023-03-20 18:45:00\",\n",
    "    \"2023-04-10 10:00:00\",\n",
    "    \"2023-05-05 14:20:00\"\n",
    "]\n",
    "\n",
    "people = [\n",
    "    \"John Smith\",\n",
    "    \"Mary Johnson\",\n",
    "    \"James Williams\",\n",
    "    \"Patricia Brown\",\n",
    "    \"Michael Davis\",\n",
    "    \"Jennifer Miller\",\n",
    "    \"William Wilson\",\n",
    "    \"Linda Moore\",\n",
    "    \"David Taylor\",\n",
    "    \"Barbara Anderson\"\n",
    "]\n",
    "\n",
    "cities = [\n",
    "    \"Tokyo lake\",\n",
    "    \"New York City lake\",\n",
    "    \"Paris lake\",\n",
    "    \"London lake\",\n",
    "    \"Dubai lake\",\n",
    "    \"Singapore\",\n",
    "    \"Sydney\",\n",
    "    \"Berlin\",\n",
    "    \"Hong Kong\",\n",
    "    \"Rio de Janeiro\"\n",
    "]\n",
    "\n",
    "data = [\n",
    "    \"Shadows moving quietly, secrets hidden in the night.\",\n",
    "    \"Warm sounds lingering like sunlight.\",\n",
    "    \"Quiet thoughts drifting like clouds.\",\n",
    "    \"The soft shift between day and night.\",\n",
    "    \"Delicate, clear sounds like winter chimes.\",\n",
    "    \"Faint murmurs, as soft as moonlight.\",\n",
    "    \"A cool breeze carrying distant dreams.\",\n",
    "    \"Dreams that echo long after waking.\",\n",
    "    \"A calm horizon meeting the earth.\",\n",
    "    \"Flames flickering in a graceful rhythm.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69076844-79a6-486f-8748-6bdacf346523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.DataFrame({'desc': [\"minore di 3 anni\" for i in range(0, 10)]})\n",
    "#df = pd.DataFrame({'desc': datetime_strings})\n",
    "df = pd.DataFrame({'desc': cities})\n",
    "column_analysis = ColumnAnalysis()\n",
    "df_feat = await column_analysis.classify_columns_async(df)\n",
    "\n",
    "columns = [\n",
    "    'min_value', 'max_value', 'mean_value', 'std_dev', 'unique_count', 'special_values',\n",
    "    'average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens', 'periods', 'commas', 'common_prefixes', 'common_suffixes',\n",
    "    'alphabetic_chars', 'digit_chars', 'special_chars', 'valid_urls', 'address_count', 'valid_emails', 'min_date',\n",
    "    'max_date', 'date_range', 'year_counts', 'month_counts'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eacca11f-5d2c-4c49-b391-f0e7b3a31256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NE'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a263b54-9c88-4481-9a57-860acef0f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a single GLiNER model instance\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17887db3-ccca-4dc4-b3f7-011dcc10340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC\n"
     ]
    }
   ],
   "source": [
    "# Define your label mapping\n",
    "label_mapping = {\n",
    "    \"person\": \"PERS\",\n",
    "    \"organization\": \"ORG\",\n",
    "    \"location\": \"LOC\",\n",
    "    \"film\": \"OTHERS\",\n",
    "    \"others\": \"OTHERS\",\n",
    "    \"videogames\": \"OTHERS\",\n",
    "    \"date\": \"OTHERS\",\n",
    "    \"galaxy\": \"OTHERS\",\n",
    "    \"species\": \"OTHERS\"\n",
    "}\n",
    "\n",
    "# Define the labels that the model should predict\n",
    "new_labels = [\n",
    "    \"person\", \"organization\", \"location\", \"others\", \"film\", \"videogames\", \"species\", \"date\", \"galaxy\"\n",
    "]\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_ner_types(text_list):\n",
    "    results = []\n",
    "\n",
    "    for text in text_list:\n",
    "        # Predict entities for the text\n",
    "        entities = model.predict_entities(text, new_labels, threshold=0.3)\n",
    "\n",
    "        # Map the predicted entities to the target labels\n",
    "        mapped_entities = []\n",
    "        for entity in entities:\n",
    "            mapped_entities.append({\n",
    "                'text': text,\n",
    "                'entity_text': entity['text'],\n",
    "                'start': entity['start'],\n",
    "                'end': entity['end'],\n",
    "                'prediction': label_mapping.get(entity['label'], 'UNKNOWN')\n",
    "            })\n",
    "\n",
    "        results.extend(mapped_entities)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Get predictions\n",
    "result_df = predict_ner_types(cities)\n",
    "\n",
    "# Display the result\n",
    "counter = Counter(result_df['prediction'])\n",
    "most_common_elements = counter.most_common(1)[0]\n",
    "\n",
    "print(most_common_elements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffb34475-51ee-4261-8c30-5b9aa3320618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokyo lake',\n",
       " 'New York City lake',\n",
       " 'Paris lake',\n",
       " 'London lake',\n",
       " 'Dubai lake',\n",
       " 'Singapore',\n",
       " 'Sydney',\n",
       " 'Berlin',\n",
       " 'Hong Kong',\n",
       " 'Rio de Janeiro']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
