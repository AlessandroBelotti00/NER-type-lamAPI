{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ecb2c1-051e-45cc-a88d-2b17e81268ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GLiNER\n",
      "  Downloading gliner-0.2.13-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting torch>=2.0.0 (from GLiNER)\n",
      "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers>=4.38.2 (from GLiNER)\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.21.4 (from GLiNER)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from GLiNER) (4.66.1)\n",
      "Collecting onnxruntime (from GLiNER)\n",
      "  Downloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting sentencepiece (from GLiNER)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.21.4->GLiNER)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->GLiNER) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->GLiNER) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->GLiNER) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->GLiNER) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->GLiNER) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->GLiNER) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->GLiNER) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->GLiNER) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=2.0.0->GLiNER)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->GLiNER)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.2->GLiNER) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.2->GLiNER) (2023.12.25)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.38.2->GLiNER)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.38.2->GLiNER)\n",
      "  Downloading tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting coloredlogs (from onnxruntime->GLiNER)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.11/site-packages (from onnxruntime->GLiNER) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from onnxruntime->GLiNER) (4.24.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->GLiNER)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->GLiNER) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->GLiNER) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->GLiNER) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->GLiNER) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->GLiNER) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.0->GLiNER) (1.3.0)\n",
      "Downloading gliner-0.2.13-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m526.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m812.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m409.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m880.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, coloredlogs, tokenizers, onnxruntime, nvidia-cusolver-cu12, transformers, torch, GLiNER\n",
      "Successfully installed GLiNER-0.2.13 coloredlogs-15.0.1 filelock-3.16.1 huggingface-hub-0.25.1 humanfriendly-10.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 onnxruntime-1.19.2 safetensors-0.4.5 sentencepiece-0.2.0 tokenizers-0.20.0 torch-2.4.1 transformers-4.45.1 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install GLiNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38adf943-dbc5-479c-b917-ef8252f9ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import Counter\n",
    "\n",
    "# GLiNER related imports (assuming 'gliner' is a valid package)\n",
    "from gliner import GLiNER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9fb00-45b6-40a6-8824-ad8edc6735bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnAnalysis:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.entity_type_dict = {\n",
    "            \"PERSON\": \"NE\",\n",
    "            \"NORP\": \"NE\",\n",
    "            \"FAC\": \"NE\",\n",
    "            \"ORG\": \"NE\",\n",
    "            \"GPE\": \"NE\",\n",
    "            \"LOC\": \"NE\",\n",
    "            \"PRODUCT\": \"NE\",\n",
    "            \"EVENT\": \"NE\",\n",
    "            \"WORK_OF_ART\": \"NE\",\n",
    "            \"LAW\": \"NE\",\n",
    "            \"LANGUAGE\": \"NE\",\n",
    "            \"DATE\": \"LIT\",\n",
    "            \"TIME\": \"LIT\",\n",
    "            \"PERCENT\": \"LIT\",\n",
    "            \"MONEY\": \"LIT\",\n",
    "            \"QUANTITY\": \"LIT\",\n",
    "            \"ORDINAL\": \"LIT\",\n",
    "            \"CARDINAL\": \"LIT\",\n",
    "            \"URL\": \"LIT\",\n",
    "            \"DESC\": \"LIT\",\n",
    "            \"TOKEN\": \"NE\",\n",
    "            \"INTEGER\": \"LIT\",\n",
    "            \"FLOAT\": \"LIT\",\n",
    "            \"DATETIME\": \"LIT\",\n",
    "            \"ADDRESS\": \"LIT\",\n",
    "            \"EMAIL\": \"LIT\"\n",
    "        }\n",
    "\n",
    "        self.LIT_DATATYPE = {\n",
    "            \"DATE\": \"DATETIME\", \n",
    "            \"TIME\": \"STRING\", \n",
    "            \"PERCENT\": \"STRING\", \n",
    "            \"MONEY\": \"STRING\", \n",
    "            \"QUANTITY\": \"STRING\", \n",
    "            \"ORDINAL\": \"NUMBER\", \n",
    "            \"CARDINAL\": \"NUMBER\", \n",
    "            \"URL\": \"STRING\",\n",
    "            \"DESC\": \"STRING\",\n",
    "            \"TOKEN\": \"STRING\",\n",
    "            \"INTEGER\": \"NUMBER\",\n",
    "            \"FLOAT\": \"NUMBER\",\n",
    "            \"DATETIME\": \"DATETIME\",\n",
    "            \"ADDRESS\": \"STRING\",\n",
    "            \"EMAIL\": \"STRING\",\n",
    "            \"STRING\": \"STRING\"\n",
    "        }\n",
    "\n",
    "        self.NE_DATATYPE = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"]\n",
    "    \n",
    "    def most_frequent_element(self, input_list):\n",
    "        counter = Counter(input_list)\n",
    "        most_common = counter.most_common(1)\n",
    "        return most_common[0][0] if most_common else None\n",
    "\n",
    "    def extract_number_features(self, column):\n",
    "        try:\n",
    "            col = pd.to_numeric(column, errors='coerce')\n",
    "            return {\n",
    "                'min_value': np.min(col),\n",
    "                'max_value': np.max(col),\n",
    "                'mean_value': np.mean(col),\n",
    "                'std_dev': np.std(col),\n",
    "                'unique_count': len(set(col))\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting number features: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def extract_named_entity_features(self, column):\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'all_caps': sum(1 for entry in column if str(entry).isupper()),\n",
    "            'capitalized': sum(1 for entry in column if str(entry).istitle()),\n",
    "            'hyphens': sum(str(entry).count('-') for entry in column),\n",
    "            'periods': sum(str(entry).count('.') for entry in column),\n",
    "            'commas': sum(str(entry).count(',') for entry in column)\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_string_features(self, column):\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'all_caps': sum(1 for entry in column if str(entry).isupper()),\n",
    "            'capitalized': sum(1 for entry in column if str(entry).istitle()),\n",
    "            'alphabetic_chars': sum(char.isalpha() for entry in column for char in str(entry)),\n",
    "            'digit_chars': sum(char.isdigit() for entry in column for char in str(entry)),\n",
    "            'special_chars': sum(not char.isalnum() for entry in column for char in str(entry))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_datetime_features(self, column):\n",
    "        dates = pd.to_datetime(column, errors='coerce')\n",
    "        features = {\n",
    "            'min_date': dates.min(),\n",
    "            'max_date': dates.max(),\n",
    "            'year_counts': dates.dt.year.value_counts().to_dict(),\n",
    "            'month_counts': dates.dt.month.value_counts().to_dict()\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_url_features(self, column):\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'valid_urls': sum(1 for entry in column if re.match(url_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_address_features(self, column):\n",
    "        address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'address_count': sum(1 for entry in column if re.match(address_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    def extract_email_features(self, column):\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        lengths = [len(str(entry)) for entry in column]\n",
    "        features = {\n",
    "            'average_length': np.mean(lengths) if lengths else 0,\n",
    "            'min_length': np.min(lengths) if lengths else 0,\n",
    "            'max_length': np.max(lengths) if lengths else 0,\n",
    "            'valid_emails': sum(1 for entry in column if re.match(email_pattern, str(entry)))\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    async def fetch_entity(self, session, cell):\n",
    "        if cell is None or pd.isna(cell):\n",
    "            return None\n",
    "        cell = str(cell)\n",
    "        url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "        params = {\n",
    "            'name': cell,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            #'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{cell}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            #'sort': [\n",
    "            #    f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            #]\n",
    "        }\n",
    "        async with session.get(url, params=params, ssl=False, timeout=100) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            return None\n",
    "\n",
    "    async def classify_columns_async(self, df):\n",
    "        def combine_scores(j_score, ed_score, w1=0.5, w2=0.5):\n",
    "            return w1 * j_score + w2 * ed_score\n",
    "\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        address_pattern = re.compile(r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        datetime_pattern = re.compile(\n",
    "            r'(?:\\d{4}-\\d{2}-\\d{2})'  # YYYY-MM-DD format\n",
    "            r'|(?:31(?:\\/|-|\\.)0?[13578]|1[02](?:\\/|-|\\.)\\d{4})'  # 31 days months\n",
    "            r'|(?:29|30(?:\\/|-|\\.)0?[1,3-9]|1[0-2](?:\\/|-|\\.)\\d{4})'  # 29/30 days months\n",
    "            r'|(?:0?[1-9]|[12]\\d|3[01])(?:\\/|-|\\.)'  # Day\n",
    "            r'(?:0?[1-9]|1[0-2])(?:\\/|-|\\.)\\d{4}'  # Month\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/(?:\\d{2})'  # MM/DD/YY format\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/\\d{2}'  # MM/DD/YY format\n",
    "            r'\\b\\d{2}/(?:0?[1-9]|[12]\\d|3[01])/(?:0?[1-9]|1[0-2])\\b'  # YY/DD/MM format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d\\.[0-5]\\d'  # HH:MM.SS format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d'  # HH:MM format\n",
    "            r'|(?:[0-5]?\\d):[0-5]\\d(?:\\.\\d{1,2})?'  # H:MM or H:MM.S format\n",
    "            r'|(?:2[0-3]|[01]?\\d)h[0-5]?\\d(?:m[0-5]?\\d(?:\\.\\d{1,2})?s)?',  # HhMMmSSs format\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        col_type = []\n",
    "        feature_list = []\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for col_name, col_data in df.items():\n",
    "                type = []\n",
    "                count_cell = 0\n",
    "\n",
    "                for cell in col_data:\n",
    "                    label = None\n",
    "                    is_number = False\n",
    "    \n",
    "                    try:\n",
    "                        if math.isnan(cell):\n",
    "                            label = \"None\"\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                    if isinstance(cell, str):\n",
    "                        if cell == \"NaN\" or cell == \"nan\":\n",
    "                            label = \"None\"\n",
    "                        elif re.match(url_pattern, cell):\n",
    "                            label = \"URL\"\n",
    "                        elif re.match(email_pattern, cell):\n",
    "                            label = \"EMAIL\"\n",
    "                        elif re.match(address_pattern, cell):\n",
    "                            label = \"ADDRESS\"\n",
    "                        elif re.match(datetime_pattern, cell):\n",
    "                            label = \"DATETIME\"\n",
    "                    \n",
    "                    if label is None:  # if it's none of the types below\n",
    "                        try:\n",
    "                            cell_str = str(cell)\n",
    "                            if ',' in cell_str or '.' in cell_str or '%' in cell_str or '$' in cell_str:\n",
    "                                cell_str = cell_str.replace('.', '').replace(',', '').replace('%', '').replace('$', '')\n",
    "                            if len(cell_str) - len(re.findall(r'\\d', cell_str)) < 5 and len(re.findall(r'\\d', cell_str)) != 0:\n",
    "                                is_number = True\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    if is_number:\n",
    "                        label = \"NUMBER\"\n",
    "                    elif isinstance(cell, bool):\n",
    "                        label = \"STRING\"\n",
    "                    elif label != \"None\" and len(cell.split(\" \")) >= 15:\n",
    "                        label = \"NOA\"\n",
    "                    elif label != \"None\" and len(cell.split(\" \")) >= 1 and len(cell) <= 4:\n",
    "                        label = \"STRING\"\n",
    "                    \n",
    "                    if label is not None:\n",
    "                        type.append(label)\n",
    "                    else:\n",
    "                        if count_cell > 5:\n",
    "                            type.append(\"STRING\")\n",
    "                            break  \n",
    "                        else:                \n",
    "                            tasks = [self.fetch_entity(session, cell) for cell in col_data if cell is not None and count_cell <= 5]\n",
    "                            responses = await asyncio.gather(*tasks)\n",
    "                            \n",
    "                            for cell, data in zip(col_data, responses):\n",
    "                                #print(f\"{cell}-->{data[0]}\")\n",
    "                                try:\n",
    "                                    if data and len(data) > 0 and data[0]['NERtype'] != None:\n",
    "                                        if combine_scores(data[0]['jaccard_score'], data[0]['ed_score']) >= 0.7:\n",
    "                                            #type.append(f\"NE_{data[0]['NERtype']}\")\n",
    "                                            type.append(f\"NE\")\n",
    "                                    else:\n",
    "                                        # if you didn't find a NER type for this i2tem\n",
    "                                        type.append(\"STRING\")\n",
    "                                    count_cell += 1\n",
    "                                except:\n",
    "                                    continue\n",
    "\n",
    "                most_common_type = self.most_frequent_element(type)\n",
    "                col_type.append(most_common_type)\n",
    "\n",
    "                if most_common_type == \"NUMBER\":\n",
    "                    features = self.extract_number_features(col_data)\n",
    "                elif most_common_type in ['NE_PERS', 'NE_LOC', 'NE_ORG', 'NE_OTHERS']:\n",
    "                    features = self.extract_named_entity_features(col_data)\n",
    "                elif most_common_type == \"STRING\" or most_common_type == \"NOA\":\n",
    "                    features = self.extract_string_features(col_data)\n",
    "                elif most_common_type == \"DATETIME\":\n",
    "                    features = self.extract_datetime_features(col_data)\n",
    "                elif most_common_type == \"URL\":\n",
    "                    features = self.extract_url_features(col_data)\n",
    "                elif most_common_type == \"ADDRESS\":\n",
    "                    features = self.extract_address_features(col_data)\n",
    "                elif most_common_type == \"EMAIL\":\n",
    "                    features = self.extract_email_features(col_data)\n",
    "                else:\n",
    "                    features = {}\n",
    "\n",
    "                features['column_name'] = col_name\n",
    "                features['column_type'] = most_common_type\n",
    "                return most_common_type\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "    def classify_columns(self, df):\n",
    "        loop = asyncio.get_event_loop()\n",
    "        return loop.run_until_complete(self.classify_columns_async(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695dac20-8e6b-4871-a7c1-64ed2ca98535",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m result\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_counts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_counts\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Convert the target variable to numeric\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mLabelEncoder\u001b[49m()\n\u001b[1;32m     18\u001b[0m formats \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fmt \u001b[38;5;129;01min\u001b[39;00m formats:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# df1 and df2 are the same\n",
    "\n",
    "df1 = pd.read_csv(\"./R1_train_df.csv\")\n",
    "df2 = pd.read_csv(\"./R3_train_df.csv\")\n",
    "df3 = pd.read_csv(\"./R4_train_df.csv\")\n",
    "df4 = pd.read_csv(\"./HT2_train_df.csv\")\n",
    "\n",
    "# filtering because otherwise the model gets values too high\n",
    "df3 = df3[(df3['max_value'] <= 1.000000e+10) ]\n",
    "\n",
    "result = pd.concat([df3, df4, df1, df2], axis=0)\n",
    "result.drop(['date_range', 'year_counts', 'month_counts'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "formats = ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S']\n",
    "\n",
    "for fmt in formats:\n",
    "    result['min_date'] = pd.to_datetime(result['min_date'], format=fmt, errors='coerce')\n",
    "    result['max_date'] = pd.to_datetime(result['max_date'], format=fmt, errors='coerce')\n",
    "\n",
    "\n",
    "result['min_month'] = result['min_date'].dt.month\n",
    "result['min_year'] = result['min_date'].dt.year\n",
    "result['max_month'] = result['max_date'].dt.month\n",
    "result['max_year'] = result['max_date'].dt.year\n",
    "result = result.dropna(subset=['column_type'])\n",
    "\n",
    "\n",
    "result.iloc[:, 2:26] = result.iloc[:, 2:26].fillna(-1) \n",
    "result.iloc[:, 26:30] = result.iloc[:, 26:30].fillna(0)  # fill the ['min_month', 'min_year', 'max_month', 'max_year'] \n",
    "\n",
    "X = result.drop(['max_date', 'min_date', 'column_name', 'column_type'], axis=1)  # Drop the target column from features\n",
    "y = label_encoder.fit_transform(result['column_type'].values)\n",
    "\n",
    "# One-hot encode the target variable for multiclass classification\n",
    "y = to_categorical(y)\n",
    "\n",
    "\n",
    "# see how imbalanced is the dataset\n",
    "result.groupby('column_type').size().reset_index(name='count')\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868eb03-18e7-4ef9-bfcc-7d2cc5daed30",
   "metadata": {},
   "source": [
    "# NE - LIT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdbbe0fe-2447-400d-a37d-80f11f2177dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##      SOME EXAMPLES       ##\n",
    "##############################\n",
    "\n",
    "\n",
    "datetime_strings = [\n",
    "    \"2023-02-15 08:30:00\",\n",
    "    \"2023-03-20 18:45:00\",\n",
    "    \"2023-04-10 10:00:00\",\n",
    "    \"2023-05-05 14:20:00\"\n",
    "]\n",
    "\n",
    "people = [\n",
    "    \"John Smith\",\n",
    "    \"Mary Johnson\",\n",
    "    \"James Williams\",\n",
    "    \"Patricia Brown\",\n",
    "    \"Michael Davis\",\n",
    "    \"Jennifer Miller\",\n",
    "    \"William Wilson\",\n",
    "    \"Linda Moore\",\n",
    "    \"David Taylor\",\n",
    "    \"Barbara Anderson\"\n",
    "]\n",
    "\n",
    "cities = [\n",
    "    \"Tokyo lake\",\n",
    "    \"New York City lake\",\n",
    "    \"Paris lake\",\n",
    "    \"London lake\",\n",
    "    \"Dubai lake\",\n",
    "    \"Singapore\",\n",
    "    \"Sydney\",\n",
    "    \"Berlin\",\n",
    "    \"Hong Kong\",\n",
    "    \"Rio de Janeiro\"\n",
    "]\n",
    "\n",
    "data = [\n",
    "    \"Shadows moving quietly, secrets hidden in the night.\",\n",
    "    \"Warm sounds lingering like sunlight.\",\n",
    "    \"Quiet thoughts drifting like clouds.\",\n",
    "    \"The soft shift between day and night.\",\n",
    "    \"Delicate, clear sounds like winter chimes.\",\n",
    "    \"Faint murmurs, as soft as moonlight.\",\n",
    "    \"A cool breeze carrying distant dreams.\",\n",
    "    \"Dreams that echo long after waking.\",\n",
    "    \"A calm horizon meeting the earth.\",\n",
    "    \"Flames flickering in a graceful rhythm.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7840b49-3a57-4a28-b8ca-a51f72ee1bed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GLiNER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_dev\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_values\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_caps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapitalized\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyphens\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperiods\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommon_prefixes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommon_suffixes\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malphabetic_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdigit_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_chars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_urls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_count\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_emails\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_counts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_counts\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize a single GLiNER model instance\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGLiNER\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murchade/gliner_base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Define your label mapping\u001b[39;00m\n\u001b[1;32m     13\u001b[0m label_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOTHERS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GLiNER' is not defined"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    'min_value', 'max_value', 'mean_value', 'std_dev', 'unique_count', 'special_values',\n",
    "    'average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens', 'periods', 'commas', 'common_prefixes', 'common_suffixes',\n",
    "    'alphabetic_chars', 'digit_chars', 'special_chars', 'valid_urls', 'address_count', 'valid_emails', 'min_date',\n",
    "    'max_date', 'date_range', 'year_counts', 'month_counts'\n",
    "]\n",
    "\n",
    "# Initialize a single GLiNER model instance\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "\n",
    "\n",
    "# Define your label mapping\n",
    "label_mapping = {\n",
    "    \"person\": \"PERS\",\n",
    "    \"organization\": \"ORG\",\n",
    "    \"location\": \"LOC\",\n",
    "    \"film\": \"OTHERS\",\n",
    "    \"others\": \"OTHERS\",\n",
    "    \"videogames\": \"OTHERS\",\n",
    "    \"date\": \"OTHERS\",\n",
    "    \"galaxy\": \"OTHERS\",\n",
    "    \"species\": \"OTHERS\"\n",
    "}\n",
    "\n",
    "# Define the labels that the model should predict\n",
    "new_labels = [\n",
    "    \"person\", \"organization\", \"location\", \"others\", \"film\", \"videogames\", \"species\", \"date\", \"galaxy\"\n",
    "]\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_ner_types(text_list):\n",
    "    results = []\n",
    "\n",
    "    for text in text_list:\n",
    "        # Predict entities for the text\n",
    "        entities = model.predict_entities(text, new_labels, threshold=0.3)\n",
    "\n",
    "        # Map the predicted entities to the target labels\n",
    "        mapped_entities = []\n",
    "        for entity in entities:\n",
    "            mapped_entities.append({\n",
    "                'text': text,\n",
    "                'entity_text': entity['text'],\n",
    "                'start': entity['start'],\n",
    "                'end': entity['end'],\n",
    "                'prediction': label_mapping.get(entity['label'], 'UNKNOWN')\n",
    "            })\n",
    "\n",
    "        results.extend(mapped_entities)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e40fc-5164-451f-b847-62188e29e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##      THE TABLES          ##\n",
    "##############################\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the directory\n",
    "base_dir = r'.\\data\\Dataset\\Dataset\\2T_Round4\\tables'\n",
    "\n",
    "# Use glob to find all CSV files in the directory and subdirectories\n",
    "csv_files = glob.glob(os.path.join(base_dir, '**', '*.csv'), recursive=True)\n",
    "\n",
    "# number of items per column to use in the model\n",
    "len_param = 25\n",
    "\n",
    "# Loop through the list of files with tqdm progress bar\n",
    "for file in tqdm(csv_files, desc='Reading CSV files', unit='file'):\n",
    "    df = pd.read_csv(file, header=None)\n",
    "    print(f\"{file}: \")\n",
    "    for column in combined_df.columns:\n",
    "        if len(column) <= len_param:\n",
    "            c_kind = await column_analysis.classify_columns_async(column)\n",
    "        else:\n",
    "            c_kind = await column_analysis.classify_columns_async(column[:len_param])\n",
    "            \n",
    "        if c_kind != 'NE':\n",
    "            print(f\"Kind of columns: {c_kind}\")\n",
    "        else:\n",
    "            # Get predictions\n",
    "            result_df = predict_ner_types(cities)\n",
    "            \n",
    "            # Display the result\n",
    "            counter = Counter(result_df['prediction'])\n",
    "            most_common_elements = counter.most_common(1)[0]\n",
    "            \n",
    "            print(f\"Kind of columns: {c_kind}_{most_common_elements[0]}\")\n",
    "\n",
    "    print(\"___________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968e692-cbe4-4954-b145-288ed615251f",
   "metadata": {},
   "source": [
    "## Testing single columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69076844-79a6-486f-8748-6bdacf346523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.DataFrame({'desc': [\"minore di 3 anni\" for i in range(0, 10)]})\n",
    "#df = pd.DataFrame({'desc': datetime_strings})\n",
    "df = pd.DataFrame({'desc': cities})\n",
    "column_analysis = ColumnAnalysis()\n",
    "df_feat = await column_analysis.classify_columns_async(df)\n",
    "\n",
    "columns = [\n",
    "    'min_value', 'max_value', 'mean_value', 'std_dev', 'unique_count', 'special_values',\n",
    "    'average_length', 'min_length', 'max_length', 'all_caps', 'capitalized', 'hyphens', 'periods', 'commas', 'common_prefixes', 'common_suffixes',\n",
    "    'alphabetic_chars', 'digit_chars', 'special_chars', 'valid_urls', 'address_count', 'valid_emails', 'min_date',\n",
    "    'max_date', 'date_range', 'year_counts', 'month_counts'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eacca11f-5d2c-4c49-b391-f0e7b3a31256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NE'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a263b54-9c88-4481-9a57-860acef0f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a single GLiNER model instance\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17887db3-ccca-4dc4-b3f7-011dcc10340b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC\n"
     ]
    }
   ],
   "source": [
    "# Define your label mapping\n",
    "label_mapping = {\n",
    "    \"person\": \"PERS\",\n",
    "    \"organization\": \"ORG\",\n",
    "    \"location\": \"LOC\",\n",
    "    \"film\": \"OTHERS\",\n",
    "    \"others\": \"OTHERS\",\n",
    "    \"videogames\": \"OTHERS\",\n",
    "    \"date\": \"OTHERS\",\n",
    "    \"galaxy\": \"OTHERS\",\n",
    "    \"species\": \"OTHERS\"\n",
    "}\n",
    "\n",
    "# Define the labels that the model should predict\n",
    "new_labels = [\n",
    "    \"person\", \"organization\", \"location\", \"others\", \"film\", \"videogames\", \"species\", \"date\", \"galaxy\"\n",
    "]\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_ner_types(text_list):\n",
    "    results = []\n",
    "\n",
    "    for text in text_list:\n",
    "        # Predict entities for the text\n",
    "        entities = model.predict_entities(text, new_labels, threshold=0.3)\n",
    "\n",
    "        # Map the predicted entities to the target labels\n",
    "        mapped_entities = []\n",
    "        for entity in entities:\n",
    "            mapped_entities.append({\n",
    "                'text': text,\n",
    "                'entity_text': entity['text'],\n",
    "                'start': entity['start'],\n",
    "                'end': entity['end'],\n",
    "                'prediction': label_mapping.get(entity['label'], 'UNKNOWN')\n",
    "            })\n",
    "\n",
    "        results.extend(mapped_entities)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Get predictions\n",
    "result_df = predict_ner_types(cities)\n",
    "\n",
    "# Display the result\n",
    "counter = Counter(result_df['prediction'])\n",
    "most_common_elements = counter.most_common(1)[0]\n",
    "\n",
    "print(most_common_elements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffb34475-51ee-4261-8c30-5b9aa3320618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokyo lake',\n",
       " 'New York City lake',\n",
       " 'Paris lake',\n",
       " 'London lake',\n",
       " 'Dubai lake',\n",
       " 'Singapore',\n",
       " 'Sydney',\n",
       " 'Berlin',\n",
       " 'Hong Kong',\n",
       " 'Rio de Janeiro']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
