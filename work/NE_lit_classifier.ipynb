{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4fc3a-0dce-4366-907b-927dac9615af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be3fbb-0e7a-44c2-8807-381e580db097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec340f-ef96-4940-9e89-a8be6c17a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "columns = []\n",
    "target = []\n",
    "average_numeric_counts = []\n",
    "\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "\n",
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "R1_cea = [item[0]for item in R1_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5aa8b-e43e-49c1-9ab2-99df78ebf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "mapping = [\"ORG\", \"PER\", \"LOC\", \"OTHERS\"]\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "\n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R1_cea).any()\n",
    "        if NE_flag:\n",
    "            joined_cells = column.str.cat(sep='-')\n",
    "            print(joined_cells)\n",
    "            print(\"________________\")\n",
    "            classifier_output = (classifier(joined_cells, mapping))\n",
    "            \n",
    "            # Get the highest score and its corresponding label\n",
    "            max_score_index = classifier_output['scores'].index(max(classifier_output['scores']))\n",
    "            highest_label = classifier_output['labels'][max_score_index]\n",
    "            \n",
    "            print(highest_label)\n",
    "            break\n",
    "            \n",
    "            doc = nlp(joined_cells)\n",
    "            entities = {key: [] for key in mapping.keys()}\n",
    "\n",
    "            # Extract entities and classify them\n",
    "            for ent in doc.ents:\n",
    "                for key, values in mapping.items():\n",
    "                    if ent.label_ in values:\n",
    "                        entities[key].append(ent.text)\n",
    "                        break\n",
    "            \n",
    "            # Find the key of the longest entities list\n",
    "            longest_key = max(entities, key=lambda k: len(entities[k]))\n",
    "            print(f\"{joined_cells} --> The key of the longest list is '{longest_key}'\")\n",
    "                    \n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc0c11-e811-4540-9a74-c29dd551b55f",
   "metadata": {},
   "source": [
    "## Round1_T2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfe186-559d-4a4b-a568-eacafc41f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "R1_cea = [item[0]for item in R1_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866bea9-b680-4c47-b179-93f270b6bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Place\",\n",
    "    \"PopulatedPlace\",\n",
    "    \"City\",\n",
    "    \"Country\",\n",
    "    \"Region\",\n",
    "    \"Mountain\",\n",
    "    \"Island\",\n",
    "    \"Lake\",\n",
    "    \"River\",\n",
    "    \"Park\",\n",
    "    \"Building\",\n",
    "    \"HistoricPlace\",\n",
    "    \"Monument\",\n",
    "    \"Bridge\",\n",
    "    \"Road\",\n",
    "    \"Airport\",\n",
    "    \"Person\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"Politician\",\n",
    "    \"Scientist\",\n",
    "    \"Writer\",\n",
    "    \"Actor\",\n",
    "    \"Musician\",\n",
    "    \"MilitaryPerson\",\n",
    "    \"Religious\",\n",
    "    \"Royalty\",\n",
    "    \"Criminal\",\n",
    "    \"Organisation\",\n",
    "    \"Company\",\n",
    "    \"EducationalInstitution\",\n",
    "    \"PoliticalParty\",\n",
    "    \"SportsTeam\",\n",
    "    \"Non-ProfitOrganisation\",\n",
    "    \"GovernmentAgency\",\n",
    "    \"ReligiousOrganisation\",\n",
    "    \"Band\",\n",
    "    \"Library\",\n",
    "    \"Museum\",\n",
    "    \"Hospital\",\n",
    "    \"University\",\n",
    "    \"TradeUnion\"\n",
    "]\n",
    "\n",
    "# Mapping of subtypes to macro classes\n",
    "mapping = {\n",
    "    \"Place\": [\"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\", \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"],\n",
    "    \"Person\": [\"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\", \"Religious\", \"Royalty\", \"Criminal\"],\n",
    "    \"Organisation\": [\"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\", \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\"],\n",
    "    \"Institution\": [\"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab481172-9273-43e5-b4d2-d1011cec6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R1_cea).any()\n",
    "        if NE_flag:\n",
    "            joined_cells = column.str.cat(sep='-')        \n",
    "            doc = nlp(joined_cells)\n",
    "            entities = {\"ORG\": [], \"PERS\": [], \"LOC\": [], \"OTHERS\": []}\n",
    "\n",
    "            # Extract entities and classify them\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    entities[\"ORG\"].append(ent.text)\n",
    "                elif ent.label_ == \"PERSON\":\n",
    "                    entities[\"PERS\"].append(ent.text)\n",
    "                elif ent.label_ == \"GPE\" or ent.label_ == \"FAC\":  # GPE (Geopolitical Entity)\n",
    "                    entities[\"LOC\"].append(ent.text)\n",
    "                else:\n",
    "                    entities[\"OTHERS\"].append(ent.text)\n",
    "            \n",
    "            # Find the key of the longest entities list\n",
    "            longest_key = max(entities, key=lambda k: len(entities[k]))\n",
    "            print(f\"{joined_cells} --> The key of the longest list is '{longest_key}'\")\n",
    "                    \n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408542e-7a90-4b4b-b1a8-74fc2448eed5",
   "metadata": {},
   "source": [
    "## Round3_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da63443-b6a5-4060-b097-e6e4a2fb9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)\n",
    "\n",
    "R3_cea = [item[0]for item in R3_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c261e-3ff1-41ca-8cde-34c090d858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R3_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673eea67-ad8c-4b23-99ef-c5f2dc86b1cc",
   "metadata": {},
   "source": [
    "## 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c0a1c-045b-4cc6-ac29-d60215b8fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_2T_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf378a-2f88-44c0-a551-843d7dc9bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_2T_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a4ed1-7939-4dfc-9eb4-9c9c0f102ef4",
   "metadata": {},
   "source": [
    "## Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208edf55-99eb-4018-a0a7-031dbd192a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b461089-7f54-4ed5-9aef-669855493e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af83a2d-7e99-4577-85ec-b297fdf376ad",
   "metadata": {},
   "source": [
    "## DF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c14bf0-1ea7-4856-90a9-e433c1c807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df_def = pd.DataFrame({\n",
    "    'column names': columns,\n",
    "    'median_lengths': median_lengths,\n",
    "    'median_token_counts': median_token_counts,\n",
    "    'average_numeric_counts': average_numeric_counts,\n",
    "    'target': target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c179f-5eb2-4c54-9d77-4639ddb9f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_def.to_csv('./data/NE_lit_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e1e43-0b44-4204-b52a-80be8cce57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#   READ DIRECTLY THE DATASET HERE\n",
    "###################################\n",
    "\n",
    "df = pd.read_csv('./data/NE_lit_dataset.csv')\n",
    "filtered_df = df[df['target'].isin(['lit', 'NE'])]\n",
    "\n",
    "# Displaying the filtered DataFrame\n",
    "df[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b29d6-834d-47ce-bbf7-eac0d736e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = df['target'].value_counts()\n",
    "\n",
    "# Extract counts for specific values\n",
    "ne_count = target_counts.get(\"NE\", 0)\n",
    "lit_count = target_counts.get(\"lit\", 0)\n",
    "none_count = df.shape[0] - (ne_count+lit_count)\n",
    "\n",
    "print(f\"Count of 'NE': {ne_count}\")\n",
    "print(f\"Count of 'lit': {lit_count}\")\n",
    "print(f\"Count of 'NaN': {none_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b0060-e2de-4d00-a043-56bc1709897d",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2256078-4db3-4a4a-98c0-1e49c19b7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df['target'] = label_encoder.fit_transform(filtered_df['target'])\n",
    "\n",
    "# One-hot encode the 'column names' column\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = one_hot_encoder.fit_transform(filtered_df[['column names']])\n",
    "\n",
    "# Combine the encoded categorical data with the numeric data\n",
    "numeric_data = filtered_df[['median_lengths', 'median_token_counts', 'average_numeric_counts']].values\n",
    "X = np.hstack([encoded_columns, numeric_data])\n",
    "y = filtered_df['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28204b07-3a07-4b09-8748-ec0d4f4eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "mapped_predictions = [\"lit\" if pred == 1 else \"NE\" for pred in y_pred.ravel()]\n",
    "\n",
    "# Extract the part of X_test that corresponds to the one-hot encoded columns\n",
    "encoded_columns_test = X_test[:, :encoded_columns.shape[1]]\n",
    "\n",
    "# Inverse transform the one-hot encoded columns to get the original categorical labels\n",
    "original_labels = one_hot_encoder.inverse_transform(encoded_columns_test)\n",
    "\n",
    "# Print a few examples to check\n",
    "for i in range(100):\n",
    "    print(f'Original label: {original_labels[i]}, Predicted: {mapped_predictions[i]}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06397d40-ffd2-4fc2-a622-3a3338c45d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
