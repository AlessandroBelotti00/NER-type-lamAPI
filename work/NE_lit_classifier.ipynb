{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b4fc3a-0dce-4366-907b-927dac9615af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: tf-keras in /opt/conda/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /opt/conda/lib/python3.11/site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.24.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.24.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.11/site-packages (from spacy) (1.24.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.18.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.7.4 pydantic-core-2.18.4 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install tf-keras\n",
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2be3fbb-0e7a-44c2-8807-381e580db097",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mliteral_recognizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LiteralRecognizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import pipeline\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "57ec340f-ef96-4940-9e89-a8be6c17a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "class ColumnAnalysis:\n",
    "\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.entity_type_dict = {\n",
    "            \"PERSON\": \"NE\",\n",
    "            \"NORP\": \"NE\",\n",
    "            \"FAC\": \"NE\",\n",
    "            \"ORG\": \"NE\",\n",
    "            \"GPE\": \"NE\",\n",
    "            \"LOC\": \"NE\",\n",
    "            \"PRODUCT\": \"NE\",\n",
    "            \"EVENT\": \"NE\",\n",
    "            \"WORK_OF_ART\": \"NE\",\n",
    "            \"LAW\": \"NE\",\n",
    "            \"LANGUAGE\": \"NE\",\n",
    "            \"DATE\": \"LIT\",\n",
    "            \"TIME\": \"LIT\",\n",
    "            \"PERCENT\": \"LIT\",\n",
    "            \"MONEY\": \"LIT\",\n",
    "            \"QUANTITY\": \"LIT\",\n",
    "            \"ORDINAL\": \"LIT\",\n",
    "            \"CARDINAL\": \"LIT\",\n",
    "            \"URL\": \"LIT\",\n",
    "            \"DESC\": \"LIT\",\n",
    "            \"TOKEN\": \"NE\",\n",
    "            \"INTEGER\": \"LIT\",\n",
    "            \"FLOAT\": \"LIT\",\n",
    "            \"DATETIME\": \"LIT\",\n",
    "            \"ADDRESS\": \"LIT\",\n",
    "            \"EMAIL\": \"LIT\"\n",
    "        }\n",
    "\n",
    "        self.LIT_DATATYPE = {\n",
    "            \"DATE\": \"DATETIME\", \n",
    "            \"TIME\": \"STRING\", \n",
    "            \"PERCENT\": \"STRING\", \n",
    "            \"MONEY\": \"STRING\", \n",
    "            \"QUANTITY\": \"STRING\", \n",
    "            \"ORDINAL\": \"NUMBER\", \n",
    "            \"CARDINAL\": \"NUMBER\", \n",
    "            \"URL\": \"STRING\",\n",
    "            \"DESC\": \"STRING\",\n",
    "            \"TOKEN\": \"STRING\",\n",
    "            \"INTEGER\": \"NUMBER\",\n",
    "            \"FLOAT\": \"NUMBER\",\n",
    "            \"DATETIME\": \"DATETIME\",\n",
    "            \"ADDRESS\": \"STRING\",\n",
    "            \"EMAIL\": \"STRING\",\n",
    "            \"STRING\": \"STRING\"\n",
    "        }\n",
    "\n",
    "        self.NE_DATATYPE = [\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\"]\n",
    "\n",
    "\n",
    "    def classifiy_columns(self, columns = []):\n",
    "\n",
    "        def most_frequent_element(input_list):\n",
    "            # Count occurrences of each element in the list\n",
    "            counter = Counter(input_list)\n",
    "            \n",
    "            # Get the most common element(s) and their count\n",
    "            most_common = counter.most_common(1)  # Returns a list of (element, count)\n",
    "            \n",
    "            # If there are ties (multiple elements with the same frequency), this will return the first one encountered.\n",
    "            return most_common[0][0]  # Return the element from the first tuple\n",
    "\n",
    "       \n",
    "        def update_dict(dictionary, key, value=1):\n",
    "            if key not in dictionary:\n",
    "                dictionary[key] = 0\n",
    "            dictionary[key] += value    \n",
    "\n",
    "        url_pattern = re.compile(r'^(https?|ftp)://[^\\s/$.?#].[^\\s]*$', re.IGNORECASE)\n",
    "        email_pattern = re.compile(r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$', re.IGNORECASE)\n",
    "        address_pattern = re.compile(\n",
    "            r'\\d+\\s+\\w+\\s+(?:street|st|avenue|ave|road|rd|boulevard|blvd|lane|ln|drive|dr|court|ct|circle|cir|place|pl)\\.?\\s*\\w*', re.IGNORECASE)\n",
    "        datetime_pattern = re.compile(\n",
    "            r'(?:\\d{4}-\\d{2}-\\d{2})'  # YYYY-MM-DD format\n",
    "            r'|(?:31(?:\\/|-|\\.)0?[13578]|1[02](?:\\/|-|\\.)\\d{4})'  # 31 days months\n",
    "            r'|(?:29|30(?:\\/|-|\\.)0?[1,3-9]|1[0-2](?:\\/|-|\\.)\\d{4})'  # 29/30 days months\n",
    "            r'|(?:0?[1-9]|[12]\\d|3[01])(?:\\/|-|\\.)'  # Day\n",
    "            r'(?:0?[1-9]|1[0-2])(?:\\/|-|\\.)\\d{4}'  # Month\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/(?:\\d{2})'  # MM/DD/YY format\n",
    "            r'|(?:0?[1-9]|1[0-2])/(?:0?[1-9]|[12]\\d|3[01])/\\d{2}'  # MM/DD/YY format\n",
    "            r'\\b\\d{2}/(?:0?[1-9]|[12]\\d|3[01])/(?:0?[1-9]|1[0-2])\\b'  # YY/DD/MM format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d\\.[0-5]\\d'  # HH:MM.SS format\n",
    "            r'|(?:[01]?\\d|2[0-3]):[0-5]\\d'  # HH:MM format\n",
    "            r'|(?:[0-5]?\\d):[0-5]\\d(?:\\.\\d{1,2})?'  # H:MM or H:MM.S format\n",
    "            r'|(?:2[0-3]|[01]?\\d)h[0-5]?\\d(?:m[0-5]?\\d(?:\\.\\d{1,2})?s)?',  # HhMMmSSs format\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        #pattern = r'^[\\d,.]+$'  # ^ asserts position at the start, [\\d,.]+ matches one or more digits, commas, or periods, $ asserts position at the end.\n",
    "        #return re.match(pattern, input_string) is not None\n",
    "\n",
    "        col_type = []\n",
    "        final_result = {}\n",
    "        \n",
    "        for col_name, col_data in df.items():\n",
    "            \n",
    "            # Analyze the concatenated text using Spacy\n",
    "            labels = {}\n",
    "            tags = {\"NE\": 0, \"LIT\": 0}\n",
    "            type = []\n",
    "            \n",
    "            for cell in col_data:\n",
    "                is_number = False\n",
    "                label = None\n",
    "            \n",
    "                if isinstance(cell, str):\n",
    "                    if cell == \"NaN\":\n",
    "                        label = \"\"\n",
    "                    \n",
    "                    # Check if the cell is a URL\n",
    "                    elif re.match(url_pattern, cell):\n",
    "                        label = \"URL\"\n",
    "                    \n",
    "                    # Check if the cell is an Email\n",
    "                    elif re.match(email_pattern, cell):\n",
    "                        label = \"EMAIL\"\n",
    "                    \n",
    "                    # Check if the cell is an Address\n",
    "                    elif re.match(address_pattern, cell):\n",
    "                        label = \"ADDRESS\"\n",
    "                        \n",
    "                    # Check if the cell matches datetime pattern\n",
    "                    elif re.match(datetime_pattern, cell):\n",
    "                        label = \"DATETIME\"\n",
    "\n",
    "\n",
    "                # Check if the cell is a number\n",
    "                if label is None:\n",
    "                    try:\n",
    "                        cell = str(cell)\n",
    "                        if ',' in cell or '.' in cell or '%' in cell or '$' in cell:\n",
    "                            cell = cell.replace('.', '').replace(',', '').replace('%', '').replace('$', '')\n",
    "                        \n",
    "                        if len(cell) - len(re.findall(r'\\d', cell)) < 5:\n",
    "                            is_number = True\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                \n",
    "                if is_number:\n",
    "                    label = \"NUMBER\"\n",
    "                elif len(cell.split(\" \")) >= 15:   #is the choosen treshold for discriminate descriptions and strings\n",
    "                    label = \"NOA\"\n",
    "                elif len(cell.split(\" \")) == 1 and len(cell) <= 4:\n",
    "                    label = \"STRING\"\n",
    "                    \n",
    "                if label is not None:\n",
    "                    type.append(label)\n",
    "                else:\n",
    "                    # do the lookup\n",
    "                    url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "                    params = {\n",
    "                        'name': cell,\n",
    "                        'token': 'lamapi_demo_2023',\n",
    "                        'kg': 'wikidata',\n",
    "                        'limit': 10,\n",
    "                        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{cell}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "                        'sort': [\n",
    "                            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                        ]\n",
    "                    }\n",
    "\n",
    "                    response = requests.get(url, params=params)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        if len(data) > 0:\n",
    "                            print(f\"{cell} --> NE_{data[0]['NERtype']}\")\n",
    "                            type.append(f\"NE_{data[0]['NERtype']}\")\n",
    "                            break\n",
    "                        else:\n",
    "                            type.append(\"None\")\n",
    "\n",
    "            col_type.append(next((x for x in type if x != \"\"), None))\n",
    "  \n",
    "           \n",
    "        return col_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5aa8b-e43e-49c1-9ab2-99df78ebf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "#tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "target = []\n",
    "column_analysis = ColumnAnalysis()\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "\n",
    "    result = column_analysis.classifiy_columns(df.iloc[1:100])\n",
    "\n",
    "    final = list(zip(df.iloc[2], df.columns, result))\n",
    "    filtered_data = [item for item in final if item[-1] != 'None']\n",
    "    print(final)\n",
    "    print(\"____________________\")\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc0c11-e811-4540-9a74-c29dd551b55f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Round1_T2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfe186-559d-4a4b-a568-eacafc41f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "R1_cea = [item[0]for item in R1_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866bea9-b680-4c47-b179-93f270b6bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"Place\",\n",
    "    \"PopulatedPlace\",\n",
    "    \"City\",\n",
    "    \"Country\",\n",
    "    \"Region\",\n",
    "    \"Mountain\",\n",
    "    \"Island\",\n",
    "    \"Lake\",\n",
    "    \"River\",\n",
    "    \"Park\",\n",
    "    \"Building\",\n",
    "    \"HistoricPlace\",\n",
    "    \"Monument\",\n",
    "    \"Bridge\",\n",
    "    \"Road\",\n",
    "    \"Airport\",\n",
    "    \"Person\",\n",
    "    \"Artist\",\n",
    "    \"Athlete\",\n",
    "    \"Politician\",\n",
    "    \"Scientist\",\n",
    "    \"Writer\",\n",
    "    \"Actor\",\n",
    "    \"Musician\",\n",
    "    \"MilitaryPerson\",\n",
    "    \"Religious\",\n",
    "    \"Royalty\",\n",
    "    \"Criminal\",\n",
    "    \"Organisation\",\n",
    "    \"Company\",\n",
    "    \"EducationalInstitution\",\n",
    "    \"PoliticalParty\",\n",
    "    \"SportsTeam\",\n",
    "    \"Non-ProfitOrganisation\",\n",
    "    \"GovernmentAgency\",\n",
    "    \"ReligiousOrganisation\",\n",
    "    \"Band\",\n",
    "    \"Library\",\n",
    "    \"Museum\",\n",
    "    \"Hospital\",\n",
    "    \"University\",\n",
    "    \"TradeUnion\"\n",
    "]\n",
    "\n",
    "# Mapping of subtypes to macro classes\n",
    "mapping = {\n",
    "    \"Place\": [\"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\", \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"],\n",
    "    \"Person\": [\"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\", \"Religious\", \"Royalty\", \"Criminal\"],\n",
    "    \"Organisation\": [\"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\", \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\"],\n",
    "    \"Institution\": [\"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab481172-9273-43e5-b4d2-d1011cec6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R1_cea).any()\n",
    "        if NE_flag:\n",
    "            joined_cells = column.str.cat(sep='-')        \n",
    "            doc = nlp(joined_cells)\n",
    "            entities = {\"ORG\": [], \"PERS\": [], \"LOC\": [], \"OTHERS\": []}\n",
    "\n",
    "            # Extract entities and classify them\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"ORG\":\n",
    "                    entities[\"ORG\"].append(ent.text)\n",
    "                elif ent.label_ == \"PERSON\":\n",
    "                    entities[\"PERS\"].append(ent.text)\n",
    "                elif ent.label_ == \"GPE\" or ent.label_ == \"FAC\":  # GPE (Geopolitical Entity)\n",
    "                    entities[\"LOC\"].append(ent.text)\n",
    "                else:\n",
    "                    entities[\"OTHERS\"].append(ent.text)\n",
    "            \n",
    "            # Find the key of the longest entities list\n",
    "            longest_key = max(entities, key=lambda k: len(entities[k]))\n",
    "            print(f\"{joined_cells} --> The key of the longest list is '{longest_key}'\")\n",
    "                    \n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408542e-7a90-4b4b-b1a8-74fc2448eed5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Round3_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da63443-b6a5-4060-b097-e6e4a2fb9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)\n",
    "\n",
    "R3_cea = [item[0]for item in R3_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c261e-3ff1-41ca-8cde-34c090d858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R3_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673eea67-ad8c-4b23-99ef-c5f2dc86b1cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c0a1c-045b-4cc6-ac29-d60215b8fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_2T_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcf378a-2f88-44c0-a551-843d7dc9bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_2T_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a4ed1-7939-4dfc-9eb4-9c9c0f102ef4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208edf55-99eb-4018-a0a7-031dbd192a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)\n",
    "\n",
    "R4_cea = [item[0]for item in R4_sorted_mentions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b461089-7f54-4ed5-9aef-669855493e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "\n",
    "def count_numbers_in_string(s):\n",
    "    return len(re.findall(r'\\d+', str(s)))\n",
    "\n",
    "median_lengths = []\n",
    "median_token_counts = []\n",
    "average_numeric_counts = []\n",
    "target = []\n",
    "columns = []\n",
    "\n",
    "# Iterate through each table\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        column = df[col].astype(str)\n",
    "        \n",
    "        # Calculate median length for the current column\n",
    "        median_length = column.apply(len).median()\n",
    "        median_lengths.append(median_length)\n",
    "        \n",
    "        # Calculate median token count for the current column\n",
    "        median_token_count = column.apply(lambda x: len(x.split())).median()\n",
    "        median_token_counts.append(median_token_count)\n",
    "        \n",
    "        # Calculate average count of numeric values in the current column\n",
    "        total_numeric_count = column.apply(count_numbers_in_string).sum()\n",
    "        average_numeric_count = total_numeric_count / len(df) if len(df) > 0 else 0\n",
    "        average_numeric_counts.append(average_numeric_count)\n",
    "        \n",
    "        # Check for NE flag\n",
    "        NE_flag = column.isin(R4_cea).any()\n",
    "        if NE_flag:\n",
    "            target.append(\"NE\")\n",
    "        elif median_length - average_numeric_count < 2:\n",
    "            target.append(\"lit\")\n",
    "        else:\n",
    "            target.append(\"None\")\n",
    "    \n",
    "    columns.extend(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af83a2d-7e99-4577-85ec-b297fdf376ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c14bf0-1ea7-4856-90a9-e433c1c807a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame\n",
    "df_def = pd.DataFrame({\n",
    "    'column names': columns,\n",
    "    'median_lengths': median_lengths,\n",
    "    'median_token_counts': median_token_counts,\n",
    "    'average_numeric_counts': average_numeric_counts,\n",
    "    'target': target\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c179f-5eb2-4c54-9d77-4639ddb9f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_def.to_csv('./data/NE_lit_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e1e43-0b44-4204-b52a-80be8cce57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#   READ DIRECTLY THE DATASET HERE\n",
    "###################################\n",
    "\n",
    "df = pd.read_csv('./data/NE_lit_dataset.csv')\n",
    "filtered_df = df[df['target'].isin(['lit', 'NE'])]\n",
    "\n",
    "# Displaying the filtered DataFrame\n",
    "df[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b29d6-834d-47ce-bbf7-eac0d736e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = df['target'].value_counts()\n",
    "\n",
    "# Extract counts for specific values\n",
    "ne_count = target_counts.get(\"NE\", 0)\n",
    "lit_count = target_counts.get(\"lit\", 0)\n",
    "none_count = df.shape[0] - (ne_count+lit_count)\n",
    "\n",
    "print(f\"Count of 'NE': {ne_count}\")\n",
    "print(f\"Count of 'lit': {lit_count}\")\n",
    "print(f\"Count of 'NaN': {none_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b0060-e2de-4d00-a043-56bc1709897d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2256078-4db3-4a4a-98c0-1e49c19b7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target variable to numeric\n",
    "label_encoder = LabelEncoder()\n",
    "filtered_df['target'] = label_encoder.fit_transform(filtered_df['target'])\n",
    "\n",
    "# One-hot encode the 'column names' column\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = one_hot_encoder.fit_transform(filtered_df[['column names']])\n",
    "\n",
    "# Combine the encoded categorical data with the numeric data\n",
    "numeric_data = filtered_df[['median_lengths', 'median_token_counts', 'average_numeric_counts']].values\n",
    "X = np.hstack([encoded_columns, numeric_data])\n",
    "y = filtered_df['target'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28204b07-3a07-4b09-8748-ec0d4f4eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "mapped_predictions = [\"lit\" if pred == 1 else \"NE\" for pred in y_pred.ravel()]\n",
    "\n",
    "# Extract the part of X_test that corresponds to the one-hot encoded columns\n",
    "encoded_columns_test = X_test[:, :encoded_columns.shape[1]]\n",
    "\n",
    "# Inverse transform the one-hot encoded columns to get the original categorical labels\n",
    "original_labels = one_hot_encoder.inverse_transform(encoded_columns_test)\n",
    "\n",
    "# Print a few examples to check\n",
    "for i in range(100):\n",
    "    print(f'Original label: {original_labels[i]}, Predicted: {mapped_predictions[i]}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06397d40-ffd2-4fc2-a622-3a3338c45d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
