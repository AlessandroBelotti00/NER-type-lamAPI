{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e93348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SPARQLWrapper in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from SPARQLWrapper) (7.0.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62066ba3-f7de-456f-9526-3e0604538592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from requests import get\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360cc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping definition\n",
    "\n",
    "PERS = 0\n",
    "LOC = 1\n",
    "ORG = 2\n",
    "OTHERS = 4\n",
    "none = 4\n",
    "\n",
    "entity_to_id = {\n",
    "    'PERS': PERS,\n",
    "    'LOC': LOC,\n",
    "    'ORG': ORG,\n",
    "    'OTHERS': OTHERS,\n",
    "    None: none\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82770b3a-6e3d-40f0-9680-bf905ab6d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./id_to_ner.json\"\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    id_to_ner = json.load(file)\n",
    "\n",
    "with open('./HT2_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95919468-1f01-420a-8aff-2b9500aa2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438801/438801 [00:46<00:00, 9409.99it/s] \n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR2-2021.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('../data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./R4_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd58ab00-03ba-4342-85a9-4d3a2d270e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9412429/9412429 [18:19<00:00, 8908.78it/s] "
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 144. MiB for an array with shape (2, 9412429) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mupdate(chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Concatenate all processed chunks to form the final DataFrame\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m final_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Optionally, save the final DataFrame to a new CSV file\u001b[39;00m\n\u001b[0;32m     34\u001b[0m final_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/alligator_training_data/Round4_training_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:360\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mConcatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03malong the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03mValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    347\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m     objs,\n\u001b[0;32m    349\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    358\u001b[0m )\n\u001b[1;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:595\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    591\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    593\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 595\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[0;32m    599\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\pandas\\core\\internals\\concat.py:231\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    225\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 144. MiB for an array with shape (2, 9412429) and data type int64"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/Round4.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dda5cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3f51760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:36<00:00,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"../data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cta_file = '../data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a8d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6400512 [00:19<?, ?it/s]\n",
      "100%|██████████| 6400512/6400512 [27:47<00:00, 3837.32it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/2T-2020.csv'\n",
    "output_file = '../data/alligator_training_data/2T_2020_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/2T_2020_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9496c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT3_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9abb77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527730/527730 [01:42<00:00, 5155.63it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR3-2021.csv'\n",
    "output_file = '../data/alligator_training_data/HT3_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
