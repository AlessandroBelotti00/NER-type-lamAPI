{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79e93348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.1)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, safetensors, requests, pyarrow, filelock, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 13.0.0\n",
      "    Uninstalling pyarrow-13.0.0:\n",
      "      Successfully uninstalled pyarrow-13.0.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-2.21.0 dill-0.3.8 filelock-3.15.4 huggingface-hub-0.24.5 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 safetensors-0.4.4 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.0 xxhash-3.4.1\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.5)\n",
      "Collecting gliner\n",
      "  Downloading gliner-0.2.9-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting torch>=2.0.0 (from gliner)\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: transformers>=4.38.2 in /opt/conda/lib/python3.11/site-packages (from gliner) (4.44.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.4 in /opt/conda/lib/python3.11/site-packages (from gliner) (0.24.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from gliner) (4.66.5)\n",
      "Collecting onnxruntime (from gliner)\n",
      "  Downloading onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting sentencepiece (from gliner)\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->gliner) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->gliner) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->gliner) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->gliner) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->gliner) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.4->gliner) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->gliner) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->gliner) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->gliner) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=2.0.0->gliner)\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->gliner)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.2->gliner) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.2->gliner) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.2->gliner) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.2->gliner) (0.19.1)\n",
      "Collecting coloredlogs (from onnxruntime->gliner)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.11/site-packages (from onnxruntime->gliner) (24.3.7)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from onnxruntime->gliner) (4.24.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->gliner) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.4->gliner) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.0->gliner) (1.3.0)\n",
      "Downloading gliner-0.2.9-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.18.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime, nvidia-cusolver-cu12, torch, gliner\n",
      "Successfully installed coloredlogs-15.0.1 gliner-0.2.9 humanfriendly-10.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 onnxruntime-1.18.1 sentencepiece-0.2.0 torch-2.4.0 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets\n",
    "! pip install tqdm\n",
    "! pip install gliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62066ba3-f7de-456f-9526-3e0604538592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gliner import GLiNER\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360cc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_1 = 'data/embedding_training_data/Round1_train.csv'\n",
    "csv_file_2 = 'data/embedding_training_data/Round3_train.csv'\n",
    "csv_file_3 = 'data/embedding_training_data/2T_train.csv'\n",
    "csv_file_4 = 'data/embedding_training_data/HT2_train.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv(csv_file_1)\n",
    "df2 = pd.read_csv(csv_file_2)\n",
    "df3 = pd.read_csv(csv_file_3)\n",
    "df4 = pd.read_csv(csv_file_4)\n",
    "\n",
    "dataset = pd.concat([df1, df2, df3, df4], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d63a8db-bd3a-4c99-89cb-28abe80da6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>need for speed: carbon</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>2006 video game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alex rodriguez</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>American baseball player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>school of rock</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>2003 film by Richard Linklater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lex luger</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>American professional wrestler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crash bash</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>2000 video game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8439</th>\n",
       "      <td>Call e</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>original song written and composed by Debbie H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8440</th>\n",
       "      <td>V* V1148 Cen</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>Mira variable star in the constellation Centaurus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8441</th>\n",
       "      <td>Springfield Township</td>\n",
       "      <td>LOC</td>\n",
       "      <td>township in Union County, New Jersey, United S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8442</th>\n",
       "      <td>osmium-181</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>isotope of osmium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>Parma</td>\n",
       "      <td>LOC</td>\n",
       "      <td>human settlement in Gubakha Urban Okrug, Perm ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28039 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text   label  \\\n",
       "0     need for speed: carbon  OTHERS   \n",
       "1             alex rodriguez  OTHERS   \n",
       "2             school of rock  OTHERS   \n",
       "3                  lex luger  OTHERS   \n",
       "4                 crash bash  OTHERS   \n",
       "...                      ...     ...   \n",
       "8439                  Call e  OTHERS   \n",
       "8440            V* V1148 Cen  OTHERS   \n",
       "8441    Springfield Township     LOC   \n",
       "8442              osmium-181  OTHERS   \n",
       "8443                   Parma     LOC   \n",
       "\n",
       "                                                   desc  \n",
       "0                                       2006 video game  \n",
       "1                              American baseball player  \n",
       "2                        2003 film by Richard Linklater  \n",
       "3                        American professional wrestler  \n",
       "4                                       2000 video game  \n",
       "...                                                 ...  \n",
       "8439  original song written and composed by Debbie H...  \n",
       "8440  Mira variable star in the constellation Centaurus  \n",
       "8441  township in Union County, New Jersey, United S...  \n",
       "8442                                  isotope of osmium  \n",
       "8443  human settlement in Gubakha Urban Okrug, Perm ...  \n",
       "\n",
       "[28039 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82770b3a-6e3d-40f0-9680-bf905ab6d143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc69dc325b624ecd9445d624a3b9f12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/gliner/model.py:568: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_file, map_location=torch.device(map_location))\n"
     ]
    }
   ],
   "source": [
    "from gliner import GLiNER\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "# Assuming model is loaded\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519be09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new labels and the mapping to the original labels\n",
    "new_labels = [\n",
    "    \"person\", \"organization\", \"location\", \"others\", \"movie\", \"videogames\"\n",
    "]\n",
    "\n",
    "# Define a mapping from new labels to original labels\n",
    "label_mapping = {\n",
    "    \"person\": \"person\",\n",
    "    \"organization\": \"organization\",\n",
    "    \"location\": \"location\",\n",
    "    \"movie\": \"others\",  \n",
    "    \"others\": \"others\",\n",
    "    \"videogames\": \"others\"\n",
    "}\n",
    "\n",
    "df = []\n",
    "\n",
    "# Wrap the dataset iterator with tqdm to add a progress bar\n",
    "for _, row in tqdm(dataset.iloc[10000:20000].iterrows(), total=dataset.shape[0], desc=\"Processing Rows\"):\n",
    "    # Predict entities using the new set of labels\n",
    "    entities = model.predict_entities(row['text'] + ' - ' + row['desc'], new_labels, threshold=0.05)\n",
    "    \n",
    "    # Map the predicted labels to the original labels\n",
    "    ner_type = [label_mapping.get(entity[\"label\"], \"others\") for entity in entities]\n",
    "\n",
    "    counter = Counter(ner_type)\n",
    "    try:\n",
    "      most_common_element, element_count = counter.most_common(1)[0]\n",
    "      print(f\"{row['text'] + ' - ' + row['desc']} ==> {most_common_element}\")\n",
    "      \n",
    "      new_row = {\n",
    "        'text': row['text'],\n",
    "        'target': row['label'],\n",
    "        'prediction': most_common_element\n",
    "      }\n",
    "      df.append(new_row)\n",
    "    except: \n",
    "      pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a83ffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('./data/embedding_training_data/2_predictions.csv', index=False)\n",
    "\n",
    "print(\"DataFrame saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a20279-6901-4743-a731-60620fde4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_1 = 'data/embedding_training_data/1_predictions.csv'\n",
    "csv_file_2 = 'data/embedding_training_data/2_predictions.csv'\n",
    "# Read the CSV files\n",
    "df1 = pd.read_csv(csv_file_1)\n",
    "df2 = pd.read_csv(csv_file_2)\n",
    "\n",
    "dataset = pd.concat([df1, df2], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95919468-1f01-420a-8aff-2b9500aa2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438801/438801 [01:10<00:00, 6215.54it/s]\n",
      "100%|██████████| 438801/438801 [01:12<00:00, 6023.20it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR2-2021.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('../data/alligator_training_data/HT2_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4c5867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./R4_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd58ab00-03ba-4342-85a9-4d3a2d270e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2820000/9412429 [05:10<09:25, 11667.16it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m key_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m        \n\u001b[0;32m     18\u001b[0m ner_mention \u001b[38;5;241m=\u001b[39m entity_to_id[id_to_ner\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[1;32m---> 19\u001b[0m ner_column \u001b[38;5;241m=\u001b[39m entity_to_id[\u001b[43mkey_to_cell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[0;32m     20\u001b[0m mentions\u001b[38;5;241m.\u001b[39mappend(ner_mention)\n\u001b[0;32m     21\u001b[0m key_columns\u001b[38;5;241m.\u001b[39mappend(ner_column)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2820000/9412429 [05:23<09:25, 11667.16it/s]"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/Round4.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda5cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f51760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:52<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"../data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cta_file = '../data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4a8d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2820000/9412429 [06:53<16:05, 6824.64it/s] \n",
      "100%|██████████| 6400512/6400512 [12:58<00:00, 8216.47it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/2T-2020.csv'\n",
    "output_file = '../data/alligator_training_data/2T_2020_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/2T_2020_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9496c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT3_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9abb77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527730/527730 [01:06<00:00, 7981.71it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR3-2021.csv'\n",
    "output_file = '../data/alligator_training_data/HT3_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
