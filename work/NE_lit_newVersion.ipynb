{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e93348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SPARQLWrapper in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from SPARQLWrapper) (7.0.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62066ba3-f7de-456f-9526-3e0604538592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from requests import get\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360cc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping definition\n",
    "\n",
    "PERS = 0\n",
    "LOC = 1\n",
    "ORG = 2\n",
    "OTHERS = 3\n",
    "none = 4\n",
    "\n",
    "entity_to_id = {\n",
    "    'PERS': PERS,\n",
    "    'LOC': LOC,\n",
    "    'ORG': ORG,\n",
    "    'OTHERS': OTHERS,\n",
    "    None: none\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82770b3a-6e3d-40f0-9680-bf905ab6d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./id_to_ner.json\"\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    id_to_ner = json.load(file)\n",
    "\n",
    "with open('./HT2_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "519be09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LOC'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_cell['0HDWETS2 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95919468-1f01-420a-8aff-2b9500aa2e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438801/438801 [01:10<00:00, 6215.54it/s]\n",
      "100%|██████████| 438801/438801 [01:12<00:00, 6023.20it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR2-2021.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('../data/alligator_training_data/HT2_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4c5867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./R4_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd58ab00-03ba-4342-85a9-4d3a2d270e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2820000/9412429 [05:10<09:25, 11667.16it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m key_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m        \n\u001b[0;32m     18\u001b[0m ner_mention \u001b[38;5;241m=\u001b[39m entity_to_id[id_to_ner\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[1;32m---> 19\u001b[0m ner_column \u001b[38;5;241m=\u001b[39m entity_to_id[\u001b[43mkey_to_cell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m]\n\u001b[0;32m     20\u001b[0m mentions\u001b[38;5;241m.\u001b[39mappend(ner_mention)\n\u001b[0;32m     21\u001b[0m key_columns\u001b[38;5;241m.\u001b[39mappend(ner_column)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2820000/9412429 [05:23<09:25, 11667.16it/s]"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/Round4.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    processed_chunks.append(chunk)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda5cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f51760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:52<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"../data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cta_file = '../data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4a8d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2820000/9412429 [06:53<16:05, 6824.64it/s] \n",
      "100%|██████████| 6400512/6400512 [12:58<00:00, 8216.47it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/2T-2020.csv'\n",
    "output_file = '../data/alligator_training_data/2T_2020_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = [] \n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"\n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/2T_2020_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9496c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HT3_ner_type.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9abb77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 527730/527730 [01:06<00:00, 7981.71it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_file = '../data/datasets_alligator_completi/HardTableR3-2021.csv'\n",
    "output_file = '../data/alligator_training_data/HT3_training_data.csv'\n",
    "\n",
    "chunk_size = 10000 \n",
    "\n",
    "# Create an iterator to read the CSV file in chunks\n",
    "chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "total_rows = sum(1 for row in open(csv_file)) - 1\n",
    "progress_bar = tqdm(total=total_rows)\n",
    "processed_chunks = []\n",
    "header_written = False\n",
    "\n",
    "for chunk in chunk_iterator:\n",
    "    mentions = []\n",
    "    key_columns = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        key = row['key']\n",
    "        id = row['id']\n",
    "        key_column = f\"{key.split('-')[0]} {key.split('-')[2]}\"        \n",
    "        ner_mention = entity_to_id[id_to_ner.get(id, None)]\n",
    "        ner_column = entity_to_id[key_to_cell.get(key_column, None)]\n",
    "        mentions.append(ner_mention)\n",
    "        key_columns.append(ner_column)\n",
    "    \n",
    "    chunk.insert(2, 'ner_mention', mentions)  # Insert 'mention' column after 'id'\n",
    "    chunk.insert(3, 'ner_column', key_columns)  # Insert 'key_column_mention' after 'mention'\n",
    "    if not header_written:\n",
    "        chunk.to_csv(output_file, mode='w', index=False)\n",
    "        header_written = True\n",
    "    else:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "    progress_bar.update(chunk.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate all processed chunks to form the final DataFrame\n",
    "#final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final DataFrame to a new CSV file\n",
    "#final_df.to_csv('./data/alligator_training_data/Round4_training_data.csv', index=False)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
