{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e343dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import requests\n",
    "from aiohttp import ClientResponseError\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c120554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100%|██████████| 1750/1750 [00:18<00:00, 93.90it/s] \n"
     ]
    }
   ],
   "source": [
    "### in case you want HT2\n",
    "\n",
    "tables_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "pattern = r'^\\.'\n",
    "\n",
    "# Create a list of file paths, excluding files that start with a dot\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n",
    "\n",
    "\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/HT2_ner_type.json', 'r') as f:\n",
    "    ner_type = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69009798",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44482ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 55612.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "    if value is not None:\n",
    "        ### SOFT FILTERING CONSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query' : f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}], \"should\": [{{\"term\": {{\"NERtype\": \"{value}\"}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell_sample):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_id = key_to_cell[key][1]\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        match = re.search(r'Q(\\d+)$', q_id)\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['should'][0]['term']['NERtype']\n",
    "            queries.append((query, match[0], ner_type_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(name, value):    \n",
    "    name = name.replace('\"', ' ')\n",
    "\n",
    "    if value is not None:\n",
    "        ### HARD FILTERING CONSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}, {{\"term\": {{\"NERtype\": \"{value}\"}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell_sample):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_id = key_to_cell[key][1]\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        match = re.search(r'Q(\\d+)$', q_id)\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            # Navigate through the dictionary to extract the value of \"NERtype\"\n",
    "            ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "            queries.append((query, match[0],ner_type_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ee6b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 3619/4000 [12:56<06:19,  1.00it/s]"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 84>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries))\n\u001b[1;32m---> 88\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfailed_queries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# For environments like Jupyter\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\nest_asyncio.py:89\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\asyncio\\futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(queries, url, pbar, failed_queries)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, \u001b[38;5;28mid\u001b[39m, _ \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m     67\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(process_item(session, url, \u001b[38;5;28mid\u001b[39m, headers, param, semaphore, pbar))\n\u001b[1;32m---> 69\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (mrr_increment, count), (param, \u001b[38;5;28mid\u001b[39m, NERtype) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, queries):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mrr_increment \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\asyncio\\tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\asyncio\\tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    232\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mprocess_item\u001b[1;34m(session, url, id, headers, params, semaphore, pbar)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_item\u001b[39m(session, url, \u001b[38;5;28mid\u001b[39m, headers, params, semaphore, pbar):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fetch(session, url, params, headers, semaphore)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientResponseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\backoff\\_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[0;32m    148\u001b[0m }\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    153\u001b[0m     giveup_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m giveup(e)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mfetch\u001b[1;34m(session, url, params, headers, semaphore)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@backoff\u001b[39m\u001b[38;5;241m.\u001b[39mon_exception(\n\u001b[0;32m     16\u001b[0m     backoff\u001b[38;5;241m.\u001b[39mexpo, \n\u001b[0;32m     17\u001b[0m     (aiohttp\u001b[38;5;241m.\u001b[39mClientError, aiohttp\u001b[38;5;241m.\u001b[39mhttp_exceptions\u001b[38;5;241m.\u001b[39mHttpProcessingError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(session, url, params, headers, semaphore):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Convert all params to str, int, or float\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m#params = {k: (int(v) if isinstance(v, np.integer) else str(v)) for k, v in params.items()}\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     27\u001b[0m                 response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raises an exception for 4XX/5XX status codes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\aiohttp\\client.py:1141\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[1;32m-> 1141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\aiohttp\\client.py:560\u001b[0m, in \u001b[0;36mClientSession._request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[0;32m    558\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\aiohttp\\client_reqrep.py:894\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[1;34m(self, connection)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m connection\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timer:\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    896\u001b[0m         \u001b[38;5;66;03m# read response\u001b[39;00m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\aiohttp\\helpers.py:720\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assume queries is a list of tuples [(param1, id1), (param2, id2), ...]\n",
    "\n",
    "failed_queries = {}\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=10, \n",
    "    max_time=400\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        # Convert all params to str, int, or float\n",
    "        #params = {k: (int(v) if isinstance(v, np.integer) else str(v)) for k, v in params.items()}\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{id}'\")\n",
    "            asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar, failed_queries):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id, _ in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id, NERtype) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                failed_queries[id] = NERtype\n",
    "            else:\n",
    "                m_mrr += mrr_increment\n",
    "                cont_el += count\n",
    "\n",
    "        asyncio.get_event_loop().call_soon_threadsafe(pbar.close)\n",
    "\n",
    "    print(f\"Coverage of 2T: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar, failed_queries))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar, failed_queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e6926",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/HT2_failed_queries_SOFT.json', 'w') as json_file:\n",
    "    json.dump(failed_queries, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6bdd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cd060b-eb7e-460f-a1d7-63b8b15cad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)\n",
    "\n",
    "\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0385ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 30.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PERS\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e9d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d6b59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "count = 0\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            count += 1\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb3fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 48/3865 [00:37<31:35,  2.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st helena pounds: Q374453 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 63/3865 [00:45<29:30,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abraham the syrian: Q1292819 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 189/3865 [01:50<12:25,  4.93it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queensland, australia: Q36074 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 226/3865 [02:02<17:14,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darksiders: Q30525856 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 393/3865 [02:36<11:20,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icarus, international journal of solar system studies: Q1656088 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 501/3865 [02:53<13:50,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eastern hognose snake: Q2699564 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 627/3865 [03:14<05:49,  9.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dijibouti: Q977 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 742/3865 [03:32<07:15,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swee waxbill: Q27075727 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 924/3865 [04:05<10:17,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dusky canada goose: Q27600982 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 952/3865 [04:08<05:37,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmona retusa: Q15283553 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1034/3865 [04:23<10:47,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterhousea floribunda: Q108742313 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1041/3865 [04:24<05:44,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oldsquaw: Q26597 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1100/3865 [04:33<05:21,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "northern short-tailed shrew: Q1766543 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1272/3865 [04:59<03:52, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ireland: Q27 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1466/3865 [05:30<06:05,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeff leonard: Q3176709 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1555/3865 [05:46<06:03,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bill hutchinson: Q4909545 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 1596/3865 [05:52<05:22,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eggert lake: Q491288 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1657/3865 [06:03<06:29,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achnatherum speciosum: Q15507171 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1660/3865 [06:04<06:21,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyril, apostle to the slavs: Q239925 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1675/3865 [06:06<05:29,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african citril: Q27075797 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 1771/3865 [06:22<03:12, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raven squad: Q5237049 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1814/3865 [06:30<06:06,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green wood-hoopoe: Q811583 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1819/3865 [06:30<04:10,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lac a l'eau-claire: Q1699973 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1844/3865 [06:34<04:14,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pale flycatcher: Q1318056 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1949/3865 [06:53<05:49,  5.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oplismenus burmannii: Q13936835 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2017/3865 [07:05<04:14,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carex heliophila: Q2938524 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2036/3865 [07:08<05:09,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lake james, north carolina: Q1485910 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2233/3865 [07:41<03:25,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new south wales, australia: Q3224 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 2336/3865 [07:56<03:25,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eastern screech-owl: Q251939 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 2401/3865 [08:06<03:02,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "egyptian rousette: Q754983 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 2457/3865 [08:15<05:26,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahamas, the: Q778 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2594/3865 [08:38<03:26,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eagle owl: Q214293 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2599/3865 [08:38<02:48,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koninklijk museum van het leger en de krijgsgeschiedenis: Q1395176 NOT FOUND in ORG\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2603/3865 [08:39<03:40,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african pied hornbill: Q226128 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 2681/3865 [08:53<02:02,  9.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lac st-jean: Q979922 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 2713/3865 [08:57<02:13,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southern water snake: Q2065834 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2827/3865 [09:16<03:20,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haronga madagascariensis: Q5194906 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 2948/3865 [09:36<02:03,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white gopher snake: Q1663774 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 2968/3865 [09:38<02:05,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african pygmy-goose: Q386949 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 3025/3865 [09:47<01:51,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown snake-eagle: Q549126 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3078/3865 [09:56<02:15,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charley root: Q5085471 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 3095/3865 [09:59<01:49,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little greenbul: Q25249305 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 3208/3865 [10:16<01:31,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawfish frog: Q26849093 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 3231/3865 [10:19<01:21,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bud man: Q125074 NOT FOUND in ORG\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 3298/3865 [10:29<01:09,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banded martin: Q1589557 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 3312/3865 [10:31<01:52,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "african scops-owl: Q1270188 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 3463/3865 [10:53<00:37, 10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southern flying squirrel: Q913350 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 3491/3865 [10:58<00:45,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black cuckoo-shrike: Q1306580 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 3524/3865 [11:04<01:06,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schedonorus arundinaceus: Q157922 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 3562/3865 [11:10<00:41,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "springhare: Q3027980 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 3574/3865 [11:12<00:56,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharpe's starling: Q27075613 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 3594/3865 [11:16<00:45,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worm snake: Q2940133 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 3730/3865 [11:37<00:17,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickerel frog: Q28035920 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 3749/3865 [11:40<00:15,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baystones: Q7968079 NOT FOUND in LOC\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 3761/3865 [11:43<00:24,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow-spotted barbet: Q5734788 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 3785/3865 [11:46<00:11,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "southeastern five-lined skink: Q137678 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 3839/3865 [11:55<00:03,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix, the: Q83495 NOT FOUND in OTHERS\n",
      "___________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3862/3865 [11:58<00:00,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.9844760672703752\n",
      "Measure Reciprocal Rank of R1: 0.9581293661060406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "failed_queries = {}\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    ### SOFT FILTERING CONTSTRAINT\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        ],\n",
    "                        \"should\": [\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        pbar.update(1)  # No need to await here\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        print(f\"{name}: {GT_id_match[0]} NOT FOUND in {value[1]}\")\n",
    "        print(\"___________________________\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar, failed_queries):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:                \n",
    "                failed_queries[url_id] = name\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar, failed_queries))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8afa8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 5060.35it/s]\n"
     ]
    }
   ],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "failed_queries = {}\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "\n",
    "    ### HARD FILTERING CONTSTRAINT\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }},\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        pbar.update(1)  # No need to await here\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        print(f\"{name}: {GT_id_match[0]} NOT FOUND in {value[1]}\")\n",
    "        print(\"___________________________\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar, failed_queries):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:             \n",
    "                failed_queries[url_id] = name\n",
    "\n",
    "\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar, failed_queries))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0400192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 463/4000 [06:03<46:13,  1.28it/s]\n",
      "  1%|          | 45/4000 [00:30<39:26,  1.67it/s]  2024-08-01 14:41:40,512 - INFO - Backing off fetch(...) for 0.0s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:41:40,514 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:41:40,515 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "  2%|▏         | 99/4000 [00:52<26:48,  2.42it/s]  2024-08-01 14:42:02,516 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "  3%|▎         | 103/4000 [00:54<28:58,  2.24it/s]2024-08-01 14:42:04,495 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:42:04,495 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:42:04,498 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "  5%|▌         | 215/4000 [01:39<18:40,  3.38it/s]  2024-08-01 14:42:49,510 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "  7%|▋         | 277/4000 [01:58<11:53,  5.22it/s]2024-08-01 14:43:08,493 - INFO - Backing off fetch(...) for 1.0s (asyncio.exceptions.TimeoutError)\n",
      "  7%|▋         | 287/4000 [02:02<30:39,  2.02it/s]2024-08-01 14:43:12,492 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "  7%|▋         | 289/4000 [02:03<28:06,  2.20it/s]2024-08-01 14:43:13,493 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "  9%|▉         | 356/4000 [02:30<27:42,  2.19it/s]2024-08-01 14:43:41,493 - INFO - Backing off fetch(...) for 0.5s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:43:41,496 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "  9%|▉         | 359/4000 [02:32<32:54,  1.84it/s]2024-08-01 14:43:43,511 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      " 10%|▉         | 382/4000 [02:43<14:16,  4.22it/s]2024-08-01 14:43:53,492 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:43:53,493 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      " 10%|█         | 401/4000 [02:52<25:52,  2.32it/s]2024-08-01 14:44:02,494 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      " 11%|█         | 449/4000 [03:13<38:40,  1.53it/s]  2024-08-01 14:44:23,508 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 719/4000 [04:40<35:58,  1.52it/s]2024-08-01 14:45:50,510 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 728/4000 [04:47<41:10,  1.32it/s]  2024-08-01 14:45:58,506 - INFO - Backing off fetch(...) for 1.0s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 729/4000 [04:50<1:05:08,  1.19s/it]2024-08-01 14:46:00,509 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:03,494 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:03,499 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 730/4000 [04:54<1:50:35,  2.03s/it]2024-08-01 14:46:05,506 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:06,507 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:07,497 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:07,500 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:08,502 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:08,502 - INFO - Backing off fetch(...) for 0.0s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:08,502 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:08,502 - INFO - Backing off fetch(...) for 0.5s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:09,510 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:09,510 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:10,509 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:11,497 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:11,498 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:11,498 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:11,498 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:11,504 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:11,504 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:12,508 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 731/4000 [05:03<3:46:58,  4.17s/it]2024-08-01 14:46:13,490 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:13,490 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:13,494 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:15,503 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:16,507 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:17,507 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:17,511 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:17,514 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:18,516 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:18,518 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:18,521 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:19,492 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:19,496 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:20,507 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:21,495 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:22,505 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:23,510 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:23,511 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:23,511 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:23,514 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:25,499 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:27,490 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:27,492 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:28,507 - INFO - Backing off fetch(...) for 0.5s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:29,499 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:30,506 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:31,506 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:32,492 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:34,497 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:34,497 - INFO - Backing off fetch(...) for 0.0s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:34,505 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:36,491 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:37,505 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:37,509 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:38,506 - INFO - Backing off fetch(...) for 0.5s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:38,508 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:38,510 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:38,512 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:38,512 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:40,504 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:40,507 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:41,506 - INFO - Backing off fetch(...) for 0.0s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:42,509 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:42,512 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:42,513 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:42,516 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:42,516 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:42,521 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:43,492 - INFO - Backing off fetch(...) for 0.3s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:43,492 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:43,497 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:43,498 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:43,501 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:46,507 - INFO - Backing off fetch(...) for 0.0s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:47,501 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:48,514 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:48,514 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:48,531 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:49,508 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:49,509 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:49,509 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:50,510 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:50,510 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:51,492 - INFO - Backing off fetch(...) for 0.7s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:51,493 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:53,509 - INFO - Backing off fetch(...) for 1.0s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:54,504 - INFO - Backing off fetch(...) for 0.8s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:54,507 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:54,510 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:54,513 - INFO - Backing off fetch(...) for 0.6s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:56,491 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:57,510 - INFO - Backing off fetch(...) for 0.4s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:57,510 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:59,502 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:46:59,506 - INFO - Backing off fetch(...) for 0.9s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 732/4000 [05:51<15:26:49, 17.02s/it]2024-08-01 14:47:01,494 - INFO - Backing off fetch(...) for 0.1s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 734/4000 [05:52<7:54:08,  8.71s/it] 2024-08-01 14:47:02,499 - INFO - Backing off fetch(...) for 0.2s (asyncio.exceptions.TimeoutError)\n",
      "2024-08-01 14:47:02,499 - INFO - Backing off fetch(...) for 0.5s (asyncio.exceptions.TimeoutError)\n",
      " 18%|█▊        | 736/4000 [05:53<4:03:47,  4.48s/it]"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assume queries is a list of tuples [(param1, id1), (param2, id2), ...]\n",
    "\n",
    "failed_queries = {}\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=10, \n",
    "    max_time=400\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        # Convert all params to str, int, or float\n",
    "        params = {k: (int(v) if isinstance(v, np.integer) else str(v)) for k, v in params.items()}\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{id}'\")\n",
    "            asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar, failed_queries):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                failed_queries[id] = param\n",
    "            else:\n",
    "                m_mrr += mrr_increment\n",
    "                cont_el += count\n",
    "\n",
    "        asyncio.get_event_loop().call_soon_threadsafe(pbar.close)\n",
    "\n",
    "    print(f\"Coverage of 2T: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar, failed_queries))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar, failed_queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d162709",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/HT2_failed_queries_SOFT.json', 'w') as json_file:\n",
    "    json.dump(failed_queries, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48472390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2T_failed_queries_HARD.json', 'r') as f:\n",
    "    failed_queries_hard = json.load(f)\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2T_failed_queries_SOFT.json', 'r') as f:\n",
    "    failed_queries_soft = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9496a761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "581"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(failed_queries_hard.keys()) & set(failed_queries_soft.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1043957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed_queries_hard: 590 vs failed_queries_soft: 587\n"
     ]
    }
   ],
   "source": [
    "print(f\"failed_queries_hard: {len(failed_queries_hard)} vs failed_queries_soft: {len(failed_queries_soft)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
