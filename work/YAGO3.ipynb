{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0e592d-d6ee-4f23-9ad8-f9de351a6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /opt/conda/lib/python3.11/site-packages (7.0.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from rdflib) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib) (3.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
      "Collecting multiprocessing\n",
      "  Downloading multiprocessing-2.6.2.1.tar.gz (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[7 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-z7v2gh21/multiprocessing_6d159aaf0c9b494484c84cd720c4723f/setup.py\", line 94\n",
      "  \u001b[31m   \u001b[0m     print 'Macros:'\n",
      "  \u001b[31m   \u001b[0m     ^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m SyntaxError: Missing parentheses in call to 'print'. Did you mean print(...)?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install rdflib\n",
    "! pip install multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506e2b01-54de-454d-a13e-b7c71d4f43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip  # For reading compressed files\n",
    "from rdflib import Graph  # For working with RDF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e082872a-9d86-4789-9731-5c5c752f6720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 lines\n",
      "Processed 200000 lines in total\n"
     ]
    }
   ],
   "source": [
    "file_path1 = \"./my-data/yago-wd-full-types.nt.gz\"\n",
    "file_path2 = \"./my-data/yago-wd-annotated-facts.ntx.gz\"\n",
    "file_path3 = \"./my-data/yago-wd-facts.nt.gz\"  # prefix\n",
    "file_path4 = \"./my-data/yago-wd-labels.nt.gz\"\n",
    "file_path5 = \"./my-data/yago-wd-class.nt.gz\"\n",
    "file_path6 = \"./my-data/yago-wd-sameAs.nt.gz\"\n",
    "\n",
    "# Create an RDFlib Graph object to store the parsed RDF data\n",
    "graph = Graph()\n",
    "\n",
    "# Define the chunk size (number of lines to process per iteration)\n",
    "chunk_size = 100000\n",
    "\n",
    "# Open the compressed NT file and process it in chunks\n",
    "try:\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        line_count = 0\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            # Decode the line from bytes to string\n",
    "            line_str = line.decode('utf-8').strip()\n",
    "            \n",
    "            # Add the line to the current chunk\n",
    "            chunk.append(line_str)\n",
    "            \n",
    "            # Check if the chunk size is reached, then process the chunk\n",
    "            if len(chunk) >= chunk_size:\n",
    "                graph.parse(data='\\n'.join(chunk), format='nt')\n",
    "                line_count += len(chunk)\n",
    "                print(f\"Processed {line_count} lines\")\n",
    "                break\n",
    "                chunk = []  # Reset the chunk\n",
    "            \n",
    "        # Process the remaining lines in the last chunk\n",
    "        if chunk:\n",
    "            graph.parse(data='\\n'.join(chunk), format='nt')\n",
    "            line_count += len(chunk)\n",
    "            print(f\"Processed {line_count} lines in total\")\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing RDF data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2304f326-1be8-4051-a04c-ad53d96a2810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Serialize the RDF graph into a string (e.g., Turtle format) and print the parsed data\n",
    "serialized_data = graph.serialize(format='turtle')\n",
    "\n",
    "serialized_lines = serialized_data.splitlines()\n",
    "\n",
    "# Print each line of the serialized data\n",
    "for i in range(2, len(serialized_lines), 2):\n",
    "    line = serialized_lines[i]\n",
    "    print(line)\n",
    "    parts = line.split()\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        # The URL is the third part (index 2) of the split line\n",
    "        url = parts[2]\n",
    "\n",
    "        # Split the URL to extract the domain (namespace)\n",
    "        if '//' in url:\n",
    "            domain = url.split('//')[1].split('/')[1]\n",
    "            if domain[:-1] == \"yagoGeoEntity\":\n",
    "                #print(domain[:-1])\n",
    "                print(line)\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e77b3e7-4694-43fb-a89c-486f1ad90427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500500 lines\n",
      "Processed 1001000 lines\n",
      "Processed 1501500 lines\n",
      "Processed 2002000 lines\n",
      "Processed 2502500 lines\n",
      "Processed 3003000 lines\n",
      "Processed 3503500 lines\n",
      "Processed 4004000 lines\n",
      "Processed 4504500 lines\n",
      "Processed 5005000 lines\n",
      "Processed 5505500 lines\n",
      "Processed 6006000 lines\n",
      "Processed 6506500 lines\n",
      "Processed 7007000 lines\n",
      "Processed 7507500 lines\n",
      "Processed 8008000 lines\n",
      "Processed 8508500 lines\n",
      "Processed 9009000 lines\n",
      "Processed 9509500 lines\n",
      "Processed 10010000 lines\n",
      "Processed 10510500 lines\n",
      "Processed 11011000 lines\n",
      "Processed 11511500 lines\n",
      "Processed 12012000 lines\n",
      "Processed 12512500 lines\n",
      "Processed 13013000 lines\n",
      "Processed 13513500 lines\n",
      "Processed 14014000 lines\n",
      "Processed 14514500 lines\n",
      "Processed 15015000 lines\n",
      "Processed 15515500 lines\n",
      "Processed 16016000 lines\n",
      "Processed 16516500 lines\n",
      "Processed 17017000 lines\n",
      "Processed 17517500 lines\n",
      "Processed 18018000 lines\n",
      "Processed 18518500 lines\n",
      "Processed 19019000 lines\n",
      "Processed 19519500 lines\n",
      "Processed 20020000 lines\n",
      "Processed 20520500 lines\n",
      "Processed 21021000 lines\n",
      "Processed 21521500 lines\n",
      "Processed 22022000 lines\n",
      "Processed 22522500 lines\n",
      "Processed 23023000 lines\n",
      "Processed 23523500 lines\n",
      "Processed 24024000 lines\n",
      "Processed 24524500 lines\n",
      "Processed 25025000 lines\n",
      "Processed 25525500 lines\n",
      "Processed 26026000 lines\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Process the remaining lines in the last chunk\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     line_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#print(f\"Processed {line_count} lines in total\")\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/rdflib/graph.py:1492\u001b[0m, in \u001b[0;36mGraph.parse\u001b[0;34m(self, source, publicID, format, location, file, data, **args)\u001b[0m\n\u001b[1;32m   1489\u001b[0m parser \u001b[38;5;241m=\u001b[39m plugin\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mformat\u001b[39m, Parser)()\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;66;03m# TODO FIXME: Parser.parse should have **kwargs argument.\u001b[39;00m\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m se:\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m could_not_guess_format:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:377\u001b[0m, in \u001b[0;36mNTParser.parse\u001b[0;34m(cls, source, sink, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m         f \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetreader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)(b)\n\u001b[1;32m    376\u001b[0m parser \u001b[38;5;241m=\u001b[39m W3CNTriplesParser(NTGraphSink(sink))\n\u001b[0;32m--> 377\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:196\u001b[0m, in \u001b[0;36mW3CNTriplesParser.parse\u001b[0;34m(self, f, bnode_context)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbnode_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnode_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParseError:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid line: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:239\u001b[0m, in \u001b[0;36mW3CNTriplesParser.parseline\u001b[0;34m(self, bnode_context)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparseline\u001b[39m(\u001b[38;5;28mself\u001b[39m, bnode_context: Optional[_BNodeContextType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_wspace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# The line is empty or a comment\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:260\u001b[0m, in \u001b[0;36mW3CNTriplesParser.eat\u001b[0;34m(self, pattern)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meat\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern: Pattern[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Match[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 260\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mpattern\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m m:  \u001b[38;5;66;03m# @@ Why can't we get the original pattern?\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# print(dir(pattern))\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# print repr(self.line), type(self.line)\u001b[39;00m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to eat \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (pattern\u001b[38;5;241m.\u001b[39mpattern, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path1 = \"./my-data/yago-wd-full-types.nt.gz\"\n",
    "\n",
    "# Create an RDFlib Graph object to store the parsed RDF data\n",
    "graph = Graph()\n",
    "\n",
    "# Open the compressed NT file and process it in chunks\n",
    "try:\n",
    "    with gzip.open(file_path1, 'rb') as f:\n",
    "        line_count = 0\n",
    "        chunk_size = 1000 \n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            line_str = line.decode('utf-8').strip()\n",
    "            chunk.append(line_str)\n",
    "            \n",
    "            if len(chunk) >= chunk_size:\n",
    "                    graph.parse(data='\\n'.join(chunk), format='nt')\n",
    "                    line_count += len(chunk)\n",
    "                    chunk = []  # Reset chunk after processing\n",
    "                    print(f\"Processed {line_count} lines\")\n",
    "            \n",
    "            # Process the remaining lines in the last chunk\n",
    "            if chunk:\n",
    "                graph.parse(data='\\n'.join(chunk), format='nt')\n",
    "                line_count += len(chunk)\n",
    "                #print(f\"Processed {line_count} lines in total\")\n",
    "            \n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing RDF data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2812da77-37ff-4d5d-ac9e-149da0b9470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_serialized_data = graph.serialize(format='turtle')\n",
    "types_serialized_lines = types_serialized_data.splitlines()\n",
    "\n",
    "humans = []\n",
    "locations = []\n",
    "organizations = []\n",
    "others = []\n",
    "\n",
    "for line in types_serialized_lines:\n",
    "    #line = serialized_lines[i]\n",
    "    #print(line)\n",
    "    parts = line.split()\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        url = parts[2]\n",
    "        item = url.rsplit('/', 1)[-1][:-1]\n",
    "        if item == \"Human\":\n",
    "            humans.append(line)\n",
    "        elif item == \"Organization\":\n",
    "            organizations.append(line)\n",
    "        elif item in [\"Location\", \"Place\", \"City\", \"Village\"]:\n",
    "            locations.append(line)\n",
    "        else:\n",
    "            others.append(line)\n",
    "        \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4252a0-6726-48ad-af25-01fa320947cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Humans: 16425\n",
      "Number of Locations: 326\n",
      "Number of Organizations: 367\n",
      "Number of Others: 31066\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Humans:\", len(humans))\n",
    "print(\"Number of Locations:\", len(locations))\n",
    "print(\"Number of Organizations:\", len(organizations))\n",
    "print(\"Number of Others:\", len(others))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f9686-189f-4a4a-9a67-7a50e177420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_serialized_data = graph.serialize(format='turtle')\n",
    "facts_serialized_lines = facts_serialized_data.splitlines()\n",
    "\n",
    "for line in facts_serialized_lines:\n",
    "    #line = serialized_lines[i]\n",
    "    print(line)\n",
    "    parts = line.split()\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        \n",
    "        url = parts[2]\n",
    "        if \"Asteroid\" == url.rsplit('/', 1)[-1][:-1]:\n",
    "            print(line)\n",
    "        \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd6eaab-2c5d-4107-bca2-f24f3239726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "import gzip\n",
    "\n",
    "file_path = \"./my-data/yago-wd-class.nt.gz\"\n",
    "\n",
    "# Create an RDF graph\n",
    "g = Graph()\n",
    "\n",
    "# Open the gzipped RDF file and parse it into the graph\n",
    "with gzip.open(file_path, 'rb') as f:\n",
    "    g.parse(f, format='nt')\n",
    "\n",
    "\n",
    "class_serialized_data = g.serialize(format='turtle')\n",
    "class_serialized_lines = class_serialized_data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05279030-3c58-4c74-a6bd-21df84949d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./my-data/yago-wd-full-types.nt.gz\"\n",
    "\n",
    "# Create an RDFlib Graph object to store the parsed RDF data\n",
    "graph = Graph()\n",
    "cont=0\n",
    "# Open the compressed NT file and process it in chunks\n",
    "try:\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        chunk = []\n",
    "        for line in f:\n",
    "            if cont == 100000:\n",
    "                break\n",
    "            else:\n",
    "                cont += 1\n",
    "                line_str = line.decode('utf-8').strip()\n",
    "                chunk.append(line_str)\n",
    "            \n",
    "        graph.parse(data='\\n'.join(chunk), format='nt')\n",
    "                    \n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{file_path}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing RDF data: {e}\")\n",
    "\n",
    "class_serialized_data = graph.serialize(format='turtle')\n",
    "class_serialized_lines = class_serialized_data.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d59aa289-9b79-4185-8ece-cf8bdc0c7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original lists with duplicates\n",
    "organization_list = [\n",
    "    'Airline', 'Consortium', 'Corporation', 'CollegeOrUniversity', 'ElementarySchool', 'HighSchool', \n",
    "    'MiddleSchool', 'Preschool', 'School', 'FundingScheme', 'GovernmentOrganization', 'LibrarySystem',\n",
    "    'AnimalShelter', 'ArchiveOrganization', 'AutoBodyShop', 'AutoDealer', 'AutoPartsStore', 'AutoRental', \n",
    "    'AutoRepair', 'AutoWash', 'GasStation', 'MotorcycleDealer', 'MotorcycleRepair', 'ChildCare', 'Dentist', \n",
    "    'DryCleaningOrLaundry', 'FireStation', 'Hospital', 'PoliceStation', 'EmploymentAgency', 'AdultEntertainment', \n",
    "    'AmusementPark', 'ArtGallery', 'Casino', 'ComedyClub', 'MovieTheater', 'NightClub', 'AccountingService', \n",
    "    'AutomatedTeller', 'BankOrCreditUnion', 'InsuranceAgency', 'Bakery', 'BarOrPub', 'Brewery', 'CafeOrCoffeeShop', \n",
    "    'Distillery', 'FastFoodRestaurant', 'IceCreamShop', 'Restaurant', 'Winery', 'PostOffice', 'BeautySalon', \n",
    "    'DaySpa', 'HairSalon', 'HealthClub', 'NailSalon', 'TattooParlor', 'Electrician', 'GeneralContractor', \n",
    "    'HVACBusiness', 'HousePainter', 'Locksmith', 'MovingCompany', 'Plumber', 'RoofingContractor', 'InternetCafe', \n",
    "    'Attorney', 'Notary', 'Library', 'BedAndBreakfast', 'Campground', 'Hostel', 'Hotel', 'Motel', 'Resort', \n",
    "    'VacationRental', 'DiagnosticLab', 'NGO', 'NewsMediaOrganization', 'OnlineStore', 'DanceGroup', 'MusicGroup', \n",
    "    'TheaterGroup', 'PoliticalParty', 'FundingAgency', 'ResearchProject', 'ResearchOrganization', \n",
    "    'SearchRescueOrganization', 'SportsTeam', 'WorkersUnion', 'EducationalOrganization', 'LocalBusiness', \n",
    "    'MedicalOrganization', 'OnlineBusiness', 'PerformingGroup', 'Project', 'SportsOrganization'\n",
    "]\n",
    "\n",
    "person_list = [\n",
    "    'Patient', 'Human'\n",
    "]\n",
    "\n",
    "place_list = [\n",
    "    'Apartment', 'CampingPitch', 'SingleFamilyResidence', 'HotelRoom', 'MeetingRoom', 'Suite', 'City', \n",
    "    'Country', 'SchoolDistrict', 'State', 'Airport', 'Aquarium', 'Beach', 'BoatTerminal', 'Bridge', 'BusStation', \n",
    "    'BusStop', 'Campground', 'Cemetery', 'Crematorium', 'CityHall', 'Courthouse', 'DefenceEstablishment', 'Embassy', \n",
    "    'LegislativeBuilding', 'Hospital', 'MovieTheater', 'Museum', 'MusicVenue', 'Park', 'ParkingFacility', \n",
    "    'PerformingArtsTheater', 'BuddhistTemple', 'CatholicChurch', 'HinduTemple', 'Mosque', 'Synagogue', 'Playground', \n",
    "    'PoliceStation', 'PublicToilet', 'RVPark', 'StadiumOrArena', 'SubwayStation', 'TaxiStand', 'TrainStation', 'Zoo', \n",
    "    'Canal', 'LakeBodyOfWater', 'OceanBodyOfWater', 'Pond', 'Reservoir', 'RiverBodyOfWater', 'SeaBodyOfWater', \n",
    "    'Waterfall', 'Continent', 'Mountain', 'Volcano', 'ApartmentComplex', 'GatedResidenceCommunity', 'TouristAttraction', \n",
    "    'TouristDestination', 'Accommodation', 'AdministrativeArea', 'CivicStructure', 'Landform', \n",
    "    'LandmarksOrHistoricalBuildings', 'LocalBusiness', 'Residence'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51144c4a-a093-4648-916e-9ab1ffe27d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORG = []\n",
    "LOC = []\n",
    "PERS = []\n",
    "OTHERS = []\n",
    "\n",
    "for line in class_serialized_lines[3:]:\n",
    "    #print(line)\n",
    "    parts = line.split(' ')\n",
    "    if len(parts) >= 3:\n",
    "        input_string = parts[2].rsplit('/', 1)[-1]\n",
    "        cleaned_string = input_string.rstrip('> .')\n",
    "        \n",
    "        #print(cleaned_string)\n",
    "        \n",
    "        if cleaned_string in organization_list:\n",
    "            ORG.append(line)\n",
    "            #print(f\"ORG: {line}\")\n",
    "        elif cleaned_string in place_list:\n",
    "            LOC.append(line)\n",
    "            #print(f\"LOC: {line}\")\n",
    "        elif cleaned_string in person_list:\n",
    "            PERS.append(line)\n",
    "            #print(f\"PERS: {line}\")\n",
    "        elif cleaned_string != \"\": \n",
    "            OTHERS.append(line)\n",
    "            #print(f\"OTHERS: {line}\")\n",
    "    #print(parts[1].split(' ')[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6863016f-a6a1-48b2-83e1-1b0d7473ca1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<http://yago-knowledge.org/resource/116th_Street_station_(IRT_Ninth_Avenue_Line)> a <http://schema.org/SubwayStation> .',\n",
       " '<http://yago-knowledge.org/resource/25th_Street_station_(BMT_Fifth_Avenue_Line)> a <http://schema.org/SubwayStation> .',\n",
       " '<http://yago-knowledge.org/resource/28th_Street_station_(IRT_Sixth_Avenue_Line)> a <http://schema.org/SubwayStation> .',\n",
       " '<http://yago-knowledge.org/resource/28th_Street_station_(IRT_Third_Avenue_Line)> a <http://schema.org/SubwayStation> .',\n",
       " '<http://yago-knowledge.org/resource/56th_Street_station> a <http://schema.org/SubwayStation> .',\n",
       " '<http://yago-knowledge.org/resource/59th_Street%2FUniversity_of_Chicago_station> a <http://schema.org/TrainStation> .',\n",
       " '<http://yago-knowledge.org/resource/7_April_Stadium_(Qamishli)> a <http://schema.org/StadiumOrArena> .',\n",
       " '<http://yago-knowledge.org/resource/A._Timmasagar> a <http://schema.org/AdministrativeArea> .',\n",
       " '<http://yago-knowledge.org/resource/ANTA_Washington_Square_Theatre> a <http://schema.org/PerformingArtsTheater> .',\n",
       " '<http://yago-knowledge.org/resource/ASB_Bridge> a <http://schema.org/Bridge> .']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOC[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe29c153-c22b-4ed4-a9f0-de2bf0aed2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG = 1543\n",
      "LOC = 5588\n",
      "PERS = 31302\n",
      "OTHERS = 52771\n"
     ]
    }
   ],
   "source": [
    "print(f\"ORG = {len(ORG)}\")\n",
    "print(f\"LOC = {len(LOC)}\")\n",
    "print(f\"PERS = {len(PERS)}\")\n",
    "print(f\"OTHERS = {len(OTHERS)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
