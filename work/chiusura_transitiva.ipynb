{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4c08b0",
   "metadata": {},
   "source": [
    "# Prove per extended type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce44212",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f1d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import time\n",
    "import bz2\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c2e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def retrieve_superclasses(entity_id):\n",
    "    \"\"\"\n",
    "    Retrieve all superclasses of a given Wikidata entity ID.\n",
    "\n",
    "    Args:\n",
    "        entity_id (str): The ID of the entity (e.g., \"Q207784\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are superclass IDs, and values are their labels.\n",
    "    \"\"\"\n",
    "    # Define the SPARQL endpoint and query\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?superclass ?superclassLabel WHERE {{\n",
    "      wd:{entity_id} (wdt:P279)* ?superclass.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to query the SPARQL endpoint with retries\n",
    "    def query_wikidata(sparql_client, query, retries=3, delay=5):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                sparql_client.setQuery(query)\n",
    "                sparql_client.setReturnFormat(JSON)\n",
    "                results = sparql_client.query().convert()\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  # Handle Too Many Requests error\n",
    "                    print(f\"Rate limit hit. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    break\n",
    "        return None\n",
    "\n",
    "    # Set up the SPARQL client\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "\n",
    "    # Execute the query with retries\n",
    "    results = query_wikidata(sparql, query)\n",
    "\n",
    "    # Process results and return as a dictionary\n",
    "    if results:\n",
    "        superclass_dict = {}\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            superclass_id = result[\"superclass\"][\"value\"].split(\"/\")[-1]  # Extract entity ID from the URI\n",
    "            label = result[\"superclassLabel\"][\"value\"]\n",
    "            superclass_dict[superclass_id] = label\n",
    "        return superclass_dict\n",
    "    else:\n",
    "        print(\"Failed to retrieve data after multiple attempts.\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9accc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: [Errno 2] No such file or directory: './data/latest-all.json.bz2'\n"
     ]
    }
   ],
   "source": [
    "wikidata_dump_path = './data/latest-all.json.bz2'\n",
    "SIZE_PROC = 1000\n",
    "chunk_size = 1000  # Number of rows per chunk\n",
    "\n",
    "\n",
    "def process_entity(item):\n",
    "    try:\n",
    "        entity = item['id']\n",
    "        labels = item.get(\"labels\", {})\n",
    "        english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = item.get('descriptions', {}).get('en', {})\n",
    "        NERtype = None\n",
    "\n",
    "        if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "            p31_claims = item[\"claims\"].get(\"P279\", [])\n",
    "\n",
    "            types_list = []\n",
    "\n",
    "            for claim in p31_claims:\n",
    "                mainsnak = claim.get(\"mainsnak\", {})\n",
    "                datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                types_list.append(\"Q\"+str(numeric_id))\n",
    "\n",
    "            return [english_label, types_list]\n",
    "        \n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# Initial setup for data processing\n",
    "counter = 0\n",
    "\n",
    "# Process data and print relevant details\n",
    "try:\n",
    "    with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=SIZE_PROC)\n",
    "        \n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line[:-2])\n",
    "                english_label, types_list = process_entity(item)\n",
    "\n",
    "                print(english_label)\n",
    "                for el in types_list:\n",
    "                    superclasses = retrieve_superclasses(el)  # Replace with your entity ID\n",
    "                    print(f\"[{el}] - Number of Superclasses: {len(superclasses)}\")\n",
    "\n",
    "                print(\"_______________________\")\n",
    "\n",
    "            \n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if counter == SIZE_PROC:\n",
    "                break\n",
    "            counter += 1\n",
    "        pbar.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53638d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "superclasses = retrieve_superclasses(\"Q1320047\")  # Replace with your entity ID\n",
    "print(\"Number of Superclasses:\", len(superclasses))\n",
    "print(\"Superclass Dictionary:\")\n",
    "print(superclasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa110705-1156-4e13-80bf-47118fbe99b7",
   "metadata": {},
   "source": [
    "# Esperimenti fatti con Riccardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa46058-7f05-44fe-8a0c-50de9d7fc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "from json.decoder import JSONDecodeError\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40e8bb-a509-464f-bcbe-ae31bf1ca664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "print(client)\n",
    "\n",
    "log_c = client.wikidata.log\n",
    "items_c = client[DB_NAME].items\n",
    "objects_c = client[DB_NAME].objects\n",
    "literals_c = client[DB_NAME].literals\n",
    "types_c = client[DB_NAME].types\n",
    "\n",
    "c_ref = {\n",
    "    \"items\": items_c,\n",
    "    \"objects\":objects_c, \n",
    "    \"literals\":literals_c, \n",
    "    \"types\":types_c\n",
    "}\n",
    "\n",
    "def flush_buffer(buffer):\n",
    "    for key in buffer:\n",
    "        if len(buffer[key]) > 0:\n",
    "            c_ref[key].insert_many(buffer[key])\n",
    "            buffer[key] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a39dd3-c790-49e0-89cd-31c59c50e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "    --------------------------------------------\n",
    "    For example, if you have an item with types A, B, and C, and you specify a forward property that applies to type B, the item will \n",
    "    be included in the result because it has type B, even if it also has types A and C\n",
    "    --------------------------------------------  \n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b9f8a-d16f-4c7b-bb0d-f54959bc0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with \"capital city\"\n",
    "\n",
    "list = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "for el in list:\n",
    "    data = {\n",
    "        \"json\": [\n",
    "            \"Q\"+(str(el))\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    try:\n",
    "        result = response.json()\n",
    "        label = result[\"Q\"+(str(el))]['labels']['en']\n",
    "        print(label)  # Print the label or ID with indentation\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767029cd-8e59-4753-b58b-e6e882457c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the necessary subclass sets with individual try-except blocks\n",
    "def fetch_wikidata_subclasses():\n",
    "    try:\n",
    "        organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        organization_subclass = []\n",
    "    \n",
    "    try:\n",
    "        country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        country_subclass = []\n",
    "    \n",
    "    try:\n",
    "        city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        city_subclass = []\n",
    "    \n",
    "    try:\n",
    "        capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        capitals_subclass = []\n",
    "    \n",
    "    try:\n",
    "        admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        admTerr_subclass = []\n",
    "    \n",
    "    try:\n",
    "        family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        family_subclass = []\n",
    "    \n",
    "    try:\n",
    "        sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        sportLeague_subclass = []\n",
    "    \n",
    "    try:\n",
    "        venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        venue_subclass = []\n",
    "    \n",
    "    # Removing overlaps for organization_subclass\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    \n",
    "    try:\n",
    "        geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        geolocation_subclass = []\n",
    "    \n",
    "    try:\n",
    "        food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        food_subclass = []\n",
    "    \n",
    "    try:\n",
    "        edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        edInst_subclass = []\n",
    "    \n",
    "    try:\n",
    "        govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        govAgency_subclass = []\n",
    "    \n",
    "    try:\n",
    "        intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        intOrg_subclass = []\n",
    "    \n",
    "    try:\n",
    "        timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        timeZone_subclass = []\n",
    "\n",
    "    # Removing overlaps for geolocation_subclass\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    \n",
    "    return organization_subclass, geolocation_subclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151343c6-b70c-494c-ac88-8272c27a259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./organization_subclass.txt\", \"w\") as file:\n",
    "    for item in organization_subclass:\n",
    "        file.write(f\"{item}\\n\")  # Write each item on a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f6239-fba0-4b0c-9ba0-0aa3ea6865f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "wikidata_dump_path = './data/latest-all.json.bz2'\n",
    "SIZE_PROC = 1000\n",
    "chunk_size = 1000  # Number of rows per chunk\n",
    "\n",
    "organization_subclass, geolocation_subclass = fetch_wikidata_subclasses()\n",
    "\n",
    "def process_entity(item):\n",
    "    try:\n",
    "        entity = item['id']\n",
    "        labels = item.get(\"labels\", {})\n",
    "        english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = item.get('descriptions', {}).get('en', {})\n",
    "        NERtype = None\n",
    "\n",
    "        if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "            p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "\n",
    "            # Initialize a counter to track occurrences of NER types\n",
    "            ner_counter = Counter()\n",
    "\n",
    "            for claim in p31_claims:\n",
    "                mainsnak = claim.get(\"mainsnak\", {})\n",
    "                datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "\n",
    "                # Classify NER types\n",
    "                if numeric_id == 5:\n",
    "                    ner_counter['PERS'] += 1\n",
    "                elif numeric_id in geolocation_subclass or any(k.lower() in description.get('value', '').lower().split() for k in [\"district\", \"city\", \"country\", \"capital\", \"state\"]):\n",
    "                    ner_counter['LOC'] += 1\n",
    "                elif numeric_id in organization_subclass:\n",
    "                    ner_counter['ORG'] += 1\n",
    "                else:\n",
    "                    ner_counter['OTHERS'] += 1\n",
    "                    \n",
    "            # Get the most common NER type\n",
    "            if ner_counter:\n",
    "                NERtype, _ = ner_counter.most_common(1)[0]  # Get the most common type\n",
    "\n",
    "            # Print label, ID, type, and NER classification\n",
    "            print(f\"{english_label} - {entity}: (NER type: {NERtype})\")\n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# Initial setup for data processing\n",
    "counter = 0\n",
    "\n",
    "# Process data and print relevant details\n",
    "try:\n",
    "    with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=SIZE_PROC)\n",
    "        \n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line[:-2])\n",
    "                process_entity(item)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if counter == SIZE_PROC:\n",
    "                break\n",
    "            counter += 1\n",
    "        pbar.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38babb19-53a8-4f95-9615-628c3c41af11",
   "metadata": {},
   "source": [
    "# Test query chiusura transitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a3d5c-62c3-4d59-9972-617e2ed7ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bf23c-fc98-4895-8607-0bd2635eeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_subclasses(Qid):\n",
    "    # Initialize the SPARQL endpoint (Wikidata in this case)\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    # Define the SPARQL query with the provided QID\n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT ?item ?desc WHERE {{\n",
    "      wd:{Qid} wdt:P279* ?item.\n",
    "      ?item rdfs:label ?desc FILTER (lang(?desc) = \"en\").\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the query and the return format (JSON)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    # Execute the query and get results\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Parse the results\n",
    "    subclasses = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        item = result[\"item\"][\"value\"]\n",
    "        desc = result[\"desc\"][\"value\"]\n",
    "        subclasses.append({\"item\": item, \"description\": desc})\n",
    "    \n",
    "    return subclasses\n",
    "\n",
    "# Example usage\n",
    "Qid = \"Q64027599\" \n",
    "subclasses = get_subclasses(Qid)\n",
    "\n",
    "for subclass in subclasses:\n",
    "    print(f\"Item: {subclass['item']}, Description: {subclass['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dac4fe-2d11-4820-af22-02ba0e8dd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"name\": {\n",
    "                            \"query\": \"Belgium\",\n",
    "                            \"boost\": 2.0\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\"term\": {\"type\": \"realm\"}},\n",
    "                {\"term\": {\"type\": \"soverign state\"}},\n",
    "                {\"term\": {\"type\": \"country\"}}\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f06259-d09a-4311-bd15-e859b8f55dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Define the query data (decoded for readability)\n",
    "query_data = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\n",
    "          \"query_string\": {\n",
    "            \"default_field\":\"types\",  \"query\": \"Q6881511\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"name\": {\"query\":\"sinopec\",\"boost\":2.0}\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Define the parameters and token\n",
    "params = {\n",
    "    'name': 'sinopec',\n",
    "    #'query': json.dumps(query_data),  # JSON encoded query data\n",
    "    'token': 'lamapi_demo_2023'\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(url, params=params, headers={'accept': 'application/json'})\n",
    "\n",
    "# Print the response\n",
    "if response.status_code == 200:\n",
    "    res = response.json()\n",
    "    for el in res:\n",
    "        print(f\"{el['name']} ({el['id']}) with type:\")\n",
    "        for type in el['types']:\n",
    "            print(f\"                {type['name']}\")  # Assuming the response is JSON formatted\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
