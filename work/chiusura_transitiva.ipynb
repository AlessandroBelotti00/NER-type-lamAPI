{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4c08b0",
   "metadata": {},
   "source": [
    "# Prove per extended type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce44212",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import time\n",
    "import bz2\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def retrieve_superclasses(entity_id):\n",
    "    \"\"\"\n",
    "    Retrieve all superclasses of a given Wikidata entity ID.\n",
    "\n",
    "    Args:\n",
    "        entity_id (str): The ID of the entity (e.g., \"Q207784\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are superclass IDs, and values are their labels.\n",
    "    \"\"\"\n",
    "    # Define the SPARQL endpoint and query\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    query = f\"\"\"\n",
    "    SELECT ?superclass ?superclassLabel WHERE {{\n",
    "      wd:{entity_id} (wdt:P279)* ?superclass.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to query the SPARQL endpoint with retries\n",
    "    def query_wikidata(sparql_client, query, retries=3, delay=5):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                sparql_client.setQuery(query)\n",
    "                sparql_client.setReturnFormat(JSON)\n",
    "                results = sparql_client.query().convert()\n",
    "                return results\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):  # Handle Too Many Requests error\n",
    "                    print(f\"Rate limit hit. Retrying in {delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    break\n",
    "        return None\n",
    "\n",
    "    # Set up the SPARQL client\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "\n",
    "    # Execute the query with retries\n",
    "    results = query_wikidata(sparql, query)\n",
    "\n",
    "    # Process results and return as a dictionary\n",
    "    if results:\n",
    "        superclass_dict = {}\n",
    "        for result in results[\"results\"][\"bindings\"]:\n",
    "            superclass_id = result[\"superclass\"][\"value\"].split(\"/\")[-1]  # Extract entity ID from the URI\n",
    "            label = result[\"superclassLabel\"][\"value\"]\n",
    "            superclass_dict[superclass_id] = label\n",
    "        return superclass_dict\n",
    "    else:\n",
    "        print(\"Failed to retrieve data after multiple attempts.\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9accc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:40<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belgium\n",
      "[Q3624078] - Number of Superclasses: 49\n",
      "[Q43702] - Number of Superclasses: 36\n",
      "[Q6256] - Number of Superclasses: 23\n",
      "[Q20181813] - Number of Superclasses: 50\n",
      "[Q185441] - Number of Superclasses: 50\n",
      "[Q1250464] - Number of Superclasses: 49\n",
      "[Q113489728] - Number of Superclasses: 50\n",
      "_______________________\n",
      "happiness\n",
      "[Q331769] - Number of Superclasses: 29\n",
      "[Q60539479] - Number of Superclasses: 17\n",
      "_______________________\n",
      "George Washington\n",
      "[Q5] - Number of Superclasses: 25\n",
      "_______________________\n",
      "Jack Bauer\n",
      "[Q15632617] - Number of Superclasses: 21\n",
      "[Q15773317] - Number of Superclasses: 12\n",
      "[Q20085850] - Number of Superclasses: 12\n",
      "_______________________\n",
      "Douglas Adams\n",
      "[Q5] - Number of Superclasses: 25\n",
      "_______________________\n",
      "Paul Otlet\n",
      "[Q5] - Number of Superclasses: 25\n",
      "_______________________\n",
      "Wikidata\n",
      "[Q33120876] - Number of Superclasses: 69\n",
      "[Q638153] - Number of Superclasses: 53\n",
      "[Q36509592] - Number of Superclasses: 55\n",
      "[Q15633582] - Number of Superclasses: 53\n",
      "[Q593744] - Number of Superclasses: 43\n",
      "[Q7094076] - Number of Superclasses: 52\n",
      "[Q33002955] - Number of Superclasses: 47\n",
      "[Q114955954] - Number of Superclasses: 16\n",
      "[Q115471117] - Number of Superclasses: 56\n",
      "[Q1293664] - Number of Superclasses: 6\n",
      "_______________________\n",
      "Portugal\n",
      "[Q3624078] - Number of Superclasses: 49\n",
      "[Q6256] - Number of Superclasses: 23\n",
      "[Q20181813] - Number of Superclasses: 50\n",
      "[Q113489728] - Number of Superclasses: 50\n",
      "_______________________\n",
      "Antarctica\n",
      "[Q5107] - Number of Superclasses: 26\n",
      "[Q82794] - Number of Superclasses: 15\n",
      "[Q312461] - Number of Superclasses: 23\n",
      "[Q2418896] - Number of Superclasses: 16\n",
      "_______________________\n",
      "penis\n",
      "[Q712378] - Number of Superclasses: 18\n",
      "_______________________\n",
      "computer\n",
      "[Q14208553] - Number of Superclasses: 5\n",
      "_______________________\n",
      "Internet\n",
      "[Q11224256] - Number of Superclasses: 14\n",
      "[Q1301371] - Number of Superclasses: 13\n",
      "[Q14208553] - Number of Superclasses: 5\n",
      "_______________________\n",
      "pneumonoultramicroscopicsilicovolcanoconiosis\n",
      "[Q101991] - Number of Superclasses: 29\n",
      "_______________________\n",
      "Supercalifragilisticexpialidocious\n",
      "[Q105543609] - Number of Superclasses: 9\n",
      "_______________________\n",
      "November\n",
      "[Q47018901] - Number of Superclasses: 7\n",
      "_______________________\n",
      "lion\n",
      "[Q16521] - Number of Superclasses: 9\n",
      "_______________________\n",
      "dog\n",
      "[Q55983715] - Number of Superclasses: 9\n",
      "_______________________\n",
      "kitten\n",
      "_______________________\n",
      "People's Republic of China\n",
      "[Q3624078] - Number of Superclasses: 49\n",
      "[Q842112] - Number of Superclasses: 48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(english_label)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m types_list:\n\u001b[0;32m---> 46\u001b[0m     superclasses \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_superclasses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with your entity ID\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - Number of Superclasses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(superclasses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_______________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mretrieve_superclasses\u001b[0;34m(entity_id)\u001b[0m\n\u001b[1;32m     38\u001b[0m sparql \u001b[38;5;241m=\u001b[39m SPARQLWrapper(endpoint_url)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Execute the query with retries\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mquery_wikidata\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Process results and return as a dictionary\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mretrieve_superclasses.<locals>.query_wikidata\u001b[0;34m(sparql_client, query, retries, delay)\u001b[0m\n\u001b[1;32m     24\u001b[0m     sparql_client\u001b[38;5;241m.\u001b[39msetQuery(query)\n\u001b[1;32m     25\u001b[0m     sparql_client\u001b[38;5;241m.\u001b[39msetReturnFormat(JSON)\n\u001b[0;32m---> 26\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msparql_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:960\u001b[0m, in \u001b[0;36mSPARQLWrapper.query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryResult\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    943\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    Execute the query.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/SPARQLWrapper/Wrapper.py:926\u001b[0m, in \u001b[0;36mSPARQLWrapper._query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m         response \u001b[38;5;241m=\u001b[39m urlopener(request, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnFormat\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wikidata_dump_path = './data/latest-all.json.bz2'\n",
    "SIZE_PROC = 1000\n",
    "chunk_size = 1000  # Number of rows per chunk\n",
    "\n",
    "\n",
    "def process_entity(item):\n",
    "    try:\n",
    "        entity = item['id']\n",
    "        labels = item.get(\"labels\", {})\n",
    "        english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = item.get('descriptions', {}).get('en', {})\n",
    "        NERtype = None\n",
    "\n",
    "        if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "            p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "\n",
    "            types_list = []\n",
    "\n",
    "            for claim in p31_claims:\n",
    "                mainsnak = claim.get(\"mainsnak\", {})\n",
    "                datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                types_list.append(\"Q\"+str(numeric_id))\n",
    "\n",
    "            return [english_label, types_list]\n",
    "        \n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# Initial setup for data processing\n",
    "counter = 0\n",
    "\n",
    "# Process data and print relevant details\n",
    "try:\n",
    "    with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=SIZE_PROC)\n",
    "        \n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line[:-2])\n",
    "                english_label, types_list = process_entity(item)\n",
    "\n",
    "                print(english_label)\n",
    "                for el in types_list:\n",
    "                    superclasses = retrieve_superclasses(el)  # Replace with your entity ID\n",
    "                    print(f\"[{el}] - Number of Superclasses: {len(superclasses)}\")\n",
    "\n",
    "                print(\"_______________________\")\n",
    "\n",
    "            \n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if counter == SIZE_PROC:\n",
    "                break\n",
    "            counter += 1\n",
    "        pbar.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53638d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "superclasses = retrieve_superclasses(\"Q1320047\")  # Replace with your entity ID\n",
    "print(\"Number of Superclasses:\", len(superclasses))\n",
    "print(\"Superclass Dictionary:\")\n",
    "print(superclasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa110705-1156-4e13-80bf-47118fbe99b7",
   "metadata": {},
   "source": [
    "# Esperimenti fatti con Riccardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa46058-7f05-44fe-8a0c-50de9d7fc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "from json.decoder import JSONDecodeError\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40e8bb-a509-464f-bcbe-ae31bf1ca664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "print(client)\n",
    "\n",
    "log_c = client.wikidata.log\n",
    "items_c = client[DB_NAME].items\n",
    "objects_c = client[DB_NAME].objects\n",
    "literals_c = client[DB_NAME].literals\n",
    "types_c = client[DB_NAME].types\n",
    "\n",
    "c_ref = {\n",
    "    \"items\": items_c,\n",
    "    \"objects\":objects_c, \n",
    "    \"literals\":literals_c, \n",
    "    \"types\":types_c\n",
    "}\n",
    "\n",
    "def flush_buffer(buffer):\n",
    "    for key in buffer:\n",
    "        if len(buffer[key]) > 0:\n",
    "            c_ref[key].insert_many(buffer[key])\n",
    "            buffer[key] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a39dd3-c790-49e0-89cd-31c59c50e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "    --------------------------------------------\n",
    "    For example, if you have an item with types A, B, and C, and you specify a forward property that applies to type B, the item will \n",
    "    be included in the result because it has type B, even if it also has types A and C\n",
    "    --------------------------------------------  \n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b9f8a-d16f-4c7b-bb0d-f54959bc0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with \"capital city\"\n",
    "\n",
    "list = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "for el in list:\n",
    "    data = {\n",
    "        \"json\": [\n",
    "            \"Q\"+(str(el))\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    try:\n",
    "        result = response.json()\n",
    "        label = result[\"Q\"+(str(el))]['labels']['en']\n",
    "        print(label)  # Print the label or ID with indentation\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767029cd-8e59-4753-b58b-e6e882457c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the necessary subclass sets with individual try-except blocks\n",
    "def fetch_wikidata_subclasses():\n",
    "    try:\n",
    "        organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        organization_subclass = []\n",
    "    \n",
    "    try:\n",
    "        country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        country_subclass = []\n",
    "    \n",
    "    try:\n",
    "        city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        city_subclass = []\n",
    "    \n",
    "    try:\n",
    "        capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        capitals_subclass = []\n",
    "    \n",
    "    try:\n",
    "        admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        admTerr_subclass = []\n",
    "    \n",
    "    try:\n",
    "        family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        family_subclass = []\n",
    "    \n",
    "    try:\n",
    "        sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        sportLeague_subclass = []\n",
    "    \n",
    "    try:\n",
    "        venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        venue_subclass = []\n",
    "    \n",
    "    # Removing overlaps for organization_subclass\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    \n",
    "    try:\n",
    "        geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        geolocation_subclass = []\n",
    "    \n",
    "    try:\n",
    "        food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        food_subclass = []\n",
    "    \n",
    "    try:\n",
    "        edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        edInst_subclass = []\n",
    "    \n",
    "    try:\n",
    "        govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        govAgency_subclass = []\n",
    "    \n",
    "    try:\n",
    "        intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        intOrg_subclass = []\n",
    "    \n",
    "    try:\n",
    "        timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        timeZone_subclass = []\n",
    "\n",
    "    # Removing overlaps for geolocation_subclass\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    \n",
    "    return organization_subclass, geolocation_subclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151343c6-b70c-494c-ac88-8272c27a259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./organization_subclass.txt\", \"w\") as file:\n",
    "    for item in organization_subclass:\n",
    "        file.write(f\"{item}\\n\")  # Write each item on a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f6239-fba0-4b0c-9ba0-0aa3ea6865f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "wikidata_dump_path = './data/latest-all.json.bz2'\n",
    "SIZE_PROC = 1000\n",
    "chunk_size = 1000  # Number of rows per chunk\n",
    "\n",
    "organization_subclass, geolocation_subclass = fetch_wikidata_subclasses()\n",
    "\n",
    "def process_entity(item):\n",
    "    try:\n",
    "        entity = item['id']\n",
    "        labels = item.get(\"labels\", {})\n",
    "        english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = item.get('descriptions', {}).get('en', {})\n",
    "        NERtype = None\n",
    "\n",
    "        if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "            p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "\n",
    "            # Initialize a counter to track occurrences of NER types\n",
    "            ner_counter = Counter()\n",
    "\n",
    "            for claim in p31_claims:\n",
    "                mainsnak = claim.get(\"mainsnak\", {})\n",
    "                datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "\n",
    "                # Classify NER types\n",
    "                if numeric_id == 5:\n",
    "                    ner_counter['PERS'] += 1\n",
    "                elif numeric_id in geolocation_subclass or any(k.lower() in description.get('value', '').lower().split() for k in [\"district\", \"city\", \"country\", \"capital\", \"state\"]):\n",
    "                    ner_counter['LOC'] += 1\n",
    "                elif numeric_id in organization_subclass:\n",
    "                    ner_counter['ORG'] += 1\n",
    "                else:\n",
    "                    ner_counter['OTHERS'] += 1\n",
    "                    \n",
    "            # Get the most common NER type\n",
    "            if ner_counter:\n",
    "                NERtype, _ = ner_counter.most_common(1)[0]  # Get the most common type\n",
    "\n",
    "            # Print label, ID, type, and NER classification\n",
    "            print(f\"{english_label} - {entity}: (NER type: {NERtype})\")\n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# Initial setup for data processing\n",
    "counter = 0\n",
    "\n",
    "# Process data and print relevant details\n",
    "try:\n",
    "    with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=SIZE_PROC)\n",
    "        \n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line[:-2])\n",
    "                process_entity(item)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if counter == SIZE_PROC:\n",
    "                break\n",
    "            counter += 1\n",
    "        pbar.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38babb19-53a8-4f95-9615-628c3c41af11",
   "metadata": {},
   "source": [
    "# Test query chiusura transitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a3d5c-62c3-4d59-9972-617e2ed7ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bf23c-fc98-4895-8607-0bd2635eeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_subclasses(Qid):\n",
    "    # Initialize the SPARQL endpoint (Wikidata in this case)\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    # Define the SPARQL query with the provided QID\n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT ?item ?desc WHERE {{\n",
    "      wd:{Qid} wdt:P279* ?item.\n",
    "      ?item rdfs:label ?desc FILTER (lang(?desc) = \"en\").\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the query and the return format (JSON)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    # Execute the query and get results\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Parse the results\n",
    "    subclasses = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        item = result[\"item\"][\"value\"]\n",
    "        desc = result[\"desc\"][\"value\"]\n",
    "        subclasses.append({\"item\": item, \"description\": desc})\n",
    "    \n",
    "    return subclasses\n",
    "\n",
    "# Example usage\n",
    "Qid = \"Q64027599\" \n",
    "subclasses = get_subclasses(Qid)\n",
    "\n",
    "for subclass in subclasses:\n",
    "    print(f\"Item: {subclass['item']}, Description: {subclass['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dac4fe-2d11-4820-af22-02ba0e8dd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"name\": {\n",
    "                            \"query\": \"Belgium\",\n",
    "                            \"boost\": 2.0\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\"term\": {\"type\": \"realm\"}},\n",
    "                {\"term\": {\"type\": \"soverign state\"}},\n",
    "                {\"term\": {\"type\": \"country\"}}\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f06259-d09a-4311-bd15-e859b8f55dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Define the query data (decoded for readability)\n",
    "query_data = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\n",
    "          \"query_string\": {\n",
    "            \"default_field\":\"types\",  \"query\": \"Q6881511\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"name\": {\"query\":\"sinopec\",\"boost\":2.0}\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Define the parameters and token\n",
    "params = {\n",
    "    'name': 'sinopec',\n",
    "    #'query': json.dumps(query_data),  # JSON encoded query data\n",
    "    'token': 'lamapi_demo_2023'\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(url, params=params, headers={'accept': 'application/json'})\n",
    "\n",
    "# Print the response\n",
    "if response.status_code == 200:\n",
    "    res = response.json()\n",
    "    for el in res:\n",
    "        print(f\"{el['name']} ({el['id']}) with type:\")\n",
    "        for type in el['types']:\n",
    "            print(f\"                {type['name']}\")  # Assuming the response is JSON formatted\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
