{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa46058-7f05-44fe-8a0c-50de9d7fc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "from json.decoder import JSONDecodeError\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb40e8bb-a509-464f-bcbe-ae31bf1ca664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['mongo:27017'], document_class=dict, tz_aware=False, connect=True)\n"
     ]
    }
   ],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "print(client)\n",
    "\n",
    "log_c = client.wikidata.log\n",
    "items_c = client[DB_NAME].items\n",
    "objects_c = client[DB_NAME].objects\n",
    "literals_c = client[DB_NAME].literals\n",
    "types_c = client[DB_NAME].types\n",
    "\n",
    "c_ref = {\n",
    "    \"items\": items_c,\n",
    "    \"objects\":objects_c, \n",
    "    \"literals\":literals_c, \n",
    "    \"types\":types_c\n",
    "}\n",
    "\n",
    "def flush_buffer(buffer):\n",
    "    for key in buffer:\n",
    "        if len(buffer[key]) > 0:\n",
    "            c_ref[key].insert_many(buffer[key])\n",
    "            buffer[key] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a39dd3-c790-49e0-89cd-31c59c50e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "    --------------------------------------------\n",
    "    For example, if you have an item with types A, B, and C, and you specify a forward property that applies to type B, the item will \n",
    "    be included in the result because it has type B, even if it also has types A and C\n",
    "    --------------------------------------------  \n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1b9f8a-d16f-4c7b-bb0d-f54959bc0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital city\n",
      "temporary capital\n",
      "Amsterdam as a gay capital\n",
      "summer capital\n",
      "capital of regency\n",
      "capital of Korea\n",
      "Capital of Brazil\n",
      "Capital in Africa\n",
      "state capital\n",
      "commercial capital\n",
      "capital city wall\n",
      "capital of region\n",
      "municipality seat\n",
      "capital of Indonesia\n",
      "capital of county in Romania\n",
      "prefectural capital of Japan\n",
      "provincial capital\n",
      "Winter capital\n",
      "legislative capital\n",
      "executive capital\n",
      "judicial capital\n",
      "national capital\n",
      "state capital in Germany\n",
      "provincial or territorial capital city in Canada\n",
      "state or insular area capital of the United States\n",
      "cabecera municipal\n",
      "seat of the local council\n",
      "urban municipality in Germany\n",
      "barrio-pueblo of Puerto Rico\n",
      "municipality capital (Spain)\n",
      "zona urbana in Puerto Rico\n",
      "federal capital\n",
      "capital of Japan\n",
      "capital of Russia\n",
      "Capital of Republic of China (Taiwan)\n",
      "planned capital city\n",
      "Capital of Sri Lanka\n",
      "New Administrative Capital\n",
      "independent city of Germany\n",
      "Greater district town\n",
      "large district town\n",
      "large independent city of Lower Saxony\n",
      "city with special status\n",
      "medium-sized district town\n",
      "City district in Baden-Württemberg\n",
      "independent city of Saxony-Anhalt\n",
      "urban district in Saxony\n",
      "urban district of Thuringia\n",
      "district independent city of Brandenburg\n",
      "urban district in Schleswig-Holstein\n",
      "urban district of Hesse\n",
      "urban district of Bavaria\n",
      "urban district of Rhineland-Palatinate\n",
      "urban district of Mecklenburg-Vorpommern\n",
      "urban district of North Rhine-Westphalia\n",
      "urban district of Bremen\n",
      "urban district of Lower Saxony\n",
      "urban district of Hamburg\n",
      "independent city in Berlin\n",
      "Greater district town in Baden-Württemberg\n"
     ]
    }
   ],
   "source": [
    "# example with \"capital city\"\n",
    "\n",
    "list = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "for el in list:\n",
    "    data = {\n",
    "        \"json\": [\n",
    "            \"Q\"+(str(el))\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    try:\n",
    "        result = response.json()\n",
    "        label = result[\"Q\"+(str(el))]['labels']['en']\n",
    "        print(label)  # Print the label or ID with indentation\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767029cd-8e59-4753-b58b-e6e882457c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch the necessary subclass sets with individual try-except blocks\n",
    "def fetch_wikidata_subclasses():\n",
    "    try:\n",
    "        organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        organization_subclass = []\n",
    "    \n",
    "    try:\n",
    "        country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        country_subclass = []\n",
    "    \n",
    "    try:\n",
    "        city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        city_subclass = []\n",
    "    \n",
    "    try:\n",
    "        capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        capitals_subclass = []\n",
    "    \n",
    "    try:\n",
    "        admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        admTerr_subclass = []\n",
    "    \n",
    "    try:\n",
    "        family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        family_subclass = []\n",
    "    \n",
    "    try:\n",
    "        sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        sportLeague_subclass = []\n",
    "    \n",
    "    try:\n",
    "        venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        venue_subclass = []\n",
    "    \n",
    "    # Removing overlaps for organization_subclass\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    \n",
    "    try:\n",
    "        geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        geolocation_subclass = []\n",
    "    \n",
    "    try:\n",
    "        food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        food_subclass = []\n",
    "    \n",
    "    try:\n",
    "        edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        edInst_subclass = []\n",
    "    \n",
    "    try:\n",
    "        govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        govAgency_subclass = []\n",
    "    \n",
    "    try:\n",
    "        intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        intOrg_subclass = []\n",
    "    \n",
    "    try:\n",
    "        timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        timeZone_subclass = []\n",
    "\n",
    "    # Removing overlaps for geolocation_subclass\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    \n",
    "    return organization_subclass, geolocation_subclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "151343c6-b70c-494c-ac88-8272c27a259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"./organization_subclass.txt\", \"w\") as file:\n",
    "    for item in organization_subclass:\n",
    "        file.write(f\"{item}\\n\")  # Write each item on a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f1f6239-fba0-4b0c-9ba0-0aa3ea6865f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (4038966612.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "wikidata_dump_path = './data/latest-all.json.bz2'\n",
    "SIZE_PROC = 1000\n",
    "chunk_size = 1000  # Number of rows per chunk\n",
    "\n",
    "organization_subclass, geolocation_subclass = fetch_wikidata_subclasses()\n",
    "\n",
    "def process_entity(item):\n",
    "    try:\n",
    "        entity = item['id']\n",
    "        labels = item.get(\"labels\", {})\n",
    "        english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = item.get('descriptions', {}).get('en', {})\n",
    "        NERtype = None\n",
    "\n",
    "        if item.get(\"type\") == \"item\" and \"claims\" in item:\n",
    "            p31_claims = item[\"claims\"].get(\"P31\", [])\n",
    "\n",
    "            # Initialize a counter to track occurrences of NER types\n",
    "            ner_counter = Counter()\n",
    "\n",
    "            for claim in p31_claims:\n",
    "                mainsnak = claim.get(\"mainsnak\", {})\n",
    "                datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "\n",
    "                # Classify NER types\n",
    "                if numeric_id == 5:\n",
    "                    ner_counter['PERS'] += 1\n",
    "                elif numeric_id in geolocation_subclass or any(k.lower() in description.get('value', '').lower().split() for k in [\"district\", \"city\", \"country\", \"capital\", \"state\"]):\n",
    "                    ner_counter['LOC'] += 1\n",
    "                elif numeric_id in organization_subclass:\n",
    "                    ner_counter['ORG'] += 1\n",
    "                else:\n",
    "                    ner_counter['OTHERS'] += 1\n",
    "                    \n",
    "            # Get the most common NER type\n",
    "            if ner_counter:\n",
    "                NERtype, _ = ner_counter.most_common(1)[0]  # Get the most common type\n",
    "\n",
    "            # Print label, ID, type, and NER classification\n",
    "            print(f\"{english_label} - {entity}: (NER type: {NERtype})\")\n",
    "\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "# Initial setup for data processing\n",
    "counter = 0\n",
    "\n",
    "# Process data and print relevant details\n",
    "try:\n",
    "    with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=SIZE_PROC)\n",
    "        \n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line[:-2])\n",
    "                process_entity(item)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            if counter == SIZE_PROC:\n",
    "                break\n",
    "            counter += 1\n",
    "        pbar.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38babb19-53a8-4f95-9615-628c3c41af11",
   "metadata": {},
   "source": [
    "# Test query chiusura transitiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a13a3d5c-62c3-4d59-9972-617e2ed7ad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SPARQLWrapper\n",
      "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
      "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
      "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: isodate, rdflib, SPARQLWrapper\n",
      "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-7.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae9bf23c-fc98-4895-8607-0bd2635eeb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: http://www.wikidata.org/entity/Q35120, Description: entity\n",
      "Item: http://www.wikidata.org/entity/Q43229, Description: organization\n",
      "Item: http://www.wikidata.org/entity/Q58778, Description: system\n",
      "Item: http://www.wikidata.org/entity/Q167037, Description: corporation\n",
      "Item: http://www.wikidata.org/entity/Q155076, Description: juridical person\n",
      "Item: http://www.wikidata.org/entity/Q488383, Description: object\n",
      "Item: http://www.wikidata.org/entity/Q783794, Description: company\n",
      "Item: http://www.wikidata.org/entity/Q726870, Description: brick and mortar\n",
      "Item: http://www.wikidata.org/entity/Q507619, Description: retail chain\n",
      "Item: http://www.wikidata.org/entity/Q1639378, Description: social system\n",
      "Item: http://www.wikidata.org/entity/Q854457, Description: complex system\n",
      "Item: http://www.wikidata.org/entity/Q4830453, Description: business\n",
      "Item: http://www.wikidata.org/entity/Q1762621, Description: vendor\n",
      "Item: http://www.wikidata.org/entity/Q3778211, Description: legal person\n",
      "Item: http://www.wikidata.org/entity/Q5127848, Description: class\n",
      "Item: http://www.wikidata.org/entity/Q16889133, Description: class\n",
      "Item: http://www.wikidata.org/entity/Q7048977, Description: abstract entity\n",
      "Item: http://www.wikidata.org/entity/Q12569864, Description: economic entity\n",
      "Item: http://www.wikidata.org/entity/Q106559804, Description: person or organization\n",
      "Item: http://www.wikidata.org/entity/Q103940464, Description: continuant\n",
      "Item: http://www.wikidata.org/entity/Q24229398, Description: agent\n",
      "Item: http://www.wikidata.org/entity/Q21980538, Description: commercial organization\n",
      "Item: http://www.wikidata.org/entity/Q53617489, Description: independent continuant\n",
      "Item: http://www.wikidata.org/entity/Q6881511, Description: enterprise\n",
      "Item: http://www.wikidata.org/entity/Q99527517, Description: collective entity\n",
      "Item: http://www.wikidata.org/entity/Q64027599, Description: gas station chain\n",
      "Item: http://www.wikidata.org/entity/Q65553774, Description: chain\n",
      "Item: http://www.wikidata.org/entity/Q99536263, Description: retailer\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "def get_subclasses(Qid):\n",
    "    # Initialize the SPARQL endpoint (Wikidata in this case)\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    # Define the SPARQL query with the provided QID\n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT ?item ?desc WHERE {{\n",
    "      wd:{Qid} wdt:P279* ?item.\n",
    "      ?item rdfs:label ?desc FILTER (lang(?desc) = \"en\").\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the query and the return format (JSON)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    # Execute the query and get results\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Parse the results\n",
    "    subclasses = []\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        item = result[\"item\"][\"value\"]\n",
    "        desc = result[\"desc\"][\"value\"]\n",
    "        subclasses.append({\"item\": item, \"description\": desc})\n",
    "    \n",
    "    return subclasses\n",
    "\n",
    "# Example usage\n",
    "Qid = \"Q64027599\" \n",
    "subclasses = get_subclasses(Qid)\n",
    "\n",
    "for subclass in subclasses:\n",
    "    print(f\"Item: {subclass['item']}, Description: {subclass['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dac4fe-2d11-4820-af22-02ba0e8dd985",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                    \"match\": {\n",
    "                        \"name\": {\n",
    "                            \"query\": \"Belgium\",\n",
    "                            \"boost\": 2.0\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"should\": [\n",
    "                {\"term\": {\"type\": \"realm\"}},\n",
    "                {\"term\": {\"type\": \"soverign state\"}},\n",
    "                {\"term\": {\"type\": \"country\"}}\n",
    "            ],\n",
    "            \"minimum_should_match\": 1\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65f06259-d09a-4311-bd15-e859b8f55dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinopec (Q2634317) with type:\n",
      "                business\n",
      "Sinopec (Q831445) with type:\n",
      "                business\n",
      "                enterprise\n",
      "                public company\n",
      "Sinopec SSC (Q20117829) with type:\n",
      "                business\n",
      "                public company\n",
      "Sinopec Shanghai Petrochemical (Q820770) with type:\n",
      "                business\n",
      "                public company\n",
      "Sinopec Tower (Q10875936) with type:\n",
      "Sinopec Yanshan Petrochemical Company (Q15942644) with type:\n",
      "                business\n",
      "Soronko:RecentChanges (Q105429923) with type:\n",
      "                MediaWiki special page\n",
      "Yanbu Aramco Sinopec Refining Company (YASREF) Ltd (Q22689998) with type:\n",
      "                oil refinery\n",
      "2008 Formula 1 Sinopec Chinese Grand Prix (Q179363) with type:\n",
      "                Chinese Grand Prix\n",
      "II Sinopec Chinese Grand Prix (Q220824) with type:\n",
      "                Chinese Grand Prix\n",
      "2007-06-23: Prezes chińskiego Sinopec Corp nagle zrezygnował z funkcji (Q17923684) with type:\n",
      "                Wikinews article\n",
      "I Sinopec Chinese Grand Prix (Q173192) with type:\n",
      "                Chinese Grand Prix\n",
      "2007 Formula 1 Sinopec Chinese Grand Prix (Q220853) with type:\n",
      "                Chinese Grand Prix\n",
      "III Sinopec Chinese Grand Prix (Q220844) with type:\n",
      "                Chinese Grand Prix\n",
      "Sinopec Yizeng Chemical Fibre Company Limited (Q8053966) with type:\n",
      "                business\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Define the query data (decoded for readability)\n",
    "query_data = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        {\n",
    "          \"query_string\": {\n",
    "            \"default_field\":\"types\",  \"query\": \"Q6881511\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"name\": {\"query\":\"sinopec\",\"boost\":2.0}\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Define the parameters and token\n",
    "params = {\n",
    "    'name': 'sinopec',\n",
    "    #'query': json.dumps(query_data),  # JSON encoded query data\n",
    "    'token': 'lamapi_demo_2023'\n",
    "}\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(url, params=params, headers={'accept': 'application/json'})\n",
    "\n",
    "# Print the response\n",
    "if response.status_code == 200:\n",
    "    res = response.json()\n",
    "    for el in res:\n",
    "        print(f\"{el['name']} ({el['id']}) with type:\")\n",
    "        for type in el['types']:\n",
    "            print(f\"                {type['name']}\")  # Assuming the response is JSON formatted\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
