{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56057aa9-2c63-4043-8f80-4878cc54977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backoff in /opt/conda/lib/python3.11/site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffeea9db-4845-49a4-9c53-eb5f82205e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import re\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import backoff\n",
    "from tqdm.asyncio import tqdm\n",
    "from aiohttp.client_exceptions import ClientResponseError  # Add this import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e3b4f-742c-4391-8b3d-cba4dcc2f85a",
   "metadata": {},
   "source": [
    "# HardTableR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4588250-95fa-4706-b68e-a34b280fa7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:30<00:00, 33.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GT_file = './data/dataset_GT/HardTableR3-2021_f3.csv'\n",
    "\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            if row['target'] == 1:\n",
    "                ids[row[\"key\"]] = {\n",
    "                    \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"ed_score\": row['ed_score'],\n",
    "                    \"jaccard_score\": row['jaccard_score']\n",
    "                }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c8ae5f-7fd0-4f95-86a2-278b2c4abdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10779/10779 [03:06<00:00, 57.67it/s]  \n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/HardTablesR3/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR3/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    pattern = r'^\\.'\n",
    "    if re.match(pattern, table):\n",
    "        continue\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0775aab2-0c66-4e8f-9873-8b78f8160600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/HardTablesR3/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22ed76f-c6f8-4193-9dcd-96dbf2820566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted mentions saved to ./data/HardTablesR3_sorted_mentions.json\n"
     ]
    }
   ],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/HardTablesR3_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f360f-7c44-4b34-9f4f-793116902981",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/HardTablesR3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HT2_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893f18d-b456-4088-ae10-1c90925c11d3",
   "metadata": {},
   "source": [
    "# HardTableR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438beab6-4a31-44a0-afe7-7e3e0cc0f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 851/851 [00:11<00:00, 72.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GT_file = './data/dataset_GT/HardTableR2-2021_f3.csv'\n",
    "\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            if row['target'] == 1:\n",
    "                ids[row[\"key\"]] = {\n",
    "                    \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"ed_score\": row['ed_score'],\n",
    "                    \"jaccard_score\": row['jaccard_score']\n",
    "                }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8e1ff4-1705-4102-80c6-d37eb334582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2692/2692 [00:38<00:00, 70.75it/s] \n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    pattern = r'^\\.'\n",
    "    if re.match(pattern, table):\n",
    "        continue\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb507524-75b6-4bf6-a769-adf44c8d101e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 12.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cc5728-fa83-4607-b5bc-18278e8de7e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mentions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sorted_mentions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mmentions\u001b[49m\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124med_score\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m json_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/HardTablesR2_sorted_mentions.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save the sorted_mentions dictionary to a JSON file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mentions' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/HardTablesR2_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3ab3d4-8ff2-4706-909a-c6aa080afffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/HardTablesR2_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HT2_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900192a-11ff-4025-bc1f-3d19f1116a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in HT2_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in HT2_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6441c-63b7-465a-882b-12948c971b45",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50234a55-317f-4e3a-8689-dac600acbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(HT2_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = HT2_sorted_mentions[:q1_idx]\n",
    "q2 = HT2_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = HT2_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = HT2_sorted_mentions[q3_idx:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872586bb-154c-4cda-b922-64ace56d3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "HT2_sample_keys = []\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q1, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q2, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q3, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b1aa2-8d40-411e-89bf-732860f952e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in HT2_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in HT2_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7b45b-618b-48b4-b4f6-410ccc2fc7c7",
   "metadata": {},
   "source": [
    "# Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501c94f-e932-4097-b7e4-599d151fa4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "        'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "        print(data)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R1_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in HT2_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                \n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id)[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of HardTableR2: {cont_el / len(HT2_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of HardTableR2: {m_mrr / len(HT2_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(HT2_sample_keys))\n",
    "        asyncio.run(main(HT2_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(HT2_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f0321-a4ae-4508-81da-73fcdc788ff4",
   "metadata": {},
   "source": [
    "## query\n",
    "Coverage of HardTableR2: 0.99\n",
    "\r\n",
    "Measure Reciprocal Rank of HardTableR2: 0.912763999999958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68053e9-a134-4254-a150-45b5668bc24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "from tqdm.asyncio import tqdm  # Import the asynchronous version of tqdm\n",
    "import re\n",
    "import nest_asyncio\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        ],\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            await pbar.update(1)  # Await the progress bar update\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    await pbar.update(1)  # Await the progress bar update\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        async with tqdm(total=len(string_name_list)) as pbar:\n",
    "            for name, type in string_name_list.items():\n",
    "                tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "            \n",
    "            results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id[0])[0]\n",
    "                \n",
    "                response = requests.get(url, params=params, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                await pbar.update(1)  # Await the progress bar update\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca6421-7158-468a-8db8-7f23670fd1fc",
   "metadata": {},
   "source": [
    "# Round1_T2D_f3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "352ef6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [00:03<00:00, 41.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GT_file = './data/dataset_GT/Round1_T2D_f3.csv'\n",
    "\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            if row['target'] == 1:\n",
    "                ids[row[\"key\"]] = {\n",
    "                    \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"ed_score\": row['ed_score'],\n",
    "                    \"jaccard_score\": row['jaccard_score']\n",
    "                }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42cc24f3-289d-4e50-a8e0-d2f7385a10db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:03<00:00, 19.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "019789e6-0ac5-4f77-9b9e-a68d9ccb44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fcd61-91d8-4428-bf63-b4865ed7464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1e9063-4b83-49e1-97a6-e82d9ebfae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62327f8-cdfb-4c72-91d8-9a24c82ea65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R1_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R1_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac2be8-a8ef-48ef-846c-68c4df2ebdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcf5c8-742b-41d8-bc6b-6899751e3c71",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39eea9d7-8d1b-4f56-9450-a8564fe1edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61221ee6-d837-453e-9e51-5240717cf1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb108a-226f-4247-9610-011db4e5184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R1_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R1_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6316cb2-8719-4676-8b58-84c7f734a0a0",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c488ea-fe0b-4c9f-a842-803c6f60f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed47fad-1b10-4bd7-82fa-5db7a54916a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "        'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R1_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id)[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(R1_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(R1_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R1_sample_keys))\n",
    "        asyncio.run(main(R1_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R1_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92aafa-6ddf-4991-9e52-8ef02164fb15",
   "metadata": {},
   "source": [
    "### query\n",
    "Coverage of R1: 0.962\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.94661674999995793"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65fe14-c716-462e-b419-55d9475e5555",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round3_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dcd1b-493f-4de4-97b1-b57963feecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/Round3_f3.csv'\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            if row['target'] == 1:\n",
    "                ids[row[\"key\"]] = {\n",
    "                    \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                    \"name\": row['name'],\n",
    "                    \"ed_score\": row['ed_score'],\n",
    "                    \"jaccard_score\": row['jaccard_score']\n",
    "                }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e42600-5529-4f4c-82df-aef189b889d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e904f-c7c7-40a7-8660-3c95569810eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f2b55-1d9e-4594-ad61-12e2320d76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465a6cc8-4d3b-4fc7-bf7a-7785600e3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc843c-3a51-4bc3-a761-bf6cf0406678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R3_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R3_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d50e2-a75d-4464-8d65-07058466d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abe44c-da17-47b6-8608-0eda5cf4bce3",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f188f0a1-d65d-407c-995f-9bb659c15cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R3_sorted_mentions[:q1_idx]\n",
    "q2 = R3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R3_sorted_mentions[q3_idx:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19eb848-5633-4314-831c-0b1687a9bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "R3_sample_keys = []\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q1, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q2, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q3, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687287f-cc9f-4e97-99c9-e26f2bbc9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R3_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R3_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ff417-c555-4121-a23f-9d741be9ca3c",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790a441-6042-4517-b278-474fedab80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "974723e2-9aeb-4113-9cc0-6474bc08b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 3971/4000 [07:30<00:03,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R3: 0.98625\n",
      "Measure Reciprocal Rank of R3: 0.9505167499999503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "        'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R3_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R3_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id)[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R3: {cont_el / len(R3_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R3: {m_mrr / len(R3_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R3_sample_keys))\n",
    "        asyncio.run(main(R3_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R3_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d52bb4-d962-40e8-a6d0-bc8c0e388975",
   "metadata": {},
   "source": [
    "### query\n",
    "Coverage of R3: 0.98625\n",
    "\r\n",
    "Measure Reciprocal Rank of R3: 0.95051674999995032"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a53e3-e5fe-46d9-bc52-244e7802b197",
   "metadata": {},
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21682bc5-8812-4cef-b596-0160c59421c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/2T-2020_f3.csv'\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            ids[row[\"key\"]] = {\n",
    "                \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                \"name\": row['name'],\n",
    "                \"ed_score\": row['ed_score'],\n",
    "                \"jaccard_score\": row['jaccard_score']\n",
    "            }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b44b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/2T_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_2020/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3b254-fd9b-4b32-a799-5c243c8daedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/2T_2020/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee2419-0618-4f43-88b4-087bfa1b8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c16bb8b0-9ae2-4774-87f5-4b0b2a3170a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05332528-eedf-4027-bfda-d5d9e784c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R4_2T_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R4_2T_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb8cf3-f85a-4dc1-a5ce-19455a021584",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b681548a-3abe-4812-97d9-3bcee7163222",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db422eb9-a3ab-431c-b932-78e92497ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "119033ab-6741-4ada-8d44-c2925cf6cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cf738-ad6c-43bd-b6e9-967615fa8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R4_2T_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R4_2T_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714da0e-7477-4bf5-bf59-2f0f45881078",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d767ab4c-ac3f-488e-9115-b041dbbca58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480ac852-dca4-4689-b45e-0d21b5dff1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 1347/4000 [46:25<1:31:26,  2.07s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R4 2T: 0.31425\n",
      "Measure Reciprocal Rank of R4 2T: 0.30396625000000227\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 100,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"should\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "        'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R4_2T_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"should\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id)[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R4 2T: {cont_el / len(R4_2T_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R4 2T: {m_mrr / len(R4_2T_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R4_2T_sample_keys))\n",
    "        asyncio.run(main(R4_2T_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R4_2T_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258d622-4602-458c-bd9e-802885c56bac",
   "metadata": {},
   "source": [
    "### query \n",
    "Coverage of R4 2T: 0.5255\n",
    "\r\n",
    "Measure Reciprocal Rank of R4 2T: 0.5184012500000062"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60955944",
   "metadata": {},
   "source": [
    "# Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3793c96-1cdd-48fb-aaca-1c84d7ed1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_file = './data/dataset_GT/Round4_f3.csv'\n",
    "chunk_size = 1000  # Adjust this based on your memory constraints\n",
    "\n",
    "ids = {}\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "total_rows = sum(1 for line in open(GT_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for chunk_GT in pd.read_csv(GT_file, chunksize=chunk_size):\n",
    "        items = chunk_GT[chunk_GT['target'] == 1]\n",
    "        for _, row in items.iterrows():\n",
    "            \n",
    "            if row['name'] == \"imo 9528017\":\n",
    "                print(row)\n",
    "                break\n",
    "\n",
    "            \n",
    "            ids[row[\"key\"]] = {\n",
    "                \"id\": 'https://www.wikidata.org/entity/' + row['id'],\n",
    "                \"name\": row['name'],\n",
    "                \"ed_score\": row['ed_score'],\n",
    "                \"jaccard_score\": row['jaccard_score']\n",
    "            }\n",
    "        pbar.update(1)  # Update progress bar for each chunk iteration\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a3d73-a6d9-4537-bf8d-02d55391fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = set(df[\"key\"].values)\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                key_to_cell[key] = cell_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17cd3ec-c29e-48c4-8db9-3090849293f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        parts = row['url'].split('/')\n",
    "        wikidata_id = parts[-1]\n",
    "        num_rows, num_columns = df.shape\n",
    "        key = f\"{row['table_name']} {row['row']} {row['col']}\"\n",
    "        if key in ids:\n",
    "            cell_value = key_to_cell[key]\n",
    "            data = ids[key]\n",
    "            mentions[cell_value] = data\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38394dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mentions = sorted(mentions.items(), key=lambda x: x[1][\"ed_score\"])\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(sorted_mentions, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df2b9e56-0bcc-4cdb-9bb3-a23e505faf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998da0eb-bd65-4ce4-bd49-88610ce63963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ed_score and jaccard_score values\n",
    "ed_scores = [item[1]['ed_score'] for item in R4_sorted_mentions]\n",
    "jaccard_scores = [item[1]['jaccard_score'] for item in R4_sorted_mentions]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier analysis\n",
    "df = pd.DataFrame({'ED Score': ed_scores, 'Jaccard Score': jaccard_scores})\n",
    "\n",
    "# Density Plot for ED Score and Jaccard Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for ED Score\n",
    "sns.kdeplot(df['ED Score'], fill=True, label='ED Score')\n",
    "\n",
    "# Plot for Jaccard Score\n",
    "sns.kdeplot(df['Jaccard Score'], fill=True, label='Jaccard Score')\n",
    "\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ED and Jaccard Scores')\n",
    "plt.legend(loc='upper left')  # Show legend with labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673328d-fba3-48b5-8f8a-b74f67b7f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a6451-1289-489b-937f-a42891bd741a",
   "metadata": {},
   "source": [
    "## Sample extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b420df23-d18e-460b-9471-0915d141b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_sorted_mentions[:q1_idx]\n",
    "q2 = R4_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_sorted_mentions[q3_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ab86609-191e-4574-96e5-209b44bf850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000 \n",
    "R4_sample_keys = []\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q1, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q2, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q3, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q4, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c115e-6e5b-4300-9b6a-6c7764c5a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting ED scores and Jaccard scores\n",
    "ed_scores = [score[1]['ed_score'] for score in R4_sample_keys]\n",
    "jaccard_scores = [score[1]['jaccard_score'] for score in R4_sample_keys]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "sns.kdeplot(ed_scores, color='skyblue', label='Edit Distance Score', fill=True)\n",
    "sns.kdeplot(jaccard_scores, color='salmon', label='Jaccard Score', fill=True)\n",
    "\n",
    "plt.title('Edit Distance and Jaccard Score Density')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9c307c-a769-4192-82b5-f9b4711b974e",
   "metadata": {},
   "source": [
    "## Coverage Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8c75-5a9d-4d30-915d-95739e4d7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "434f331b-1795-4c53-b319-278f35be76f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3984/4000 [07:36<00:01,  8.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R4: 0.99375\n",
      "Measure Reciprocal Rank of R4: 0.9421867499999452\n"
     ]
    }
   ],
   "source": [
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, el, string_name_list, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': str(el),\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{el}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "        'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{el}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', string_name_list[el])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(R4_sample_keys, url, pbar):\n",
    "    string_name_list = {item[1]['name']: item[1]['id'] for item in R4_sample_keys}\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for el in string_name_list.keys():\n",
    "            tasks.append(process_item(session, el, string_name_list, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id)[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R4: {cont_el / len(R4_sample_keys)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R4: {m_mrr / len(R4_sample_keys)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(R4_sample_keys))\n",
    "        asyncio.run(main(R4_sample_keys, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(R4_sample_keys, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d59524-0dda-462a-a27c-16f55864af53",
   "metadata": {},
   "source": [
    "### query \n",
    "Coverage of R4: 0.99375\n",
    "\r\n",
    "Measure Reciprocal Rank of R4: 0.94218674999994523"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8d5dc-9d79-4445-9fbf-68f30328c8a1",
   "metadata": {},
   "source": [
    "## Datasets Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2ea98-7025-484e-9cb3-d0cd69c20867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(data):\n",
    "    return [item[1]['ed_score'] for item in data]\n",
    "\n",
    "ed_scores_R1 = extract_scores(R1_sample_keys)\n",
    "ed_scores_R3 = extract_scores(R3_sample_keys)\n",
    "ed_scores_R4 = extract_scores(R4_sample_keys)\n",
    "ed_scores_R4_2T = extract_scores(R4_2T_sample_keys)\n",
    "\n",
    "# Plot the KDE plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.kdeplot(ed_scores_R1, color='skyblue', label='R1 Edit Distance Score', fill=True)\n",
    "sns.kdeplot(ed_scores_R3, color='green', label='R3 Edit Distance Score', fill=True)\n",
    "sns.kdeplot(ed_scores_R4, color='red', label='R4 Edit Distance Score', fill=True)\n",
    "sns.kdeplot(ed_scores_R4_2T, color='purple', label='R4_2T Edit Distance Score', fill=True)\n",
    "\n",
    "plt.xlabel('Edit Distance Score')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Edit Distance Scores for Different Rounds')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
