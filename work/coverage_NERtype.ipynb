{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: backoff in /opt/conda/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: SPARQLWrapper in /opt/conda/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: rdflib>=6.1.1 in /opt/conda/lib/python3.11/site-packages (from SPARQLWrapper) (7.1.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install backoff\n",
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import requests\n",
    "from aiohttp import ClientResponseError\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:02<00:00, 30.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PERS\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "count = 0\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            count += 1\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SOFT CONSTRAINT\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    ### SOFT FILTERING CONTSTRAINT\n",
    "    #params = {\n",
    "    #    'name': name,\n",
    "    #    'token': 'lamapi_demo_2023',\n",
    "    #    'kg': 'wikidata',\n",
    "    #    'limit': 1000,\n",
    "    #    'query': f'''\n",
    "    #        {{\n",
    "    #            \"query\": {{\n",
    "    #                \"bool\": {{\n",
    "    #                    \"must\": [\n",
    "    #                        {{\n",
    "    #                            \"match\": {{\n",
    "    #                                \"name\": {{\n",
    "    #                                    \"query\": \"{name}\",\n",
    "    #                                    \"boost\": 2.0\n",
    "    #                                }}\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ],\n",
    "    #                    \"should\": [\n",
    "    #                        {{\n",
    "    #                            \"term\": {{\n",
    "    #                                \"NERtype\": \"{value[1]}\"\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ]\n",
    "    #                }}\n",
    "    #            }}\n",
    "    #        }}\n",
    "    #        ''',\n",
    "    #    'sort': [\n",
    "    #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "    #    ]\n",
    "    #}\n",
    "\n",
    "    ### HARD FILTERING CONTSTRAINT\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0,\n",
    "                                        \"fuzziness\": \"AUTO\"\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }},\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        pbar.update(1)  # No need to await here\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        print(f\"{name}: {GT_id_match[0]} NOT FOUND in {value[1]}\")\n",
    "        print(\"___________________________\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id[0])[0]\n",
    "\n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the soft filtering\n",
    "Coverage of R1: 0.9836745270795543\n",
    "\n",
    "Measure Reciprocal Rank of R1: 0.9616820419797453\n",
    "\n",
    "## Coverage with the hard filtering\n",
    "Coverage of R1: 0.8067357512953368\n",
    "\n",
    "Measure Reciprocal Rank of 13: 0.96043575129529763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R3_sorted_mentions[:q1_idx]\n",
    "q2 = R3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R3_sorted_mentions[q3_idx:]\n",
    "\n",
    "\n",
    "sample_size = 1000 \n",
    "R3_sample_keys = []\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q1, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q2, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q3, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R3_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round3_2019/gt/CTA_Round3_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1]            \n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PER\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = category_list\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFT CONSTRAINT\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    ### SOFT FILTERING CONTSTRAINT\n",
    "    #params = {\n",
    "    #    'name': name,\n",
    "    #    'token': 'lamapi_demo_2023',\n",
    "    #    'kg': 'wikidata',\n",
    "    #    'limit': 1000,\n",
    "    #    'query': f'''\n",
    "    #        {{\n",
    "    #            \"query\": {{\n",
    "    #                \"bool\": {{\n",
    "    #                    \"must\": [\n",
    "    #                        {{\n",
    "    #                            \"match\": {{\n",
    "    #                                \"name\": {{\n",
    "    #                                    \"query\": \"{name}\",\n",
    "    #                                    \"boost\": 2.0\n",
    "    #                                }}\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ],\n",
    "    #                    \"should\": [\n",
    "    #                        {{\n",
    "    #                            \"term\": {{\n",
    "    #                                \"NERtype\": \"{value[1]}\"\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ]\n",
    "    #                }}\n",
    "    #            }}\n",
    "    #        }}\n",
    "    #        ''',\n",
    "    #    'sort': [\n",
    "    #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "    #    ]\n",
    "    #}\n",
    "\n",
    "    ### HARD FILTERING CONTSTRAINT\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }},\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id[0])[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R3: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R3: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the soft filtering\n",
    "Coverage of R3: 0.9634817408704353\n",
    "\n",
    "Measure Reciprocal Rank of R3: 0.9472711355677341\n",
    "\n",
    "## Coverage with the hard filtering\n",
    "Coverage of R3: 0.5406758448060075\n",
    "\n",
    "Measure Reciprocal Rank of R3: 0.96075719649556936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100%|██████████| 180/180 [00:11<00:00, 16.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tables_path = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:36<00:00,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "cta_file = './data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "    if value is not None:\n",
    "        # Soft filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}}\n",
    "                    ],\n",
    "                    \"should\": [\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_summer_school_romania_2024'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell_sample):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        \n",
    "        matched_results = []\n",
    "        for q_id in q_ids:            \n",
    "            match = re.search(r'Q(\\d+)$', q_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "\n",
    "            json_data = json.dumps(data)\n",
    "            response = requests.post(url, headers=headers, data=json_data)\n",
    "            if len(response.json()) == 0:\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['should'][0]['term']['NERtype']\n",
    "            queries.append((query, match[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "\n",
    "    if value is not None:\n",
    "        # Hard filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}},\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }  \n",
    "\n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell_sample):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        \n",
    "        matched_results = []\n",
    "        for q_id in q_ids:\n",
    "            match = re.search(r'Q(\\d+)$', q_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "\n",
    "            json_data = json.dumps(data)\n",
    "            response = requests.post(url, headers=headers, data=json_data)\n",
    "            if len(response.json()) == 0:\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "            queries.append((query, match[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3973 [00:00<?, ?it/s]\u001b[A2024-08-05 11:31:19,388 - ERROR - Task exception was never retrieved\n",
      "future: <Task finished name='Task-1' coro=<main() done, defined at /tmp/ipykernel_169/1476425657.py:52> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_169/1476425657.py\", line 97, in <module>\n",
      "    asyncio.run(main(queries, url, pbar))\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 36, in run\n",
      "    loop.run_until_complete(task)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_169/1476425657.py\", line 62, in main\n",
      "    results = await asyncio.gather(*tasks)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n",
      "    future.result()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 31, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 93, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 129, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/nest_asyncio.py\", line 205, in step\n",
      "    step_orig(task, exc)\n",
      "  File \"/opt/conda/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_169/1476425657.py\", line 24, in process_item\n",
      "    data = await fetch(session, url, params, headers, semaphore)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/backoff/_async.py\", line 151, in retry\n",
      "    ret = await target(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_169/1476425657.py\", line 18, in fetch\n",
      "    return await response.json()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/aiohttp/client_reqrep.py\", line 1121, in json\n",
      "    return loads(stripped.decode(encoding))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      " 30%|███       | 129/426 [2:25:32<5:35:06, 67.70s/it]\n",
      "\n",
      "  0%|          | 1/3973 [00:07<8:03:45,  7.31s/it]\u001b[A\n",
      "  0%|          | 3/3973 [00:07<2:08:58,  1.95s/it]\u001b[A\n",
      "  0%|          | 5/3973 [00:07<1:08:13,  1.03s/it]\u001b[A\n",
      "  0%|          | 8/3973 [00:08<36:39,  1.80it/s]  \u001b[A\n",
      "  0%|          | 9/3973 [00:08<33:27,  1.97it/s]\u001b[A\n",
      "  0%|          | 11/3973 [00:08<22:49,  2.89it/s]\u001b[A\n",
      "  0%|          | 12/3973 [00:08<19:47,  3.34it/s]\u001b[A\n",
      "  0%|          | 14/3973 [00:08<14:14,  4.63it/s]\u001b[A\n",
      "  0%|          | 16/3973 [00:08<10:39,  6.19it/s]\u001b[A\n",
      "  1%|          | 20/3973 [00:09<06:38,  9.92it/s]\u001b[A\n",
      "  1%|          | 23/3973 [00:09<05:11, 12.70it/s]\u001b[A\n",
      "  1%|          | 26/3973 [00:09<05:05, 12.93it/s]\u001b[A\n",
      "  1%|          | 28/3973 [00:09<06:05, 10.80it/s]\u001b[A\n",
      "  1%|          | 30/3973 [00:09<06:20, 10.35it/s]\u001b[A\n",
      "  1%|          | 32/3973 [00:10<07:05,  9.27it/s]\u001b[A\n",
      "  1%|          | 34/3973 [00:10<06:02, 10.86it/s]\u001b[A\n",
      "  1%|          | 36/3973 [00:10<08:03,  8.15it/s]\u001b[A\n",
      "  1%|          | 38/3973 [00:11<15:15,  4.30it/s]\u001b[A\n",
      "  1%|          | 41/3973 [00:11<10:22,  6.32it/s]\u001b[A\n",
      "  1%|          | 43/3973 [00:12<09:13,  7.10it/s]\u001b[A\n",
      "  1%|          | 45/3973 [00:12<07:52,  8.31it/s]\u001b[A\n",
      "  1%|          | 47/3973 [00:12<08:39,  7.55it/s]\u001b[A\n",
      "  1%|          | 49/3973 [00:12<07:16,  8.99it/s]\u001b[A\n",
      "  1%|▏         | 52/3973 [00:12<05:52, 11.14it/s]\u001b[A\n",
      "  1%|▏         | 54/3973 [00:13<08:25,  7.75it/s]\u001b[A\n",
      "  1%|▏         | 56/3973 [00:13<08:08,  8.01it/s]\u001b[A\n",
      "  1%|▏         | 59/3973 [00:13<06:33,  9.94it/s]\u001b[A\n",
      "  2%|▏         | 61/3973 [00:14<08:08,  8.01it/s]\u001b[A\n",
      "  2%|▏         | 64/3973 [00:14<06:34,  9.90it/s]\u001b[A\n",
      "  2%|▏         | 66/3973 [00:14<07:59,  8.16it/s]\u001b[A\n",
      "  2%|▏         | 68/3973 [00:14<07:40,  8.47it/s]\u001b[A\n",
      "  2%|▏         | 70/3973 [00:14<06:43,  9.66it/s]\u001b[A\n",
      "  2%|▏         | 72/3973 [00:15<05:54, 10.99it/s]\u001b[A\n",
      "  2%|▏         | 74/3973 [00:15<07:07,  9.12it/s]\u001b[A\n",
      "  2%|▏         | 76/3973 [00:15<06:22, 10.18it/s]\u001b[A\n",
      "  2%|▏         | 78/3973 [00:15<08:47,  7.39it/s]\u001b[A\n",
      "  2%|▏         | 79/3973 [00:16<08:44,  7.42it/s]\u001b[A\n",
      "  2%|▏         | 80/3973 [00:16<09:21,  6.94it/s]\u001b[A\n",
      "  2%|▏         | 81/3973 [00:16<09:04,  7.15it/s]\u001b[A\n",
      "  2%|▏         | 83/3973 [00:16<10:00,  6.47it/s]\u001b[A\n",
      "  2%|▏         | 85/3973 [00:16<08:02,  8.06it/s]\u001b[A\n",
      "  2%|▏         | 87/3973 [00:17<08:56,  7.25it/s]\u001b[A\n",
      "  2%|▏         | 88/3973 [00:17<08:27,  7.65it/s]\u001b[A\n",
      "  2%|▏         | 89/3973 [00:17<08:13,  7.87it/s]\u001b[A\n",
      "  2%|▏         | 91/3973 [00:17<08:20,  7.76it/s]\u001b[A\n",
      "  2%|▏         | 93/3973 [00:17<06:40,  9.69it/s]\u001b[A\n",
      "  2%|▏         | 95/3973 [00:17<05:47, 11.16it/s]\u001b[A\n",
      "  2%|▏         | 98/3973 [00:18<05:55, 10.90it/s]\u001b[A\n",
      "  3%|▎         | 100/3973 [00:18<07:01,  9.19it/s]\u001b[A\n",
      "  3%|▎         | 105/3973 [00:18<05:35, 11.52it/s]\u001b[A\n",
      "  3%|▎         | 107/3973 [00:18<05:09, 12.49it/s]\u001b[A\n",
      "  3%|▎         | 109/3973 [00:19<05:27, 11.81it/s]\u001b[A\n",
      "  3%|▎         | 111/3973 [00:19<05:41, 11.30it/s]\u001b[A\n",
      "  3%|▎         | 113/3973 [00:19<07:19,  8.79it/s]\u001b[A\n",
      "  3%|▎         | 114/3973 [00:20<09:34,  6.71it/s]\u001b[A\n",
      "  3%|▎         | 115/3973 [00:20<09:51,  6.52it/s]\u001b[A\n",
      "  3%|▎         | 118/3973 [00:20<06:29,  9.89it/s]\u001b[A\n",
      "  3%|▎         | 120/3973 [00:21<11:34,  5.55it/s]\u001b[A\n",
      "  3%|▎         | 122/3973 [00:21<09:29,  6.76it/s]\u001b[A\n",
      "  3%|▎         | 124/3973 [00:21<09:41,  6.61it/s]\u001b[A\n",
      "  3%|▎         | 125/3973 [00:21<10:28,  6.13it/s]\u001b[A\n",
      "  3%|▎         | 128/3973 [00:21<07:32,  8.50it/s]\u001b[A\n",
      "  3%|▎         | 130/3973 [00:22<09:38,  6.64it/s]\u001b[A\n",
      "  3%|▎         | 131/3973 [00:22<10:28,  6.11it/s]\u001b[A\n",
      "  3%|▎         | 133/3973 [00:22<08:38,  7.40it/s]\u001b[A\n",
      "  3%|▎         | 134/3973 [00:22<08:20,  7.67it/s]\u001b[A\n",
      "  3%|▎         | 136/3973 [00:23<06:56,  9.21it/s]\u001b[A\n",
      "  3%|▎         | 138/3973 [00:23<05:49, 10.97it/s]\u001b[A\n",
      "  4%|▎         | 140/3973 [00:23<05:39, 11.28it/s]\u001b[A\n",
      "  4%|▎         | 142/3973 [00:23<05:43, 11.14it/s]\u001b[A\n",
      "  4%|▎         | 144/3973 [00:23<07:57,  8.02it/s]\u001b[A\n",
      "  4%|▎         | 146/3973 [00:24<07:04,  9.01it/s]\u001b[A\n",
      "  4%|▎         | 148/3973 [00:24<06:28,  9.85it/s]\u001b[A\n",
      "  4%|▍         | 150/3973 [00:24<05:45, 11.05it/s]\u001b[A\n",
      "  4%|▍         | 152/3973 [00:24<05:33, 11.44it/s]\u001b[A\n",
      "  4%|▍         | 154/3973 [00:24<05:04, 12.53it/s]\u001b[A\n",
      "  4%|▍         | 157/3973 [00:24<05:30, 11.55it/s]\u001b[A\n",
      "  4%|▍         | 160/3973 [00:25<04:24, 14.43it/s]\u001b[A\n",
      "  4%|▍         | 163/3973 [00:25<04:00, 15.82it/s]\u001b[A\n",
      "  4%|▍         | 165/3973 [00:25<06:39,  9.52it/s]\u001b[A\n",
      "  4%|▍         | 167/3973 [00:25<06:32,  9.71it/s]\u001b[A\n",
      "  4%|▍         | 169/3973 [00:26<07:16,  8.71it/s]\u001b[A\n",
      "  4%|▍         | 171/3973 [00:26<08:47,  7.21it/s]\u001b[A\n",
      "  4%|▍         | 173/3973 [00:26<07:13,  8.76it/s]\u001b[A\n",
      "  4%|▍         | 175/3973 [00:27<08:41,  7.29it/s]\u001b[A\n",
      "  4%|▍         | 178/3973 [00:27<07:34,  8.35it/s]\u001b[A\n",
      "  5%|▍         | 180/3973 [00:27<09:32,  6.62it/s]\u001b[A\n",
      "  5%|▍         | 182/3973 [00:28<09:59,  6.32it/s]\u001b[A\n",
      "  5%|▍         | 183/3973 [00:28<10:40,  5.92it/s]\u001b[A\n",
      "  5%|▍         | 186/3973 [00:28<08:31,  7.40it/s]\u001b[A\n",
      "  5%|▍         | 188/3973 [00:28<07:14,  8.71it/s]\u001b[A\n",
      "  5%|▍         | 190/3973 [00:28<06:42,  9.40it/s]\u001b[A\n",
      "  5%|▍         | 192/3973 [00:29<08:15,  7.63it/s]\u001b[A\n",
      "  5%|▍         | 193/3973 [00:29<08:32,  7.37it/s]\u001b[A\n",
      "  5%|▍         | 194/3973 [00:29<08:28,  7.43it/s]\u001b[A\n",
      "  5%|▍         | 195/3973 [00:29<09:10,  6.86it/s]\u001b[A\n",
      "  5%|▍         | 198/3973 [00:30<07:04,  8.88it/s]\u001b[A\n",
      "  5%|▌         | 200/3973 [00:30<06:21,  9.90it/s]\u001b[A\n",
      "  5%|▌         | 202/3973 [00:30<06:02, 10.41it/s]\u001b[A\n",
      "  5%|▌         | 204/3973 [00:30<06:42,  9.37it/s]\u001b[A\n",
      "  5%|▌         | 206/3973 [00:30<05:56, 10.55it/s]\u001b[A\n",
      "  5%|▌         | 208/3973 [00:30<06:09, 10.18it/s]\u001b[A\n",
      "  5%|▌         | 210/3973 [00:31<09:37,  6.52it/s]\u001b[A\n",
      "  5%|▌         | 212/3973 [00:31<08:10,  7.67it/s]\u001b[A\n",
      "  5%|▌         | 216/3973 [00:31<06:08, 10.20it/s]\u001b[A\n",
      "  5%|▌         | 218/3973 [00:32<07:22,  8.49it/s]\u001b[A\n",
      "  6%|▌         | 220/3973 [00:32<06:30,  9.61it/s]\u001b[A\n",
      "  6%|▌         | 222/3973 [00:32<07:18,  8.55it/s]\u001b[A\n",
      "  6%|▌         | 224/3973 [00:33<10:59,  5.68it/s]\u001b[A\n",
      "  6%|▌         | 229/3973 [00:33<06:10, 10.11it/s]\u001b[A\n",
      "  6%|▌         | 231/3973 [00:33<05:50, 10.67it/s]\u001b[A\n",
      "  6%|▌         | 233/3973 [00:33<06:50,  9.12it/s]\u001b[A\n",
      "  6%|▌         | 235/3973 [00:34<07:08,  8.72it/s]\u001b[A\n",
      "  6%|▌         | 237/3973 [00:34<06:11, 10.07it/s]\u001b[A\n",
      "  6%|▌         | 240/3973 [00:34<04:45, 13.08it/s]\u001b[A\n",
      "  6%|▌         | 242/3973 [00:34<04:29, 13.86it/s]\u001b[A\n",
      "  6%|▌         | 244/3973 [00:34<05:12, 11.94it/s]\u001b[A\n",
      "  6%|▌         | 247/3973 [00:34<04:21, 14.24it/s]\u001b[A\n",
      "  6%|▋         | 250/3973 [00:35<03:50, 16.18it/s]\u001b[A\n",
      "  6%|▋         | 252/3973 [00:35<05:16, 11.76it/s]\u001b[A\n",
      "  6%|▋         | 254/3973 [00:35<06:34,  9.44it/s]\u001b[A\n",
      "  6%|▋         | 256/3973 [00:35<05:39, 10.95it/s]\u001b[A\n",
      "  6%|▋         | 258/3973 [00:36<06:16,  9.86it/s]\u001b[A\n",
      "  7%|▋         | 260/3973 [00:36<06:37,  9.34it/s]\u001b[A\n",
      "  7%|▋         | 262/3973 [00:36<05:45, 10.74it/s]\u001b[A\n",
      "  7%|▋         | 264/3973 [00:36<06:45,  9.14it/s]\u001b[A\n",
      "  7%|▋         | 266/3973 [00:36<07:14,  8.54it/s]\u001b[A\n",
      "  7%|▋         | 269/3973 [00:37<05:34, 11.06it/s]\u001b[A\n",
      "  7%|▋         | 271/3973 [00:37<08:00,  7.71it/s]\u001b[A\n",
      "  7%|▋         | 273/3973 [00:37<08:38,  7.13it/s]\u001b[A\n",
      "  7%|▋         | 274/3973 [00:38<08:27,  7.29it/s]\u001b[A\n",
      "  7%|▋         | 276/3973 [00:38<07:14,  8.50it/s]\u001b[A\n",
      "  7%|▋         | 278/3973 [00:38<06:32,  9.42it/s]\u001b[A\n",
      "  7%|▋         | 281/3973 [00:38<05:19, 11.54it/s]\u001b[A\n",
      "  7%|▋         | 283/3973 [00:38<04:52, 12.62it/s]\u001b[A\n",
      "  7%|▋         | 285/3973 [00:38<04:56, 12.43it/s]\u001b[A\n",
      "  7%|▋         | 287/3973 [00:39<07:28,  8.21it/s]\u001b[A\n",
      "  7%|▋         | 289/3973 [00:39<06:16,  9.79it/s]\u001b[A\n",
      "  7%|▋         | 291/3973 [00:39<06:24,  9.58it/s]\u001b[A\n",
      "  7%|▋         | 294/3973 [00:40<09:14,  6.64it/s]\u001b[A\n",
      "  7%|▋         | 296/3973 [00:40<08:11,  7.48it/s]\u001b[A\n",
      "  8%|▊         | 298/3973 [00:40<10:26,  5.86it/s]\u001b[A\n",
      "  8%|▊         | 299/3973 [00:41<09:49,  6.24it/s]\u001b[A\n",
      "  8%|▊         | 300/3973 [00:41<09:06,  6.72it/s]\u001b[A\n",
      "  8%|▊         | 306/3973 [00:41<04:35, 13.29it/s]\u001b[A\n",
      "  8%|▊         | 308/3973 [00:41<05:03, 12.08it/s]\u001b[A\n",
      "  8%|▊         | 311/3973 [00:42<08:19,  7.32it/s]\u001b[A\n",
      "  8%|▊         | 314/3973 [00:42<06:54,  8.83it/s]\u001b[A\n",
      "  8%|▊         | 316/3973 [00:42<07:08,  8.53it/s]\u001b[A\n",
      "  8%|▊         | 320/3973 [00:42<05:19, 11.44it/s]\u001b[A\n",
      "  8%|▊         | 323/3973 [00:43<05:04, 12.00it/s]\u001b[A\n",
      "  8%|▊         | 325/3973 [00:43<05:04, 11.97it/s]\u001b[A\n",
      "  8%|▊         | 327/3973 [00:43<05:06, 11.90it/s]\u001b[A\n",
      "  8%|▊         | 329/3973 [00:43<05:24, 11.22it/s]\u001b[A\n",
      "  8%|▊         | 333/3973 [00:44<05:16, 11.51it/s]\u001b[A\n",
      "  8%|▊         | 335/3973 [00:44<07:13,  8.39it/s]\u001b[A\n",
      "  8%|▊         | 336/3973 [00:44<08:07,  7.46it/s]\u001b[A\n",
      "  9%|▊         | 339/3973 [00:44<05:50, 10.36it/s]\u001b[A\n",
      "  9%|▊         | 341/3973 [00:45<05:39, 10.68it/s]\u001b[A\n",
      "  9%|▊         | 343/3973 [00:45<05:59, 10.10it/s]\u001b[A\n",
      "  9%|▊         | 345/3973 [00:45<06:14,  9.69it/s]\u001b[A\n",
      "  9%|▊         | 347/3973 [00:45<05:41, 10.61it/s]\u001b[A\n",
      "  9%|▉         | 349/3973 [00:45<05:56, 10.18it/s]\u001b[A\n",
      "  9%|▉         | 351/3973 [00:46<08:05,  7.45it/s]\u001b[A\n",
      "  9%|▉         | 352/3973 [00:46<08:25,  7.16it/s]\u001b[A\n",
      "  9%|▉         | 355/3973 [00:46<08:46,  6.88it/s]\u001b[A\n",
      "  9%|▉         | 357/3973 [00:47<07:13,  8.34it/s]\u001b[A\n",
      "  9%|▉         | 359/3973 [00:47<06:00, 10.01it/s]\u001b[A\n",
      "  9%|▉         | 361/3973 [00:47<08:27,  7.12it/s]\u001b[A\n",
      "  9%|▉         | 363/3973 [00:47<07:05,  8.49it/s]\u001b[A\n",
      "  9%|▉         | 365/3973 [00:47<06:18,  9.52it/s]\u001b[A\n",
      "  9%|▉         | 367/3973 [00:48<06:00, 10.01it/s]\u001b[A\n",
      "  9%|▉         | 370/3973 [00:48<04:37, 12.99it/s]\u001b[A\n",
      "  9%|▉         | 373/3973 [00:48<04:05, 14.66it/s]\u001b[A\n",
      "  9%|▉         | 376/3973 [00:48<05:01, 11.93it/s]\u001b[A\n",
      " 10%|▉         | 378/3973 [00:48<05:02, 11.87it/s]\u001b[A\n",
      " 10%|▉         | 380/3973 [00:49<05:35, 10.72it/s]\u001b[A\n",
      " 10%|▉         | 382/3973 [00:49<06:51,  8.74it/s]\u001b[A\n",
      " 10%|▉         | 384/3973 [00:49<06:04,  9.85it/s]\u001b[A\n",
      " 10%|▉         | 386/3973 [00:49<05:51, 10.19it/s]\u001b[A\n",
      " 10%|▉         | 389/3973 [00:49<05:15, 11.37it/s]\u001b[A\n",
      " 10%|▉         | 391/3973 [00:50<05:44, 10.38it/s]\u001b[A\n",
      " 10%|▉         | 393/3973 [00:50<05:03, 11.79it/s]\u001b[A\n",
      " 10%|█         | 399/3973 [00:50<02:56, 20.29it/s]\u001b[A\n",
      " 10%|█         | 402/3973 [00:50<02:49, 21.07it/s]\u001b[A\n",
      " 10%|█         | 405/3973 [00:50<04:17, 13.88it/s]\u001b[A\n",
      " 10%|█         | 407/3973 [00:51<05:32, 10.73it/s]\u001b[A\n",
      " 10%|█         | 409/3973 [00:51<05:12, 11.39it/s]\u001b[A\n",
      " 10%|█         | 411/3973 [00:51<06:00,  9.89it/s]\u001b[A\n",
      " 10%|█         | 413/3973 [00:51<06:08,  9.67it/s]\u001b[A\n",
      " 10%|█         | 415/3973 [00:52<06:59,  8.49it/s]\u001b[A\n",
      " 11%|█         | 418/3973 [00:52<06:50,  8.66it/s]\u001b[A\n",
      " 11%|█         | 420/3973 [00:52<06:55,  8.56it/s]\u001b[A\n",
      " 11%|█         | 421/3973 [00:53<09:55,  5.97it/s]\u001b[A\n",
      " 11%|█         | 422/3973 [00:53<09:35,  6.17it/s]\u001b[A\n",
      " 11%|█         | 424/3973 [00:53<07:31,  7.86it/s]\u001b[A\n",
      " 11%|█         | 427/3973 [00:53<05:31, 10.68it/s]\u001b[A\n",
      " 11%|█         | 429/3973 [00:53<05:32, 10.65it/s]\u001b[A\n",
      " 11%|█         | 431/3973 [00:54<06:06,  9.66it/s]\u001b[A\n",
      " 11%|█         | 433/3973 [00:54<05:53, 10.01it/s]\u001b[A\n",
      " 11%|█         | 435/3973 [00:54<06:28,  9.10it/s]\u001b[A\n",
      " 11%|█         | 437/3973 [00:54<06:12,  9.49it/s]\u001b[A\n",
      " 11%|█         | 439/3973 [00:54<05:42, 10.31it/s]\u001b[A\n",
      " 11%|█         | 441/3973 [00:55<05:30, 10.70it/s]\u001b[A\n",
      " 11%|█         | 443/3973 [00:55<06:01,  9.75it/s]\u001b[A\n",
      " 11%|█         | 445/3973 [00:55<07:30,  7.83it/s]\u001b[A\n",
      " 11%|█▏        | 447/3973 [00:56<08:31,  6.89it/s]\u001b[A\n",
      " 11%|█▏        | 448/3973 [00:56<10:11,  5.76it/s]\u001b[A\n",
      " 11%|█▏        | 450/3973 [00:56<08:04,  7.27it/s]\u001b[A\n",
      " 11%|█▏        | 452/3973 [00:56<06:42,  8.75it/s]\u001b[A\n",
      " 11%|█▏        | 455/3973 [00:56<04:54, 11.96it/s]\u001b[A\n",
      " 12%|█▏        | 458/3973 [00:56<04:42, 12.44it/s]\u001b[A\n",
      " 12%|█▏        | 461/3973 [00:57<03:52, 15.09it/s]\u001b[A\n",
      " 12%|█▏        | 463/3973 [00:57<04:55, 11.89it/s]\u001b[A\n",
      " 12%|█▏        | 465/3973 [00:57<07:07,  8.21it/s]\u001b[A\n",
      " 12%|█▏        | 467/3973 [00:57<06:19,  9.23it/s]\u001b[A\n",
      " 12%|█▏        | 469/3973 [00:58<07:30,  7.77it/s]\u001b[A\n",
      " 12%|█▏        | 471/3973 [00:58<07:39,  7.62it/s]\u001b[A\n",
      " 12%|█▏        | 472/3973 [00:58<07:44,  7.53it/s]\u001b[A\n",
      " 12%|█▏        | 474/3973 [00:58<06:19,  9.21it/s]\u001b[A\n",
      " 12%|█▏        | 476/3973 [00:59<06:45,  8.62it/s]\u001b[A\n",
      " 12%|█▏        | 478/3973 [00:59<06:16,  9.29it/s]\u001b[A\n",
      " 12%|█▏        | 480/3973 [00:59<05:41, 10.23it/s]\u001b[A\n",
      " 12%|█▏        | 482/3973 [00:59<05:57,  9.76it/s]\u001b[A\n",
      " 12%|█▏        | 484/3973 [00:59<06:54,  8.41it/s]\u001b[A\n",
      " 12%|█▏        | 486/3973 [01:00<07:10,  8.11it/s]\u001b[A\n",
      " 12%|█▏        | 487/3973 [01:00<07:36,  7.63it/s]\u001b[A\n",
      " 12%|█▏        | 489/3973 [01:00<06:36,  8.78it/s]\u001b[A\n",
      " 12%|█▏        | 492/3973 [01:00<05:03, 11.47it/s]\u001b[A\n",
      " 12%|█▏        | 494/3973 [01:00<04:53, 11.86it/s]\u001b[A\n",
      " 13%|█▎        | 497/3973 [01:01<04:32, 12.74it/s]\u001b[A\n",
      " 13%|█▎        | 499/3973 [01:01<04:50, 11.95it/s]\u001b[A\n",
      " 13%|█▎        | 503/3973 [01:01<04:46, 12.12it/s]\u001b[A\n",
      " 13%|█▎        | 505/3973 [01:02<06:35,  8.77it/s]\u001b[A\n",
      " 13%|█▎        | 508/3973 [01:02<05:12, 11.08it/s]\u001b[A\n",
      " 13%|█▎        | 512/3973 [01:02<03:54, 14.79it/s]\u001b[A\n",
      " 13%|█▎        | 514/3973 [01:02<04:12, 13.70it/s]\u001b[A\n",
      " 13%|█▎        | 516/3973 [01:02<03:57, 14.56it/s]\u001b[A\n",
      " 13%|█▎        | 519/3973 [01:02<04:51, 11.84it/s]\u001b[A\n",
      " 13%|█▎        | 521/3973 [01:03<06:05,  9.45it/s]\u001b[A\n",
      " 13%|█▎        | 524/3973 [01:03<04:57, 11.59it/s]\u001b[A\n",
      " 13%|█▎        | 528/3973 [01:03<04:08, 13.84it/s]\u001b[A\n",
      " 13%|█▎        | 530/3973 [01:03<04:27, 12.88it/s]\u001b[A\n",
      " 13%|█▎        | 532/3973 [01:03<04:19, 13.27it/s]\u001b[A\n",
      " 13%|█▎        | 534/3973 [01:04<04:55, 11.62it/s]\u001b[A\n",
      " 13%|█▎        | 536/3973 [01:04<06:45,  8.47it/s]\u001b[A\n",
      " 14%|█▎        | 538/3973 [01:05<08:08,  7.03it/s]\u001b[A\n",
      " 14%|█▎        | 540/3973 [01:05<08:17,  6.90it/s]\u001b[A\n",
      " 14%|█▎        | 542/3973 [01:05<06:44,  8.48it/s]\u001b[A\n",
      " 14%|█▎        | 544/3973 [01:05<07:23,  7.73it/s]\u001b[A\n",
      " 14%|█▎        | 545/3973 [01:05<07:31,  7.58it/s]\u001b[A\n",
      " 14%|█▎        | 546/3973 [01:05<07:11,  7.94it/s]\u001b[A\n",
      " 14%|█▍        | 548/3973 [01:06<07:11,  7.94it/s]\u001b[A\n",
      " 14%|█▍        | 550/3973 [01:06<05:47,  9.84it/s]\u001b[A\n",
      " 14%|█▍        | 552/3973 [01:06<07:18,  7.81it/s]\u001b[A\n",
      " 14%|█▍        | 557/3973 [01:07<05:23, 10.54it/s]\u001b[A\n",
      " 14%|█▍        | 559/3973 [01:07<06:24,  8.87it/s]\u001b[A\n",
      " 14%|█▍        | 561/3973 [01:07<05:46,  9.85it/s]\u001b[A\n",
      " 14%|█▍        | 563/3973 [01:07<07:43,  7.36it/s]\u001b[A\n",
      " 14%|█▍        | 565/3973 [01:08<06:25,  8.84it/s]\u001b[A\n",
      " 14%|█▍        | 567/3973 [01:08<06:41,  8.47it/s]\u001b[A\n",
      " 14%|█▍        | 569/3973 [01:08<06:53,  8.23it/s]\u001b[A\n",
      " 14%|█▍        | 570/3973 [01:08<07:17,  7.78it/s]\u001b[A\n",
      " 14%|█▍        | 572/3973 [01:09<08:52,  6.38it/s]\u001b[A\n",
      " 14%|█▍        | 573/3973 [01:09<09:24,  6.02it/s]\u001b[A\n",
      " 14%|█▍        | 575/3973 [01:09<11:53,  4.76it/s]\u001b[A\n",
      " 14%|█▍        | 576/3973 [01:10<10:43,  5.28it/s]\u001b[A\n",
      " 15%|█▍        | 578/3973 [01:10<08:54,  6.35it/s]\u001b[A\n",
      " 15%|█▍        | 580/3973 [01:10<06:57,  8.12it/s]\u001b[A\n",
      " 15%|█▍        | 582/3973 [01:10<05:40,  9.95it/s]\u001b[A\n",
      " 15%|█▍        | 584/3973 [01:10<07:02,  8.03it/s]\u001b[A\n",
      " 15%|█▍        | 586/3973 [01:11<08:28,  6.67it/s]\u001b[A\n",
      " 15%|█▍        | 588/3973 [01:11<10:52,  5.19it/s]\u001b[A\n",
      " 15%|█▍        | 591/3973 [01:12<07:50,  7.19it/s]\u001b[A\n",
      " 15%|█▌        | 600/3973 [01:12<03:18, 16.97it/s]\u001b[A\n",
      " 15%|█▌        | 604/3973 [01:12<03:09, 17.78it/s]\u001b[A\n",
      " 15%|█▌        | 607/3973 [01:12<03:18, 16.92it/s]\u001b[A\n",
      " 15%|█▌        | 610/3973 [01:12<04:19, 12.94it/s]\u001b[A\n",
      " 15%|█▌        | 612/3973 [01:13<04:37, 12.13it/s]\u001b[A\n",
      " 15%|█▌        | 614/3973 [01:13<05:23, 10.38it/s]\u001b[A\n",
      " 16%|█▌        | 616/3973 [01:13<06:09,  9.09it/s]\u001b[A\n",
      " 16%|█▌        | 618/3973 [01:14<07:43,  7.24it/s]\u001b[A\n",
      " 16%|█▌        | 621/3973 [01:14<05:50,  9.56it/s]\u001b[A\n",
      " 16%|█▌        | 623/3973 [01:14<05:27, 10.24it/s]\u001b[A\n",
      " 16%|█▌        | 625/3973 [01:14<06:33,  8.50it/s]\u001b[A\n",
      " 16%|█▌        | 627/3973 [01:14<05:38,  9.87it/s]\u001b[A\n",
      " 16%|█▌        | 629/3973 [01:15<06:51,  8.12it/s]\u001b[A\n",
      " 16%|█▌        | 631/3973 [01:15<06:07,  9.10it/s]\u001b[A\n",
      " 16%|█▌        | 633/3973 [01:15<05:24, 10.29it/s]\u001b[A\n",
      " 16%|█▌        | 635/3973 [01:15<05:34,  9.97it/s]\u001b[A\n",
      " 16%|█▌        | 637/3973 [01:16<06:16,  8.87it/s]\u001b[A\n",
      " 16%|█▌        | 639/3973 [01:16<06:34,  8.44it/s]\u001b[A\n",
      " 16%|█▌        | 640/3973 [01:16<07:59,  6.95it/s]\u001b[A\n",
      " 16%|█▌        | 643/3973 [01:16<05:45,  9.65it/s]\u001b[A\n",
      " 16%|█▌        | 645/3973 [01:16<05:53,  9.42it/s]\u001b[A\n",
      " 16%|█▋        | 647/3973 [01:17<05:03, 10.96it/s]\u001b[A\n",
      " 16%|█▋        | 649/3973 [01:17<04:46, 11.62it/s]\u001b[A\n",
      " 16%|█▋        | 651/3973 [01:17<05:09, 10.73it/s]\u001b[A\n",
      " 16%|█▋        | 653/3973 [01:17<04:53, 11.31it/s]\u001b[A\n",
      " 16%|█▋        | 655/3973 [01:17<04:31, 12.20it/s]\u001b[A\n",
      " 17%|█▋        | 657/3973 [01:18<05:17, 10.44it/s]\u001b[A\n",
      " 17%|█▋        | 660/3973 [01:18<04:30, 12.23it/s]\u001b[A\n",
      " 17%|█▋        | 662/3973 [01:18<04:46, 11.56it/s]\u001b[A\n",
      " 17%|█▋        | 665/3973 [01:18<04:43, 11.66it/s]\u001b[A\n",
      " 17%|█▋        | 667/3973 [01:18<04:59, 11.02it/s]\u001b[A\n",
      " 17%|█▋        | 669/3973 [01:19<07:20,  7.49it/s]\u001b[A\n",
      " 17%|█▋        | 670/3973 [01:19<07:03,  7.81it/s]\u001b[A\n",
      " 17%|█▋        | 671/3973 [01:19<07:24,  7.43it/s]\u001b[A\n",
      " 17%|█▋        | 672/3973 [01:19<07:24,  7.42it/s]\u001b[A\n",
      " 17%|█▋        | 673/3973 [01:20<09:30,  5.78it/s]\u001b[A\n",
      " 17%|█▋        | 675/3973 [01:20<09:28,  5.80it/s]\u001b[A\n",
      " 17%|█▋        | 677/3973 [01:20<07:21,  7.46it/s]\u001b[A\n",
      " 17%|█▋        | 679/3973 [01:20<06:05,  9.01it/s]\u001b[A\n",
      " 17%|█▋        | 681/3973 [01:20<06:13,  8.81it/s]\u001b[A\n",
      " 17%|█▋        | 683/3973 [01:21<05:24, 10.14it/s]\u001b[A\n",
      " 17%|█▋        | 685/3973 [01:21<07:02,  7.79it/s]\u001b[A\n",
      " 17%|█▋        | 688/3973 [01:21<06:00,  9.11it/s]\u001b[A\n",
      " 17%|█▋        | 690/3973 [01:21<05:38,  9.71it/s]\u001b[A\n",
      " 17%|█▋        | 692/3973 [01:22<05:28,  9.99it/s]\u001b[A\n",
      " 17%|█▋        | 694/3973 [01:22<05:22, 10.16it/s]\u001b[A\n",
      " 18%|█▊        | 696/3973 [01:22<05:13, 10.45it/s]\u001b[A\n",
      " 18%|█▊        | 698/3973 [01:22<06:20,  8.60it/s]\u001b[A\n",
      " 18%|█▊        | 699/3973 [01:22<06:54,  7.89it/s]\u001b[A\n",
      " 18%|█▊        | 700/3973 [01:23<06:42,  8.13it/s]\u001b[A\n",
      " 18%|█▊        | 702/3973 [01:23<06:23,  8.52it/s]\u001b[A\n",
      " 18%|█▊        | 703/3973 [01:23<06:27,  8.45it/s]\u001b[A\n",
      " 18%|█▊        | 704/3973 [01:23<06:52,  7.93it/s]\u001b[A\n",
      " 18%|█▊        | 706/3973 [01:23<08:31,  6.39it/s]\u001b[A\n",
      " 18%|█▊        | 708/3973 [01:24<07:19,  7.43it/s]\u001b[A\n",
      " 18%|█▊        | 709/3973 [01:24<07:29,  7.27it/s]\u001b[A\n",
      " 18%|█▊        | 711/3973 [01:24<07:02,  7.72it/s]\u001b[A\n",
      " 18%|█▊        | 713/3973 [01:24<05:39,  9.60it/s]\u001b[A\n",
      " 18%|█▊        | 716/3973 [01:24<04:14, 12.77it/s]\u001b[A\n",
      " 18%|█▊        | 718/3973 [01:25<06:03,  8.95it/s]\u001b[A\n",
      " 18%|█▊        | 720/3973 [01:25<08:25,  6.44it/s]\u001b[A\n",
      " 18%|█▊        | 721/3973 [01:25<08:25,  6.43it/s]\u001b[A\n",
      " 18%|█▊        | 722/3973 [01:26<09:10,  5.90it/s]\u001b[A\n",
      " 18%|█▊        | 724/3973 [01:26<08:18,  6.51it/s]\u001b[A\n",
      " 18%|█▊        | 727/3973 [01:26<05:32,  9.76it/s]\u001b[A\n",
      " 18%|█▊        | 729/3973 [01:26<06:30,  8.31it/s]\u001b[A\n",
      " 18%|█▊        | 731/3973 [01:26<06:35,  8.20it/s]\u001b[A\n",
      " 18%|█▊        | 733/3973 [01:27<08:06,  6.66it/s]\u001b[A\n",
      " 18%|█▊        | 734/3973 [01:27<07:56,  6.80it/s]\u001b[A\n",
      " 19%|█▊        | 737/3973 [01:27<05:25,  9.93it/s]\u001b[A\n",
      " 19%|█▊        | 739/3973 [01:27<05:45,  9.35it/s]\u001b[A\n",
      " 19%|█▊        | 741/3973 [01:28<07:12,  7.46it/s]\u001b[A\n",
      " 19%|█▊        | 743/3973 [01:28<06:22,  8.45it/s]\u001b[A\n",
      " 19%|█▉        | 746/3973 [01:28<05:22, 10.01it/s]\u001b[A\n",
      " 19%|█▉        | 748/3973 [01:28<05:16, 10.20it/s]\u001b[A\n",
      " 19%|█▉        | 751/3973 [01:29<04:43, 11.36it/s]\u001b[A\n",
      " 19%|█▉        | 753/3973 [01:29<04:56, 10.85it/s]\u001b[A\n",
      " 19%|█▉        | 755/3973 [01:29<06:08,  8.74it/s]\u001b[A\n",
      " 19%|█▉        | 758/3973 [01:29<05:42,  9.40it/s]\u001b[A\n",
      " 19%|█▉        | 760/3973 [01:30<05:30,  9.73it/s]\u001b[A\n",
      " 19%|█▉        | 763/3973 [01:30<04:43, 11.34it/s]\u001b[A\n",
      " 19%|█▉        | 765/3973 [01:30<05:16, 10.15it/s]\u001b[A\n",
      " 19%|█▉        | 767/3973 [01:30<06:09,  8.67it/s]\u001b[A\n",
      " 19%|█▉        | 768/3973 [01:31<07:29,  7.13it/s]\u001b[A\n",
      " 19%|█▉        | 771/3973 [01:31<06:19,  8.43it/s]\u001b[A\n",
      " 19%|█▉        | 773/3973 [01:31<05:21,  9.95it/s]\u001b[A\n",
      " 20%|█▉        | 775/3973 [01:31<05:27,  9.75it/s]\u001b[A\n",
      " 20%|█▉        | 777/3973 [01:31<05:26,  9.78it/s]\u001b[A\n",
      " 20%|█▉        | 779/3973 [01:32<06:21,  8.37it/s]\u001b[A\n",
      " 20%|█▉        | 780/3973 [01:32<06:28,  8.22it/s]\u001b[A\n",
      " 20%|█▉        | 782/3973 [01:32<05:28,  9.72it/s]\u001b[A\n",
      " 20%|█▉        | 784/3973 [01:32<07:42,  6.90it/s]\u001b[A\n",
      " 20%|█▉        | 786/3973 [01:33<08:00,  6.63it/s]\u001b[A\n",
      " 20%|█▉        | 787/3973 [01:33<10:18,  5.15it/s]\u001b[A\n",
      " 20%|█▉        | 789/3973 [01:33<08:30,  6.23it/s]\u001b[A\n",
      " 20%|█▉        | 791/3973 [01:34<07:39,  6.93it/s]\u001b[A\n",
      " 20%|█▉        | 793/3973 [01:34<06:04,  8.72it/s]\u001b[A\n",
      " 20%|██        | 796/3973 [01:34<05:37,  9.42it/s]\u001b[A\n",
      " 20%|██        | 799/3973 [01:34<06:05,  8.68it/s]\u001b[A\n",
      " 20%|██        | 801/3973 [01:35<07:44,  6.83it/s]\u001b[A\n",
      " 20%|██        | 802/3973 [01:35<07:52,  6.71it/s]\u001b[A\n",
      " 20%|██        | 804/3973 [01:35<07:07,  7.41it/s]\u001b[A\n",
      " 20%|██        | 806/3973 [01:36<08:10,  6.45it/s]\u001b[A\n",
      " 20%|██        | 808/3973 [01:36<06:43,  7.85it/s]\u001b[A\n",
      " 20%|██        | 810/3973 [01:36<06:34,  8.02it/s]\u001b[A\n",
      " 20%|██        | 812/3973 [01:36<06:09,  8.55it/s]\u001b[A\n",
      " 20%|██        | 813/3973 [01:36<06:38,  7.92it/s]\u001b[A\n",
      " 20%|██        | 814/3973 [01:36<06:37,  7.95it/s]\u001b[A\n",
      " 21%|██        | 815/3973 [01:37<06:26,  8.18it/s]\u001b[A\n",
      " 21%|██        | 817/3973 [01:37<05:53,  8.92it/s]\u001b[A\n",
      " 21%|██        | 819/3973 [01:37<05:21,  9.82it/s]\u001b[A\n",
      " 21%|██        | 821/3973 [01:37<04:51, 10.82it/s]\u001b[A\n",
      " 21%|██        | 825/3973 [01:37<03:11, 16.40it/s]\u001b[A\n",
      " 21%|██        | 827/3973 [01:37<03:37, 14.49it/s]\u001b[A\n",
      " 21%|██        | 829/3973 [01:38<04:23, 11.92it/s]\u001b[A\n",
      " 21%|██        | 831/3973 [01:38<04:14, 12.36it/s]\u001b[A\n",
      " 21%|██        | 833/3973 [01:38<05:41,  9.20it/s]\u001b[A\n",
      " 21%|██        | 835/3973 [01:38<04:56, 10.57it/s]\u001b[A\n",
      " 21%|██        | 837/3973 [01:38<04:25, 11.81it/s]\u001b[A\n",
      " 21%|██        | 839/3973 [01:39<04:39, 11.23it/s]\u001b[A\n",
      " 21%|██        | 842/3973 [01:39<05:30,  9.47it/s]\u001b[A\n",
      " 21%|██        | 844/3973 [01:40<08:11,  6.37it/s]\u001b[A\n",
      " 21%|██▏       | 845/3973 [01:40<09:00,  5.79it/s]\u001b[A\n",
      " 21%|██▏       | 846/3973 [01:40<08:48,  5.92it/s]\u001b[A\n",
      " 21%|██▏       | 847/3973 [01:40<09:14,  5.63it/s]\u001b[A\n",
      " 21%|██▏       | 849/3973 [01:40<06:45,  7.71it/s]\u001b[A\n",
      " 21%|██▏       | 851/3973 [01:40<05:44,  9.06it/s]\u001b[A\n",
      " 21%|██▏       | 853/3973 [01:41<04:47, 10.85it/s]\u001b[A\n",
      " 22%|██▏       | 855/3973 [01:41<06:29,  8.01it/s]\u001b[A\n",
      " 22%|██▏       | 857/3973 [01:41<05:26,  9.56it/s]\u001b[A\n",
      " 22%|██▏       | 859/3973 [01:41<04:36, 11.27it/s]\u001b[A\n",
      " 22%|██▏       | 861/3973 [01:42<07:38,  6.78it/s]\u001b[A\n",
      " 22%|██▏       | 863/3973 [01:42<07:13,  7.18it/s]\u001b[A\n",
      " 22%|██▏       | 865/3973 [01:42<06:45,  7.66it/s]\u001b[A\n",
      " 22%|██▏       | 866/3973 [01:42<07:07,  7.27it/s]\u001b[A\n",
      " 22%|██▏       | 868/3973 [01:42<06:00,  8.61it/s]\u001b[A\n",
      " 22%|██▏       | 870/3973 [01:43<05:21,  9.67it/s]\u001b[A\n",
      " 22%|██▏       | 872/3973 [01:43<08:53,  5.82it/s]\u001b[A\n",
      " 22%|██▏       | 873/3973 [01:43<08:21,  6.18it/s]\u001b[A\n",
      " 22%|██▏       | 874/3973 [01:44<10:44,  4.81it/s]\u001b[A\n",
      " 22%|██▏       | 875/3973 [01:44<10:04,  5.13it/s]\u001b[A\n",
      " 22%|██▏       | 880/3973 [01:44<05:00, 10.30it/s]\u001b[A\n",
      " 22%|██▏       | 882/3973 [01:44<04:41, 10.97it/s]\u001b[A\n",
      " 22%|██▏       | 884/3973 [01:44<04:26, 11.59it/s]\u001b[A\n",
      " 22%|██▏       | 886/3973 [01:45<05:00, 10.27it/s]\u001b[A\n",
      " 22%|██▏       | 890/3973 [01:45<03:51, 13.35it/s]\u001b[A\n",
      " 22%|██▏       | 892/3973 [01:45<04:31, 11.33it/s]\u001b[A\n",
      " 23%|██▎       | 894/3973 [01:46<06:08,  8.35it/s]\u001b[A\n",
      " 23%|██▎       | 896/3973 [01:46<07:16,  7.05it/s]\u001b[A\n",
      " 23%|██▎       | 897/3973 [01:46<07:31,  6.82it/s]\u001b[A\n",
      " 23%|██▎       | 898/3973 [01:46<07:35,  6.75it/s]\u001b[A\n",
      " 23%|██▎       | 900/3973 [01:46<06:31,  7.86it/s]\u001b[A\n",
      " 23%|██▎       | 902/3973 [01:47<07:00,  7.31it/s]\u001b[A\n",
      " 23%|██▎       | 903/3973 [01:47<07:51,  6.51it/s]\u001b[A\n",
      " 23%|██▎       | 904/3973 [01:47<07:18,  7.00it/s]\u001b[A\n",
      " 23%|██▎       | 906/3973 [01:47<05:54,  8.65it/s]\u001b[A\n",
      " 23%|██▎       | 907/3973 [01:47<06:29,  7.87it/s]\u001b[A\n",
      " 23%|██▎       | 910/3973 [01:48<04:24, 11.59it/s]\u001b[A\n",
      " 23%|██▎       | 912/3973 [01:48<06:14,  8.17it/s]\u001b[A\n",
      " 23%|██▎       | 916/3973 [01:48<04:08, 12.32it/s]\u001b[A\n",
      " 23%|██▎       | 918/3973 [01:48<04:58, 10.22it/s]\u001b[A\n",
      " 23%|██▎       | 920/3973 [01:49<05:01, 10.13it/s]\u001b[A\n",
      " 23%|██▎       | 922/3973 [01:49<06:12,  8.19it/s]\u001b[A\n",
      " 23%|██▎       | 924/3973 [01:49<05:19,  9.53it/s]\u001b[A\n",
      " 23%|██▎       | 926/3973 [01:49<05:03, 10.05it/s]\u001b[A\n",
      " 23%|██▎       | 928/3973 [01:49<04:54, 10.34it/s]\u001b[A\n",
      " 23%|██▎       | 930/3973 [01:50<04:43, 10.73it/s]\u001b[A\n",
      " 23%|██▎       | 933/3973 [01:50<03:56, 12.86it/s]\u001b[A\n",
      " 24%|██▎       | 935/3973 [01:50<04:54, 10.31it/s]\u001b[A\n",
      " 24%|██▎       | 937/3973 [01:50<04:37, 10.93it/s]\u001b[A\n",
      " 24%|██▎       | 939/3973 [01:50<04:09, 12.17it/s]\u001b[A\n",
      " 24%|██▎       | 941/3973 [01:51<05:05,  9.92it/s]\u001b[A\n",
      " 24%|██▎       | 943/3973 [01:51<04:41, 10.77it/s]\u001b[A\n",
      " 24%|██▍       | 946/3973 [01:51<04:37, 10.92it/s]\u001b[A\n",
      " 24%|██▍       | 948/3973 [01:51<04:54, 10.26it/s]\u001b[A\n",
      " 24%|██▍       | 950/3973 [01:52<05:52,  8.58it/s]\u001b[A\n",
      " 24%|██▍       | 951/3973 [01:52<05:48,  8.67it/s]\u001b[A\n",
      " 24%|██▍       | 952/3973 [01:52<05:48,  8.68it/s]\u001b[A\n",
      " 24%|██▍       | 953/3973 [01:52<07:07,  7.07it/s]\u001b[A\n",
      " 24%|██▍       | 955/3973 [01:52<06:03,  8.31it/s]\u001b[A\n",
      " 24%|██▍       | 957/3973 [01:52<06:03,  8.30it/s]\u001b[A\n",
      " 24%|██▍       | 959/3973 [01:53<05:16,  9.53it/s]\u001b[A\n",
      " 24%|██▍       | 961/3973 [01:53<05:48,  8.65it/s]\u001b[A\n",
      " 24%|██▍       | 962/3973 [01:53<06:04,  8.27it/s]\u001b[A\n",
      " 24%|██▍       | 963/3973 [01:53<06:36,  7.59it/s]\u001b[A\n",
      " 24%|██▍       | 965/3973 [01:53<05:17,  9.46it/s]\u001b[A\n",
      " 24%|██▍       | 970/3973 [01:54<04:40, 10.69it/s]\u001b[A\n",
      " 24%|██▍       | 972/3973 [01:54<04:28, 11.16it/s]\u001b[A\n",
      " 25%|██▍       | 974/3973 [01:54<05:01,  9.96it/s]\u001b[A\n",
      " 25%|██▍       | 976/3973 [01:54<04:29, 11.13it/s]\u001b[A\n",
      " 25%|██▍       | 979/3973 [01:54<03:30, 14.22it/s]\u001b[A\n",
      " 25%|██▍       | 981/3973 [01:55<04:04, 12.24it/s]\u001b[A\n",
      " 25%|██▍       | 983/3973 [01:55<04:17, 11.62it/s]\u001b[A\n",
      " 25%|██▍       | 985/3973 [01:55<04:30, 11.04it/s]\u001b[A\n",
      " 25%|██▍       | 988/3973 [01:55<03:52, 12.83it/s]\u001b[A\n",
      " 25%|██▍       | 990/3973 [01:55<04:55, 10.08it/s]\u001b[A\n",
      " 25%|██▍       | 992/3973 [01:56<05:43,  8.68it/s]\u001b[A\n",
      " 25%|██▍       | 993/3973 [01:56<06:20,  7.84it/s]\u001b[A\n",
      " 25%|██▌       | 994/3973 [01:56<06:45,  7.36it/s]\u001b[A\n",
      " 25%|██▌       | 996/3973 [01:56<05:29,  9.04it/s]\u001b[A\n",
      " 25%|██▌       | 998/3973 [01:56<04:54, 10.10it/s]\u001b[A\n",
      " 25%|██▌       | 1000/3973 [01:57<07:03,  7.02it/s]\u001b[A\n",
      " 25%|██▌       | 1001/3973 [01:57<06:41,  7.40it/s]\u001b[A\n",
      " 25%|██▌       | 1002/3973 [01:57<06:55,  7.15it/s]\u001b[A\n",
      " 25%|██▌       | 1003/3973 [01:57<08:53,  5.56it/s]\u001b[A\n",
      " 25%|██▌       | 1004/3973 [01:58<08:46,  5.64it/s]\u001b[A\n",
      " 25%|██▌       | 1006/3973 [01:58<06:11,  7.99it/s]\u001b[A\n",
      " 25%|██▌       | 1008/3973 [01:58<04:52, 10.15it/s]\u001b[A\n",
      " 25%|██▌       | 1010/3973 [01:58<05:18,  9.30it/s]\u001b[A\n",
      " 25%|██▌       | 1012/3973 [01:58<04:48, 10.25it/s]\u001b[A\n",
      " 26%|██▌       | 1014/3973 [01:58<05:04,  9.72it/s]\u001b[A\n",
      " 26%|██▌       | 1016/3973 [01:59<05:58,  8.26it/s]\u001b[A\n",
      " 26%|██▌       | 1017/3973 [01:59<06:16,  7.85it/s]\u001b[A\n",
      " 26%|██▌       | 1022/3973 [01:59<04:14, 11.60it/s]\u001b[A\n",
      " 26%|██▌       | 1024/3973 [01:59<04:14, 11.58it/s]\u001b[A\n",
      " 26%|██▌       | 1027/3973 [02:00<03:53, 12.60it/s]\u001b[A\n",
      " 26%|██▌       | 1029/3973 [02:00<03:46, 13.02it/s]\u001b[A\n",
      " 26%|██▌       | 1031/3973 [02:00<04:02, 12.12it/s]\u001b[A\n",
      " 26%|██▌       | 1036/3973 [02:00<03:48, 12.85it/s]\u001b[A\n",
      " 26%|██▌       | 1038/3973 [02:00<03:55, 12.48it/s]\u001b[A\n",
      " 26%|██▌       | 1040/3973 [02:01<03:41, 13.25it/s]\u001b[A\n",
      " 26%|██▌       | 1042/3973 [02:01<04:19, 11.29it/s]\u001b[A\n",
      " 26%|██▋       | 1044/3973 [02:01<04:44, 10.30it/s]\u001b[A\n",
      " 26%|██▋       | 1046/3973 [02:01<05:26,  8.96it/s]\u001b[A\n",
      " 26%|██▋       | 1049/3973 [02:02<04:28, 10.89it/s]\u001b[A\n",
      " 26%|██▋       | 1051/3973 [02:02<04:19, 11.24it/s]\u001b[A\n",
      " 27%|██▋       | 1053/3973 [02:02<03:54, 12.47it/s]\u001b[A\n",
      " 27%|██▋       | 1055/3973 [02:02<03:40, 13.26it/s]\u001b[A\n",
      " 27%|██▋       | 1057/3973 [02:02<05:40,  8.57it/s]\u001b[A\n",
      " 27%|██▋       | 1060/3973 [02:03<04:08, 11.72it/s]\u001b[A\n",
      " 27%|██▋       | 1062/3973 [02:03<04:12, 11.55it/s]\u001b[A\n",
      " 27%|██▋       | 1067/3973 [02:03<02:37, 18.39it/s]\u001b[A\n",
      " 27%|██▋       | 1070/3973 [02:03<02:44, 17.61it/s]\u001b[A\n",
      " 27%|██▋       | 1073/3973 [02:03<03:52, 12.49it/s]\u001b[A\n",
      " 27%|██▋       | 1075/3973 [02:04<03:47, 12.76it/s]\u001b[A\n",
      " 27%|██▋       | 1077/3973 [02:04<03:41, 13.08it/s]\u001b[A\n",
      " 27%|██▋       | 1079/3973 [02:04<03:58, 12.13it/s]\u001b[A\n",
      " 27%|██▋       | 1081/3973 [02:04<04:21, 11.06it/s]\u001b[A\n",
      " 27%|██▋       | 1083/3973 [02:04<04:26, 10.83it/s]\u001b[A\n",
      " 27%|██▋       | 1085/3973 [02:05<06:46,  7.10it/s]\u001b[A\n",
      " 27%|██▋       | 1088/3973 [02:05<05:12,  9.24it/s]\u001b[A\n",
      " 27%|██▋       | 1090/3973 [02:05<04:48,  9.99it/s]\u001b[A\n",
      " 27%|██▋       | 1092/3973 [02:05<05:08,  9.33it/s]\u001b[A\n",
      " 28%|██▊       | 1094/3973 [02:06<04:46, 10.05it/s]\u001b[A\n",
      " 28%|██▊       | 1096/3973 [02:06<06:11,  7.75it/s]\u001b[A\n",
      " 28%|██▊       | 1097/3973 [02:06<07:28,  6.42it/s]\u001b[A\n",
      " 28%|██▊       | 1100/3973 [02:06<05:03,  9.47it/s]\u001b[A\n",
      " 28%|██▊       | 1102/3973 [02:07<05:36,  8.52it/s]\u001b[A\n",
      " 28%|██▊       | 1104/3973 [02:07<05:47,  8.26it/s]\u001b[A\n",
      " 28%|██▊       | 1107/3973 [02:07<04:35, 10.39it/s]\u001b[A\n",
      " 28%|██▊       | 1110/3973 [02:07<04:11, 11.39it/s]\u001b[A\n",
      " 28%|██▊       | 1112/3973 [02:08<04:28, 10.65it/s]\u001b[A\n",
      " 28%|██▊       | 1114/3973 [02:08<05:02,  9.44it/s]\u001b[A\n",
      " 28%|██▊       | 1117/3973 [02:08<03:55, 12.13it/s]\u001b[A\n",
      " 28%|██▊       | 1119/3973 [02:08<04:45, 10.00it/s]\u001b[A\n",
      " 28%|██▊       | 1122/3973 [02:08<04:06, 11.58it/s]\u001b[A\n",
      " 28%|██▊       | 1124/3973 [02:09<04:54,  9.68it/s]\u001b[A\n",
      " 28%|██▊       | 1126/3973 [02:09<04:17, 11.05it/s]\u001b[A\n",
      " 28%|██▊       | 1128/3973 [02:09<05:09,  9.19it/s]\u001b[A\n",
      " 28%|██▊       | 1131/3973 [02:09<05:07,  9.23it/s]\u001b[A\n",
      " 29%|██▊       | 1133/3973 [02:10<05:24,  8.76it/s]\u001b[A\n",
      " 29%|██▊       | 1135/3973 [02:10<04:59,  9.46it/s]\u001b[A\n",
      " 29%|██▊       | 1137/3973 [02:10<04:33, 10.37it/s]\u001b[A\n",
      " 29%|██▊       | 1139/3973 [02:10<06:01,  7.84it/s]\u001b[A\n",
      " 29%|██▊       | 1142/3973 [02:11<04:47,  9.83it/s]\u001b[A\n",
      " 29%|██▉       | 1144/3973 [02:11<04:46,  9.87it/s]\u001b[A\n",
      " 29%|██▉       | 1146/3973 [02:11<05:02,  9.35it/s]\u001b[A\n",
      " 29%|██▉       | 1148/3973 [02:11<04:41, 10.03it/s]\u001b[A\n",
      " 29%|██▉       | 1150/3973 [02:12<06:25,  7.32it/s]\u001b[A\n",
      " 29%|██▉       | 1153/3973 [02:12<05:57,  7.88it/s]\u001b[A\n",
      " 29%|██▉       | 1155/3973 [02:12<06:14,  7.52it/s]\u001b[A\n",
      " 29%|██▉       | 1158/3973 [02:12<04:46,  9.82it/s]\u001b[A\n",
      " 29%|██▉       | 1160/3973 [02:13<04:33, 10.29it/s]\u001b[A\n",
      " 29%|██▉       | 1164/3973 [02:13<03:52, 12.09it/s]\u001b[A\n",
      " 29%|██▉       | 1166/3973 [02:13<05:56,  7.87it/s]\u001b[A\n",
      " 29%|██▉       | 1168/3973 [02:14<06:30,  7.19it/s]\u001b[A\n",
      " 29%|██▉       | 1172/3973 [02:14<04:25, 10.53it/s]\u001b[A\n",
      " 30%|██▉       | 1174/3973 [02:14<04:24, 10.58it/s]\u001b[A\n",
      " 30%|██▉       | 1178/3973 [02:14<03:29, 13.32it/s]\u001b[A\n",
      " 30%|██▉       | 1180/3973 [02:15<03:51, 12.06it/s]\u001b[A\n",
      " 30%|██▉       | 1182/3973 [02:15<03:38, 12.77it/s]\u001b[A\n",
      " 30%|██▉       | 1184/3973 [02:15<04:13, 10.99it/s]\u001b[A\n",
      " 30%|██▉       | 1186/3973 [02:15<04:46,  9.73it/s]\u001b[A\n",
      " 30%|██▉       | 1188/3973 [02:15<04:27, 10.40it/s]\u001b[A\n",
      " 30%|██▉       | 1190/3973 [02:16<04:57,  9.37it/s]\u001b[A\n",
      " 30%|███       | 1192/3973 [02:16<04:11, 11.04it/s]\u001b[A\n",
      " 30%|███       | 1195/3973 [02:16<03:31, 13.15it/s]\u001b[A\n",
      " 30%|███       | 1197/3973 [02:16<03:28, 13.34it/s]\u001b[A\n",
      " 30%|███       | 1199/3973 [02:16<03:55, 11.79it/s]\u001b[A\n",
      " 30%|███       | 1201/3973 [02:17<06:48,  6.79it/s]\u001b[A\n",
      " 30%|███       | 1203/3973 [02:17<06:01,  7.67it/s]\u001b[A\n",
      " 30%|███       | 1205/3973 [02:17<04:58,  9.29it/s]\u001b[A\n",
      " 30%|███       | 1207/3973 [02:17<05:39,  8.14it/s]\u001b[A\n",
      " 30%|███       | 1209/3973 [02:18<08:16,  5.57it/s]\u001b[A\n",
      " 30%|███       | 1211/3973 [02:18<06:32,  7.04it/s]\u001b[A\n",
      " 31%|███       | 1213/3973 [02:19<07:25,  6.20it/s]\u001b[A\n",
      " 31%|███       | 1215/3973 [02:19<06:49,  6.73it/s]\u001b[A\n",
      " 31%|███       | 1217/3973 [02:19<05:29,  8.35it/s]\u001b[A\n",
      " 31%|███       | 1219/3973 [02:19<05:29,  8.36it/s]\u001b[A\n",
      " 31%|███       | 1222/3973 [02:19<03:58, 11.52it/s]\u001b[A\n",
      " 31%|███       | 1224/3973 [02:20<05:54,  7.75it/s]\u001b[A\n",
      " 31%|███       | 1227/3973 [02:20<05:45,  7.95it/s]\u001b[A\n",
      " 31%|███       | 1230/3973 [02:20<04:34,  9.99it/s]\u001b[A\n",
      " 31%|███       | 1233/3973 [02:20<04:01, 11.35it/s]\u001b[A\n",
      " 31%|███       | 1235/3973 [02:21<04:42,  9.70it/s]\u001b[A\n",
      " 31%|███       | 1237/3973 [02:21<04:36,  9.90it/s]\u001b[A\n",
      " 31%|███       | 1239/3973 [02:21<04:53,  9.32it/s]\u001b[A\n",
      " 31%|███       | 1241/3973 [02:21<04:59,  9.13it/s]\u001b[A\n",
      " 31%|███▏      | 1244/3973 [02:22<06:10,  7.36it/s]\u001b[A\n",
      " 31%|███▏      | 1246/3973 [02:22<05:10,  8.78it/s]\u001b[A\n",
      " 31%|███▏      | 1248/3973 [02:22<05:29,  8.28it/s]\u001b[A\n",
      " 31%|███▏      | 1250/3973 [02:22<04:41,  9.67it/s]\u001b[A\n",
      " 32%|███▏      | 1252/3973 [02:23<05:45,  7.88it/s]\u001b[A\n",
      " 32%|███▏      | 1254/3973 [02:23<08:14,  5.50it/s]\u001b[A\n",
      " 32%|███▏      | 1258/3973 [02:24<05:13,  8.67it/s]\u001b[A\n",
      " 32%|███▏      | 1260/3973 [02:24<05:31,  8.18it/s]\u001b[A\n",
      " 32%|███▏      | 1262/3973 [02:24<07:01,  6.43it/s]\u001b[A\n",
      " 32%|███▏      | 1264/3973 [02:25<06:09,  7.34it/s]\u001b[A\n",
      " 32%|███▏      | 1268/3973 [02:25<04:33,  9.90it/s]\u001b[A\n",
      " 32%|███▏      | 1270/3973 [02:25<04:18, 10.44it/s]\u001b[A\n",
      " 32%|███▏      | 1272/3973 [02:25<04:09, 10.82it/s]\u001b[A\n",
      " 32%|███▏      | 1274/3973 [02:25<04:11, 10.73it/s]\u001b[A\n",
      " 32%|███▏      | 1276/3973 [02:26<05:56,  7.56it/s]\u001b[A\n",
      " 32%|███▏      | 1280/3973 [02:26<03:56, 11.40it/s]\u001b[A\n",
      " 32%|███▏      | 1282/3973 [02:26<04:58,  9.00it/s]\u001b[A\n",
      " 32%|███▏      | 1286/3973 [02:26<03:44, 11.98it/s]\u001b[A\n",
      " 32%|███▏      | 1288/3973 [02:27<03:42, 12.07it/s]\u001b[A\n",
      " 32%|███▏      | 1290/3973 [02:27<03:56, 11.36it/s]\u001b[A\n",
      " 33%|███▎      | 1292/3973 [02:27<05:21,  8.34it/s]\u001b[A\n",
      " 33%|███▎      | 1294/3973 [02:28<05:52,  7.60it/s]\u001b[A\n",
      " 33%|███▎      | 1299/3973 [02:28<03:42, 12.04it/s]\u001b[A\n",
      " 33%|███▎      | 1301/3973 [02:28<03:54, 11.41it/s]\u001b[A\n",
      " 33%|███▎      | 1303/3973 [02:28<03:56, 11.27it/s]\u001b[A\n",
      " 33%|███▎      | 1305/3973 [02:28<04:03, 10.94it/s]\u001b[A\n",
      " 33%|███▎      | 1307/3973 [02:29<05:31,  8.04it/s]\u001b[A\n",
      " 33%|███▎      | 1310/3973 [02:29<04:33,  9.74it/s]\u001b[A\n",
      " 33%|███▎      | 1312/3973 [02:30<06:40,  6.64it/s]\u001b[A\n",
      " 33%|███▎      | 1315/3973 [02:30<04:58,  8.91it/s]\u001b[A\n",
      " 33%|███▎      | 1318/3973 [02:30<03:56, 11.23it/s]\u001b[A\n",
      " 33%|███▎      | 1322/3973 [02:30<02:51, 15.49it/s]\u001b[A\n",
      " 33%|███▎      | 1325/3973 [02:30<02:35, 16.99it/s]\u001b[A\n",
      " 33%|███▎      | 1328/3973 [02:30<03:03, 14.42it/s]\u001b[A\n",
      " 33%|███▎      | 1330/3973 [02:30<03:14, 13.56it/s]\u001b[A\n",
      " 34%|███▎      | 1332/3973 [02:31<03:44, 11.78it/s]\u001b[A\n",
      " 34%|███▎      | 1334/3973 [02:31<04:03, 10.84it/s]\u001b[A\n",
      " 34%|███▎      | 1336/3973 [02:31<04:11, 10.47it/s]\u001b[A\n",
      " 34%|███▎      | 1338/3973 [02:31<04:35,  9.57it/s]\u001b[A\n",
      " 34%|███▍      | 1341/3973 [02:32<04:45,  9.22it/s]\u001b[A\n",
      " 34%|███▍      | 1344/3973 [02:32<04:48,  9.11it/s]\u001b[A\n",
      " 34%|███▍      | 1345/3973 [02:32<04:56,  8.87it/s]\u001b[A\n",
      " 34%|███▍      | 1346/3973 [02:33<06:11,  7.06it/s]\u001b[A\n",
      " 34%|███▍      | 1347/3973 [02:33<06:00,  7.28it/s]\u001b[A\n",
      " 34%|███▍      | 1348/3973 [02:33<07:58,  5.49it/s]\u001b[A\n",
      " 34%|███▍      | 1349/3973 [02:33<07:18,  5.98it/s]\u001b[A\n",
      " 34%|███▍      | 1351/3973 [02:33<06:36,  6.62it/s]\u001b[A\n",
      " 34%|███▍      | 1352/3973 [02:34<07:23,  5.90it/s]\u001b[A\n",
      " 34%|███▍      | 1354/3973 [02:34<05:24,  8.08it/s]\u001b[A\n",
      " 34%|███▍      | 1356/3973 [02:34<06:49,  6.40it/s]\u001b[A\n",
      " 34%|███▍      | 1358/3973 [02:34<07:07,  6.12it/s]\u001b[A\n",
      " 34%|███▍      | 1360/3973 [02:35<06:50,  6.36it/s]\u001b[A\n",
      " 34%|███▍      | 1361/3973 [02:35<06:31,  6.68it/s]\u001b[A\n",
      " 34%|███▍      | 1363/3973 [02:35<06:26,  6.75it/s]\u001b[A\n",
      " 34%|███▍      | 1364/3973 [02:35<06:28,  6.72it/s]\u001b[A\n",
      " 34%|███▍      | 1365/3973 [02:36<07:23,  5.89it/s]\u001b[A\n",
      " 34%|███▍      | 1366/3973 [02:36<08:12,  5.30it/s]\u001b[A\n",
      " 34%|███▍      | 1367/3973 [02:36<07:14,  6.00it/s]\u001b[A\n",
      " 34%|███▍      | 1368/3973 [02:36<07:04,  6.13it/s]\u001b[A\n",
      " 35%|███▍      | 1372/3973 [02:36<04:08, 10.45it/s]\u001b[A\n",
      " 35%|███▍      | 1375/3973 [02:36<03:09, 13.72it/s]\u001b[A\n",
      " 35%|███▍      | 1377/3973 [02:37<05:44,  7.54it/s]\u001b[A\n",
      " 35%|███▍      | 1380/3973 [02:37<04:09, 10.39it/s]\u001b[A\n",
      " 35%|███▍      | 1382/3973 [02:37<05:05,  8.47it/s]\u001b[A\n",
      " 35%|███▍      | 1385/3973 [02:38<04:12, 10.26it/s]\u001b[A\n",
      " 35%|███▍      | 1387/3973 [02:38<06:34,  6.55it/s]\u001b[A\n",
      " 35%|███▍      | 1389/3973 [02:38<05:43,  7.52it/s]\u001b[A\n",
      " 35%|███▌      | 1391/3973 [02:39<04:58,  8.66it/s]\u001b[A\n",
      " 35%|███▌      | 1394/3973 [02:39<03:56, 10.91it/s]\u001b[A\n",
      " 35%|███▌      | 1396/3973 [02:39<07:03,  6.08it/s]\u001b[A\n",
      " 35%|███▌      | 1399/3973 [02:40<05:29,  7.80it/s]\u001b[A\n",
      " 35%|███▌      | 1401/3973 [02:40<05:09,  8.32it/s]\u001b[A\n",
      " 35%|███▌      | 1404/3973 [02:40<04:51,  8.82it/s]\u001b[A\n",
      " 35%|███▌      | 1406/3973 [02:40<04:20,  9.85it/s]\u001b[A\n",
      " 35%|███▌      | 1408/3973 [02:40<04:08, 10.34it/s]\u001b[A\n",
      " 35%|███▌      | 1410/3973 [02:41<05:07,  8.33it/s]\u001b[A\n",
      " 36%|███▌      | 1413/3973 [02:41<03:57, 10.79it/s]\u001b[A\n",
      " 36%|███▌      | 1415/3973 [02:41<04:46,  8.92it/s]\u001b[A\n",
      " 36%|███▌      | 1417/3973 [02:42<04:53,  8.71it/s]\u001b[A\n",
      " 36%|███▌      | 1419/3973 [02:42<05:07,  8.32it/s]\u001b[A\n",
      " 36%|███▌      | 1420/3973 [02:42<05:23,  7.90it/s]\u001b[A\n",
      " 36%|███▌      | 1421/3973 [02:42<05:38,  7.55it/s]\u001b[A\n",
      " 36%|███▌      | 1423/3973 [02:42<06:14,  6.82it/s]\u001b[A\n",
      " 36%|███▌      | 1425/3973 [02:43<06:03,  7.01it/s]\u001b[A\n",
      " 36%|███▌      | 1427/3973 [02:43<04:52,  8.70it/s]\u001b[A\n",
      " 36%|███▌      | 1429/3973 [02:43<04:40,  9.05it/s]\u001b[A\n",
      " 36%|███▌      | 1431/3973 [02:43<04:00, 10.59it/s]\u001b[A\n",
      " 36%|███▌      | 1433/3973 [02:43<04:43,  8.97it/s]\u001b[A\n",
      " 36%|███▌      | 1435/3973 [02:44<05:38,  7.50it/s]\u001b[A\n",
      " 36%|███▌      | 1438/3973 [02:44<04:25,  9.55it/s]\u001b[A\n",
      " 36%|███▌      | 1440/3973 [02:44<03:54, 10.82it/s]\u001b[A\n",
      " 36%|███▋      | 1442/3973 [02:44<04:11, 10.07it/s]\u001b[A\n",
      " 36%|███▋      | 1444/3973 [02:45<04:24,  9.54it/s]\u001b[A\n",
      " 36%|███▋      | 1447/3973 [02:45<04:08, 10.18it/s]\u001b[A\n",
      " 36%|███▋      | 1449/3973 [02:45<04:22,  9.60it/s]\u001b[A\n",
      " 37%|███▋      | 1451/3973 [02:46<05:38,  7.44it/s]\u001b[A\n",
      " 37%|███▋      | 1452/3973 [02:46<05:35,  7.51it/s]\u001b[A\n",
      " 37%|███▋      | 1455/3973 [02:46<04:19,  9.69it/s]\u001b[A\n",
      " 37%|███▋      | 1457/3973 [02:46<03:42, 11.28it/s]\u001b[A\n",
      " 37%|███▋      | 1459/3973 [02:46<03:19, 12.61it/s]\u001b[A\n",
      " 37%|███▋      | 1461/3973 [02:46<03:02, 13.75it/s]\u001b[A\n",
      " 37%|███▋      | 1464/3973 [02:46<02:48, 14.88it/s]\u001b[A\n",
      " 37%|███▋      | 1466/3973 [02:47<04:34,  9.12it/s]\u001b[A\n",
      " 37%|███▋      | 1468/3973 [02:47<04:38,  8.98it/s]\u001b[A\n",
      " 37%|███▋      | 1471/3973 [02:47<04:09, 10.02it/s]\u001b[A\n",
      " 37%|███▋      | 1473/3973 [02:47<03:47, 10.99it/s]\u001b[A\n",
      " 37%|███▋      | 1475/3973 [02:48<04:15,  9.77it/s]\u001b[A\n",
      " 37%|███▋      | 1477/3973 [02:48<04:43,  8.82it/s]\u001b[A\n",
      " 37%|███▋      | 1479/3973 [02:48<05:03,  8.23it/s]\u001b[A\n",
      " 37%|███▋      | 1480/3973 [02:48<05:17,  7.85it/s]\u001b[A\n",
      " 37%|███▋      | 1482/3973 [02:48<04:28,  9.27it/s]\u001b[A\n",
      " 37%|███▋      | 1484/3973 [02:49<04:38,  8.95it/s]\u001b[A\n",
      " 37%|███▋      | 1485/3973 [02:49<04:55,  8.42it/s]\u001b[A\n",
      " 37%|███▋      | 1486/3973 [02:49<06:51,  6.04it/s]\u001b[A\n",
      " 37%|███▋      | 1487/3973 [02:50<07:54,  5.24it/s]\u001b[A\n",
      " 37%|███▋      | 1488/3973 [02:50<08:14,  5.03it/s]\u001b[A\n",
      " 38%|███▊      | 1490/3973 [02:50<07:24,  5.58it/s]\u001b[A\n",
      " 38%|███▊      | 1495/3973 [02:50<03:55, 10.52it/s]\u001b[A\n",
      " 38%|███▊      | 1499/3973 [02:50<03:03, 13.49it/s]\u001b[A\n",
      " 38%|███▊      | 1502/3973 [02:51<02:45, 14.93it/s]\u001b[A\n",
      " 38%|███▊      | 1504/3973 [02:51<03:22, 12.17it/s]\u001b[A\n",
      " 38%|███▊      | 1507/3973 [02:51<03:10, 12.94it/s]\u001b[A\n",
      " 38%|███▊      | 1509/3973 [02:51<04:19,  9.49it/s]\u001b[A\n",
      " 38%|███▊      | 1512/3973 [02:52<04:08,  9.89it/s]\u001b[A\n",
      " 38%|███▊      | 1515/3973 [02:52<04:00, 10.21it/s]\u001b[A\n",
      " 38%|███▊      | 1517/3973 [02:52<04:28,  9.16it/s]\u001b[A\n",
      " 38%|███▊      | 1519/3973 [02:52<04:08,  9.89it/s]\u001b[A\n",
      " 38%|███▊      | 1521/3973 [02:53<04:33,  8.98it/s]\u001b[A\n",
      " 38%|███▊      | 1525/3973 [02:53<03:06, 13.12it/s]\u001b[A\n",
      " 38%|███▊      | 1527/3973 [02:53<03:23, 12.02it/s]\u001b[A\n",
      " 38%|███▊      | 1529/3973 [02:53<03:43, 10.92it/s]\u001b[A\n",
      " 39%|███▊      | 1531/3973 [02:54<04:16,  9.53it/s]\u001b[A\n",
      " 39%|███▊      | 1534/3973 [02:54<04:12,  9.67it/s]\u001b[A\n",
      " 39%|███▊      | 1536/3973 [02:54<03:57, 10.28it/s]\u001b[A\n",
      " 39%|███▊      | 1538/3973 [02:54<04:35,  8.85it/s]\u001b[A\n",
      " 39%|███▊      | 1539/3973 [02:54<04:38,  8.75it/s]\u001b[A\n",
      " 39%|███▉      | 1541/3973 [02:55<04:18,  9.40it/s]\u001b[A\n",
      " 39%|███▉      | 1543/3973 [02:55<04:13,  9.59it/s]\u001b[A\n",
      " 39%|███▉      | 1544/3973 [02:55<05:00,  8.08it/s]\u001b[A\n",
      " 39%|███▉      | 1546/3973 [02:55<04:12,  9.62it/s]\u001b[A\n",
      " 39%|███▉      | 1549/3973 [02:55<03:15, 12.39it/s]\u001b[A\n",
      " 39%|███▉      | 1551/3973 [02:55<03:12, 12.56it/s]\u001b[A\n",
      " 39%|███▉      | 1553/3973 [02:56<03:59, 10.09it/s]\u001b[A\n",
      " 39%|███▉      | 1555/3973 [02:56<03:56, 10.22it/s]\u001b[A\n",
      " 39%|███▉      | 1558/3973 [02:56<03:45, 10.69it/s]\u001b[A\n",
      " 39%|███▉      | 1561/3973 [02:56<03:10, 12.63it/s]\u001b[A\n",
      " 39%|███▉      | 1563/3973 [02:56<02:53, 13.85it/s]\u001b[A\n",
      " 39%|███▉      | 1565/3973 [02:57<03:31, 11.41it/s]\u001b[A\n",
      " 39%|███▉      | 1567/3973 [02:57<03:31, 11.36it/s]\u001b[A\n",
      " 39%|███▉      | 1569/3973 [02:57<04:20,  9.24it/s]\u001b[A\n",
      " 40%|███▉      | 1571/3973 [02:57<03:42, 10.82it/s]\u001b[A\n",
      " 40%|███▉      | 1573/3973 [02:58<04:06,  9.74it/s]\u001b[A\n",
      " 40%|███▉      | 1575/3973 [02:58<04:04,  9.81it/s]\u001b[A\n",
      " 40%|███▉      | 1577/3973 [02:58<06:30,  6.14it/s]\u001b[A\n",
      " 40%|███▉      | 1579/3973 [02:59<05:48,  6.86it/s]\u001b[A\n",
      " 40%|███▉      | 1580/3973 [02:59<05:45,  6.93it/s]\u001b[A\n",
      " 40%|███▉      | 1581/3973 [02:59<05:44,  6.95it/s]\u001b[A\n",
      " 40%|███▉      | 1583/3973 [02:59<06:12,  6.41it/s]\u001b[A\n",
      " 40%|███▉      | 1584/3973 [03:00<08:19,  4.78it/s]\u001b[A\n",
      " 40%|███▉      | 1586/3973 [03:00<07:42,  5.16it/s]\u001b[A\n",
      " 40%|███▉      | 1588/3973 [03:00<08:17,  4.80it/s]\u001b[A\n",
      " 40%|████      | 1591/3973 [03:01<05:32,  7.17it/s]\u001b[A\n",
      " 40%|████      | 1594/3973 [03:01<04:18,  9.21it/s]\u001b[A\n",
      " 40%|████      | 1596/3973 [03:01<04:09,  9.54it/s]\u001b[A\n",
      " 40%|████      | 1599/3973 [03:01<03:14, 12.23it/s]\u001b[A\n",
      " 40%|████      | 1602/3973 [03:01<02:52, 13.76it/s]\u001b[A\n",
      " 40%|████      | 1604/3973 [03:02<04:16,  9.24it/s]\u001b[A\n",
      " 40%|████      | 1606/3973 [03:02<04:15,  9.26it/s]\u001b[A\n",
      " 40%|████      | 1608/3973 [03:02<05:12,  7.57it/s]\u001b[A\n",
      " 41%|████      | 1611/3973 [03:03<04:31,  8.69it/s]\u001b[A\n",
      " 41%|████      | 1613/3973 [03:03<04:04,  9.65it/s]\u001b[A\n",
      " 41%|████      | 1615/3973 [03:03<03:55, 10.00it/s]\u001b[A\n",
      " 41%|████      | 1617/3973 [03:03<05:27,  7.18it/s]\u001b[A\n",
      " 41%|████      | 1620/3973 [03:04<04:15,  9.22it/s]\u001b[A\n",
      " 41%|████      | 1623/3973 [03:04<03:16, 11.96it/s]\u001b[A\n",
      " 41%|████      | 1625/3973 [03:04<05:50,  6.70it/s]\u001b[A\n",
      " 41%|████      | 1627/3973 [03:05<05:28,  7.13it/s]\u001b[A\n",
      " 41%|████      | 1629/3973 [03:05<05:16,  7.41it/s]\u001b[A\n",
      " 41%|████      | 1631/3973 [03:05<04:40,  8.35it/s]\u001b[A\n",
      " 41%|████      | 1633/3973 [03:05<04:40,  8.35it/s]\u001b[A\n",
      " 41%|████      | 1636/3973 [03:05<03:30, 11.11it/s]\u001b[A\n",
      " 41%|████▏     | 1639/3973 [03:06<03:25, 11.34it/s]\u001b[A\n",
      " 41%|████▏     | 1641/3973 [03:06<03:20, 11.61it/s]\u001b[A\n",
      " 41%|████▏     | 1643/3973 [03:06<04:50,  8.03it/s]\u001b[A\n",
      " 41%|████▏     | 1647/3973 [03:06<03:40, 10.54it/s]\u001b[A\n",
      " 42%|████▏     | 1649/3973 [03:07<03:38, 10.65it/s]\u001b[A\n",
      " 42%|████▏     | 1652/3973 [03:07<02:52, 13.43it/s]\u001b[A\n",
      " 42%|████▏     | 1654/3973 [03:07<02:42, 14.31it/s]\u001b[A\n",
      " 42%|████▏     | 1656/3973 [03:07<02:31, 15.34it/s]\u001b[A\n",
      " 42%|████▏     | 1658/3973 [03:07<03:05, 12.48it/s]\u001b[A\n",
      " 42%|████▏     | 1660/3973 [03:07<03:31, 10.94it/s]\u001b[A\n",
      " 42%|████▏     | 1662/3973 [03:08<03:38, 10.58it/s]\u001b[A\n",
      " 42%|████▏     | 1664/3973 [03:08<03:09, 12.19it/s]\u001b[A\n",
      " 42%|████▏     | 1666/3973 [03:08<04:47,  8.02it/s]\u001b[A\n",
      " 42%|████▏     | 1668/3973 [03:08<04:45,  8.08it/s]\u001b[A\n",
      " 42%|████▏     | 1672/3973 [03:09<04:13,  9.07it/s]\u001b[A\n",
      " 42%|████▏     | 1674/3973 [03:09<04:34,  8.39it/s]\u001b[A\n",
      " 42%|████▏     | 1675/3973 [03:09<04:53,  7.84it/s]\u001b[A\n",
      " 42%|████▏     | 1677/3973 [03:10<05:44,  6.66it/s]\u001b[A\n",
      " 42%|████▏     | 1678/3973 [03:10<05:30,  6.93it/s]\u001b[A\n",
      " 42%|████▏     | 1681/3973 [03:10<03:44, 10.21it/s]\u001b[A\n",
      " 42%|████▏     | 1683/3973 [03:10<03:57,  9.63it/s]\u001b[A\n",
      " 42%|████▏     | 1685/3973 [03:10<03:36, 10.58it/s]\u001b[A\n",
      " 42%|████▏     | 1688/3973 [03:10<03:01, 12.57it/s]\u001b[A\n",
      " 43%|████▎     | 1690/3973 [03:11<03:20, 11.37it/s]\u001b[A\n",
      " 43%|████▎     | 1692/3973 [03:11<04:18,  8.81it/s]\u001b[A\n",
      " 43%|████▎     | 1695/3973 [03:11<04:13,  8.97it/s]\u001b[A\n",
      " 43%|████▎     | 1697/3973 [03:12<04:29,  8.43it/s]\u001b[A\n",
      " 43%|████▎     | 1700/3973 [03:12<03:58,  9.51it/s]\u001b[A\n",
      " 43%|████▎     | 1702/3973 [03:12<04:57,  7.63it/s]\u001b[A\n",
      " 43%|████▎     | 1703/3973 [03:12<04:51,  7.78it/s]\u001b[A\n",
      " 43%|████▎     | 1705/3973 [03:13<04:39,  8.11it/s]\u001b[A\n",
      " 43%|████▎     | 1706/3973 [03:13<04:52,  7.76it/s]\u001b[A\n",
      " 43%|████▎     | 1707/3973 [03:13<04:56,  7.63it/s]\u001b[A\n",
      " 43%|████▎     | 1709/3973 [03:13<03:57,  9.52it/s]\u001b[A\n",
      " 43%|████▎     | 1711/3973 [03:13<04:14,  8.90it/s]\u001b[A\n",
      " 43%|████▎     | 1713/3973 [03:14<04:27,  8.44it/s]\u001b[A\n",
      " 43%|████▎     | 1715/3973 [03:14<03:47,  9.93it/s]\u001b[A\n",
      " 43%|████▎     | 1717/3973 [03:14<03:28, 10.84it/s]\u001b[A\n",
      " 43%|████▎     | 1719/3973 [03:14<05:14,  7.17it/s]\u001b[A\n",
      " 43%|████▎     | 1721/3973 [03:14<04:34,  8.21it/s]\u001b[A\n",
      " 43%|████▎     | 1723/3973 [03:15<04:09,  9.02it/s]\u001b[A\n",
      " 43%|████▎     | 1725/3973 [03:15<04:35,  8.16it/s]\u001b[A\n",
      " 43%|████▎     | 1727/3973 [03:15<03:55,  9.52it/s]\u001b[A\n",
      " 44%|████▎     | 1729/3973 [03:15<03:28, 10.77it/s]\u001b[A\n",
      " 44%|████▎     | 1731/3973 [03:16<04:55,  7.58it/s]\u001b[A\n",
      " 44%|████▎     | 1733/3973 [03:16<04:25,  8.43it/s]\u001b[A\n",
      " 44%|████▎     | 1735/3973 [03:16<03:43, 10.02it/s]\u001b[A\n",
      " 44%|████▎     | 1737/3973 [03:16<04:19,  8.61it/s]\u001b[A\n",
      " 44%|████▍     | 1739/3973 [03:17<06:06,  6.10it/s]\u001b[A\n",
      " 44%|████▍     | 1741/3973 [03:17<07:26,  5.00it/s]\u001b[A\n",
      " 44%|████▍     | 1743/3973 [03:18<06:04,  6.11it/s]\u001b[A\n",
      " 44%|████▍     | 1746/3973 [03:18<04:13,  8.79it/s]\u001b[A\n",
      " 44%|████▍     | 1749/3973 [03:18<03:19, 11.17it/s]\u001b[A\n",
      " 44%|████▍     | 1752/3973 [03:18<02:44, 13.54it/s]\u001b[A\n",
      " 44%|████▍     | 1754/3973 [03:18<02:43, 13.60it/s]\u001b[A\n",
      " 44%|████▍     | 1756/3973 [03:18<03:28, 10.62it/s]\u001b[A\n",
      " 44%|████▍     | 1758/3973 [03:19<03:27, 10.69it/s]\u001b[A\n",
      " 44%|████▍     | 1760/3973 [03:19<03:20, 11.03it/s]\u001b[A\n",
      " 44%|████▍     | 1762/3973 [03:19<04:16,  8.60it/s]\u001b[A\n",
      " 44%|████▍     | 1766/3973 [03:19<02:55, 12.58it/s]\u001b[A\n",
      " 45%|████▍     | 1768/3973 [03:19<02:55, 12.58it/s]\u001b[A\n",
      " 45%|████▍     | 1770/3973 [03:20<03:13, 11.39it/s]\u001b[A\n",
      " 45%|████▍     | 1772/3973 [03:20<03:43,  9.83it/s]\u001b[A\n",
      " 45%|████▍     | 1774/3973 [03:20<03:37, 10.10it/s]\u001b[A\n",
      " 45%|████▍     | 1776/3973 [03:21<05:36,  6.54it/s]\u001b[A\n",
      " 45%|████▍     | 1779/3973 [03:21<04:46,  7.67it/s]\u001b[A\n",
      " 45%|████▍     | 1781/3973 [03:21<05:44,  6.37it/s]\u001b[A\n",
      " 45%|████▍     | 1784/3973 [03:21<04:10,  8.74it/s]\u001b[A\n",
      " 45%|████▍     | 1786/3973 [03:22<04:31,  8.04it/s]\u001b[A\n",
      " 45%|████▌     | 1788/3973 [03:22<05:12,  7.00it/s]\u001b[A\n",
      " 45%|████▌     | 1791/3973 [03:22<04:07,  8.81it/s]\u001b[A\n",
      " 45%|████▌     | 1793/3973 [03:22<03:34, 10.18it/s]\u001b[A\n",
      " 45%|████▌     | 1795/3973 [03:23<03:11, 11.36it/s]\u001b[A\n",
      " 45%|████▌     | 1798/3973 [03:23<03:54,  9.27it/s]\u001b[A\n",
      " 45%|████▌     | 1801/3973 [03:23<03:11, 11.32it/s]\u001b[A\n",
      " 45%|████▌     | 1805/3973 [03:24<03:37,  9.95it/s]\u001b[A\n",
      " 45%|████▌     | 1807/3973 [03:24<03:19, 10.87it/s]\u001b[A\n",
      " 46%|████▌     | 1809/3973 [03:24<04:38,  7.76it/s]\u001b[A\n",
      " 46%|████▌     | 1813/3973 [03:25<04:07,  8.71it/s]\u001b[A\n",
      " 46%|████▌     | 1815/3973 [03:25<04:14,  8.49it/s]\u001b[A\n",
      " 46%|████▌     | 1816/3973 [03:25<04:18,  8.35it/s]\u001b[A\n",
      " 46%|████▌     | 1819/3973 [03:25<03:57,  9.06it/s]\u001b[A\n",
      " 46%|████▌     | 1821/3973 [03:26<04:51,  7.37it/s]\u001b[A\n",
      " 46%|████▌     | 1822/3973 [03:26<04:43,  7.59it/s]\u001b[A\n",
      " 46%|████▌     | 1823/3973 [03:26<05:26,  6.59it/s]\u001b[A\n",
      " 46%|████▌     | 1824/3973 [03:26<05:21,  6.69it/s]\u001b[A\n",
      " 46%|████▌     | 1828/3973 [03:26<03:03, 11.71it/s]\u001b[A\n",
      " 46%|████▌     | 1830/3973 [03:27<04:54,  7.27it/s]\u001b[A\n",
      " 46%|████▌     | 1832/3973 [03:27<04:15,  8.39it/s]\u001b[A\n",
      " 46%|████▌     | 1834/3973 [03:27<04:36,  7.73it/s]\u001b[A\n",
      " 46%|████▌     | 1836/3973 [03:28<04:39,  7.64it/s]\u001b[A\n",
      " 46%|████▋     | 1838/3973 [03:28<04:03,  8.75it/s]\u001b[A\n",
      " 46%|████▋     | 1840/3973 [03:28<03:47,  9.37it/s]\u001b[A\n",
      " 46%|████▋     | 1842/3973 [03:28<03:53,  9.12it/s]\u001b[A\n",
      " 46%|████▋     | 1844/3973 [03:28<04:11,  8.47it/s]\u001b[A\n",
      " 46%|████▋     | 1845/3973 [03:29<04:20,  8.16it/s]\u001b[A\n",
      " 46%|████▋     | 1847/3973 [03:29<03:37,  9.79it/s]\u001b[A\n",
      " 47%|████▋     | 1849/3973 [03:29<03:45,  9.43it/s]\u001b[A\n",
      " 47%|████▋     | 1851/3973 [03:29<04:48,  7.34it/s]\u001b[A\n",
      " 47%|████▋     | 1852/3973 [03:30<05:47,  6.10it/s]\u001b[A\n",
      " 47%|████▋     | 1854/3973 [03:30<04:45,  7.43it/s]\u001b[A\n",
      " 47%|████▋     | 1855/3973 [03:30<04:34,  7.70it/s]\u001b[A\n",
      " 47%|████▋     | 1856/3973 [03:30<05:12,  6.77it/s]\u001b[A\n",
      " 47%|████▋     | 1859/3973 [03:30<03:15, 10.79it/s]\u001b[A\n",
      " 47%|████▋     | 1861/3973 [03:30<03:43,  9.44it/s]\u001b[A\n",
      " 47%|████▋     | 1864/3973 [03:31<03:37,  9.68it/s]\u001b[A\n",
      " 47%|████▋     | 1867/3973 [03:31<03:19, 10.54it/s]\u001b[A\n",
      " 47%|████▋     | 1870/3973 [03:31<02:36, 13.43it/s]\u001b[A\n",
      " 47%|████▋     | 1872/3973 [03:31<02:47, 12.55it/s]\u001b[A\n",
      " 47%|████▋     | 1874/3973 [03:32<03:04, 11.36it/s]\u001b[A\n",
      " 47%|████▋     | 1876/3973 [03:32<03:20, 10.47it/s]\u001b[A\n",
      " 47%|████▋     | 1878/3973 [03:32<04:22,  7.97it/s]\u001b[A\n",
      " 47%|████▋     | 1880/3973 [03:32<04:27,  7.83it/s]\u001b[A\n",
      " 47%|████▋     | 1882/3973 [03:33<04:04,  8.53it/s]\u001b[A\n",
      " 47%|████▋     | 1884/3973 [03:33<03:49,  9.08it/s]\u001b[A\n",
      " 47%|████▋     | 1885/3973 [03:33<03:48,  9.12it/s]\u001b[A\n",
      " 47%|████▋     | 1887/3973 [03:33<03:49,  9.10it/s]\u001b[A\n",
      " 48%|████▊     | 1888/3973 [03:33<04:22,  7.95it/s]\u001b[A\n",
      " 48%|████▊     | 1890/3973 [03:33<03:36,  9.62it/s]\u001b[A\n",
      " 48%|████▊     | 1892/3973 [03:34<03:50,  9.01it/s]\u001b[A\n",
      " 48%|████▊     | 1894/3973 [03:34<03:57,  8.77it/s]\u001b[A\n",
      " 48%|████▊     | 1896/3973 [03:34<03:54,  8.84it/s]\u001b[A\n",
      " 48%|████▊     | 1898/3973 [03:34<03:49,  9.03it/s]\u001b[A\n",
      " 48%|████▊     | 1899/3973 [03:35<03:53,  8.88it/s]\u001b[A\n",
      " 48%|████▊     | 1900/3973 [03:35<04:32,  7.60it/s]\u001b[A\n",
      " 48%|████▊     | 1902/3973 [03:35<04:57,  6.95it/s]\u001b[A\n",
      " 48%|████▊     | 1904/3973 [03:35<05:32,  6.23it/s]\u001b[A\n",
      " 48%|████▊     | 1908/3973 [03:36<03:18, 10.40it/s]\u001b[A\n",
      " 48%|████▊     | 1910/3973 [03:36<03:09, 10.88it/s]\u001b[A\n",
      " 48%|████▊     | 1912/3973 [03:36<02:53, 11.85it/s]\u001b[A\n",
      " 48%|████▊     | 1914/3973 [03:36<03:30,  9.78it/s]\u001b[A\n",
      " 48%|████▊     | 1916/3973 [03:36<03:41,  9.30it/s]\u001b[A\n",
      " 48%|████▊     | 1918/3973 [03:37<03:11, 10.70it/s]\u001b[A\n",
      " 48%|████▊     | 1920/3973 [03:37<02:50, 12.02it/s]\u001b[A\n",
      " 48%|████▊     | 1923/3973 [03:37<03:50,  8.88it/s]\u001b[A\n",
      " 49%|████▊     | 1927/3973 [03:37<02:38, 12.88it/s]\u001b[A\n",
      " 49%|████▊     | 1929/3973 [03:37<02:44, 12.42it/s]\u001b[A\n",
      " 49%|████▊     | 1931/3973 [03:38<02:50, 12.01it/s]\u001b[A\n",
      " 49%|████▊     | 1933/3973 [03:38<05:01,  6.76it/s]\u001b[A\n",
      " 49%|████▊     | 1935/3973 [03:38<04:14,  8.00it/s]\u001b[A\n",
      " 49%|████▉     | 1937/3973 [03:39<04:29,  7.55it/s]\u001b[A\n",
      " 49%|████▉     | 1940/3973 [03:39<03:57,  8.56it/s]\u001b[A\n",
      " 49%|████▉     | 1942/3973 [03:39<05:11,  6.52it/s]\u001b[A\n",
      " 49%|████▉     | 1944/3973 [03:40<04:41,  7.20it/s]\u001b[A\n",
      " 49%|████▉     | 1946/3973 [03:40<04:45,  7.11it/s]\u001b[A\n",
      " 49%|████▉     | 1951/3973 [03:40<03:05, 10.92it/s]\u001b[A\n",
      " 49%|████▉     | 1953/3973 [03:40<03:17, 10.22it/s]\u001b[A\n",
      " 49%|████▉     | 1955/3973 [03:41<03:35,  9.35it/s]\u001b[A\n",
      " 49%|████▉     | 1957/3973 [03:41<04:39,  7.20it/s]\u001b[A\n",
      " 49%|████▉     | 1958/3973 [03:41<04:30,  7.46it/s]\u001b[A\n",
      " 49%|████▉     | 1960/3973 [03:41<04:01,  8.33it/s]\u001b[A\n",
      " 49%|████▉     | 1962/3973 [03:42<03:22,  9.93it/s]\u001b[A\n",
      " 49%|████▉     | 1964/3973 [03:42<03:28,  9.63it/s]\u001b[A\n",
      " 49%|████▉     | 1966/3973 [03:42<03:17, 10.16it/s]\u001b[A\n",
      " 50%|████▉     | 1969/3973 [03:42<02:30, 13.28it/s]\u001b[A\n",
      " 50%|████▉     | 1971/3973 [03:43<03:55,  8.49it/s]\u001b[A\n",
      " 50%|████▉     | 1975/3973 [03:43<02:50, 11.74it/s]\u001b[A\n",
      " 50%|████▉     | 1977/3973 [03:43<02:50, 11.73it/s]\u001b[A\n",
      " 50%|████▉     | 1979/3973 [03:43<02:41, 12.35it/s]\u001b[A\n",
      " 50%|████▉     | 1984/3973 [03:43<02:26, 13.56it/s]\u001b[A\n",
      " 50%|████▉     | 1986/3973 [03:43<02:18, 14.33it/s]\u001b[A\n",
      " 50%|█████     | 1988/3973 [03:44<02:35, 12.78it/s]\u001b[A\n",
      " 50%|█████     | 1990/3973 [03:44<03:39,  9.02it/s]\u001b[A\n",
      " 50%|█████     | 1992/3973 [03:44<03:14, 10.17it/s]\u001b[A\n",
      " 50%|█████     | 1994/3973 [03:44<03:07, 10.56it/s]\u001b[A\n",
      " 50%|█████     | 1996/3973 [03:44<02:53, 11.42it/s]\u001b[A\n",
      " 50%|█████     | 1999/3973 [03:45<02:39, 12.35it/s]\u001b[A\n",
      " 50%|█████     | 2001/3973 [03:45<03:00, 10.95it/s]\u001b[A\n",
      " 50%|█████     | 2003/3973 [03:45<03:10, 10.35it/s]\u001b[A\n",
      " 50%|█████     | 2006/3973 [03:45<02:38, 12.44it/s]\u001b[A\n",
      " 51%|█████     | 2008/3973 [03:46<03:34,  9.16it/s]\u001b[A\n",
      " 51%|█████     | 2010/3973 [03:46<03:28,  9.43it/s]\u001b[A\n",
      " 51%|█████     | 2013/3973 [03:46<02:53, 11.27it/s]\u001b[A\n",
      " 51%|█████     | 2015/3973 [03:46<02:51, 11.39it/s]\u001b[A\n",
      " 51%|█████     | 2017/3973 [03:46<02:43, 11.97it/s]\u001b[A\n",
      " 51%|█████     | 2020/3973 [03:47<02:20, 13.93it/s]\u001b[A\n",
      " 51%|█████     | 2022/3973 [03:47<02:58, 10.94it/s]\u001b[A\n",
      " 51%|█████     | 2024/3973 [03:47<03:00, 10.79it/s]\u001b[A\n",
      " 51%|█████     | 2027/3973 [03:47<02:44, 11.80it/s]\u001b[A\n",
      " 51%|█████     | 2029/3973 [03:47<02:48, 11.53it/s]\u001b[A\n",
      " 51%|█████     | 2031/3973 [03:48<02:46, 11.68it/s]\u001b[A\n",
      " 51%|█████     | 2033/3973 [03:48<02:31, 12.78it/s]\u001b[A\n",
      " 51%|█████     | 2035/3973 [03:48<02:54, 11.09it/s]\u001b[A\n",
      " 51%|█████▏    | 2037/3973 [03:48<02:48, 11.50it/s]\u001b[A\n",
      " 51%|█████▏    | 2039/3973 [03:48<02:58, 10.83it/s]\u001b[A\n",
      " 51%|█████▏    | 2041/3973 [03:49<04:12,  7.65it/s]\u001b[A\n",
      " 51%|█████▏    | 2044/3973 [03:49<04:03,  7.92it/s]\u001b[A\n",
      " 51%|█████▏    | 2046/3973 [03:49<03:28,  9.23it/s]\u001b[A\n",
      " 52%|█████▏    | 2048/3973 [03:50<04:05,  7.85it/s]\u001b[A\n",
      " 52%|█████▏    | 2051/3973 [03:50<03:03, 10.46it/s]\u001b[A\n",
      " 52%|█████▏    | 2053/3973 [03:50<02:48, 11.42it/s]\u001b[A\n",
      " 52%|█████▏    | 2055/3973 [03:50<02:37, 12.19it/s]\u001b[A\n",
      " 52%|█████▏    | 2057/3973 [03:50<03:08, 10.19it/s]\u001b[A\n",
      " 52%|█████▏    | 2059/3973 [03:50<03:06, 10.26it/s]\u001b[A\n",
      " 52%|█████▏    | 2061/3973 [03:51<02:56, 10.85it/s]\u001b[A\n",
      " 52%|█████▏    | 2063/3973 [03:51<03:07, 10.20it/s]\u001b[A\n",
      " 52%|█████▏    | 2066/3973 [03:51<04:00,  7.94it/s]\u001b[A\n",
      " 52%|█████▏    | 2068/3973 [03:52<04:00,  7.91it/s]\u001b[A\n",
      " 52%|█████▏    | 2071/3973 [03:52<04:11,  7.57it/s]\u001b[A\n",
      " 52%|█████▏    | 2072/3973 [03:52<04:27,  7.11it/s]\u001b[A\n",
      " 52%|█████▏    | 2073/3973 [03:53<05:57,  5.32it/s]\u001b[A\n",
      " 52%|█████▏    | 2075/3973 [03:53<05:22,  5.88it/s]\u001b[A\n",
      " 52%|█████▏    | 2077/3973 [03:53<04:37,  6.84it/s]\u001b[A\n",
      " 52%|█████▏    | 2078/3973 [03:53<04:37,  6.82it/s]\u001b[A\n",
      " 52%|█████▏    | 2079/3973 [03:53<04:37,  6.83it/s]\u001b[A\n",
      " 52%|█████▏    | 2080/3973 [03:54<05:23,  5.86it/s]\u001b[A\n",
      " 52%|█████▏    | 2082/3973 [03:54<05:00,  6.29it/s]\u001b[A\n",
      " 52%|█████▏    | 2083/3973 [03:54<04:44,  6.65it/s]\u001b[A\n",
      " 52%|█████▏    | 2084/3973 [03:54<05:38,  5.57it/s]\u001b[A\n",
      " 52%|█████▏    | 2085/3973 [03:54<05:11,  6.05it/s]\u001b[A\n",
      " 53%|█████▎    | 2089/3973 [03:55<03:40,  8.56it/s]\u001b[A\n",
      " 53%|█████▎    | 2093/3973 [03:55<02:44, 11.41it/s]\u001b[A\n",
      " 53%|█████▎    | 2095/3973 [03:55<02:55, 10.69it/s]\u001b[A\n",
      " 53%|█████▎    | 2097/3973 [03:56<03:52,  8.08it/s]\u001b[A\n",
      " 53%|█████▎    | 2099/3973 [03:56<03:24,  9.17it/s]\u001b[A\n",
      " 53%|█████▎    | 2101/3973 [03:56<03:05, 10.11it/s]\u001b[A\n",
      " 53%|█████▎    | 2104/3973 [03:56<02:23, 12.99it/s]\u001b[A\n",
      " 53%|█████▎    | 2107/3973 [03:56<02:02, 15.20it/s]\u001b[A\n",
      " 53%|█████▎    | 2109/3973 [03:56<02:06, 14.77it/s]\u001b[A\n",
      " 53%|█████▎    | 2113/3973 [03:56<01:37, 19.15it/s]\u001b[A\n",
      " 53%|█████▎    | 2116/3973 [03:57<02:01, 15.28it/s]\u001b[A\n",
      " 53%|█████▎    | 2118/3973 [03:57<02:00, 15.41it/s]\u001b[A\n",
      " 53%|█████▎    | 2120/3973 [03:57<02:35, 11.93it/s]\u001b[A\n",
      " 53%|█████▎    | 2123/3973 [03:57<02:12, 13.99it/s]\u001b[A\n",
      " 53%|█████▎    | 2125/3973 [03:57<02:19, 13.23it/s]\u001b[A\n",
      " 54%|█████▎    | 2128/3973 [03:58<01:56, 15.82it/s]\u001b[A\n",
      " 54%|█████▎    | 2130/3973 [03:58<02:57, 10.38it/s]\u001b[A\n",
      " 54%|█████▎    | 2132/3973 [03:58<03:10,  9.67it/s]\u001b[A\n",
      " 54%|█████▎    | 2135/3973 [03:59<03:55,  7.81it/s]\u001b[A\n",
      " 54%|█████▍    | 2137/3973 [03:59<03:44,  8.19it/s]\u001b[A\n",
      " 54%|█████▍    | 2140/3973 [03:59<02:47, 10.94it/s]\u001b[A\n",
      " 54%|█████▍    | 2142/3973 [04:00<04:21,  7.00it/s]\u001b[A\n",
      " 54%|█████▍    | 2145/3973 [04:00<03:54,  7.78it/s]\u001b[A\n",
      " 54%|█████▍    | 2147/3973 [04:00<03:49,  7.96it/s]\u001b[A\n",
      " 54%|█████▍    | 2150/3973 [04:00<02:55, 10.37it/s]\u001b[A\n",
      " 54%|█████▍    | 2153/3973 [04:00<02:22, 12.81it/s]\u001b[A\n",
      " 54%|█████▍    | 2155/3973 [04:01<02:54, 10.39it/s]\u001b[A\n",
      " 54%|█████▍    | 2157/3973 [04:01<03:23,  8.94it/s]\u001b[A\n",
      " 54%|█████▍    | 2159/3973 [04:01<03:25,  8.84it/s]\u001b[A\n",
      " 54%|█████▍    | 2161/3973 [04:01<03:14,  9.31it/s]\u001b[A\n",
      " 54%|█████▍    | 2164/3973 [04:02<02:35, 11.64it/s]\u001b[A\n",
      " 55%|█████▍    | 2166/3973 [04:02<02:45, 10.91it/s]\u001b[A\n",
      " 55%|█████▍    | 2168/3973 [04:02<04:31,  6.66it/s]\u001b[A\n",
      " 55%|█████▍    | 2171/3973 [04:03<03:47,  7.92it/s]\u001b[A\n",
      " 55%|█████▍    | 2173/3973 [04:03<03:47,  7.90it/s]\u001b[A\n",
      " 55%|█████▍    | 2175/3973 [04:03<03:24,  8.79it/s]\u001b[A\n",
      " 55%|█████▍    | 2178/3973 [04:03<03:20,  8.97it/s]\u001b[A\n",
      " 55%|█████▍    | 2180/3973 [04:04<03:45,  7.94it/s]\u001b[A\n",
      " 55%|█████▍    | 2181/3973 [04:04<04:03,  7.37it/s]\u001b[A\n",
      " 55%|█████▍    | 2182/3973 [04:04<04:23,  6.79it/s]\u001b[A\n",
      " 55%|█████▍    | 2183/3973 [04:04<04:35,  6.50it/s]\u001b[A\n",
      " 55%|█████▍    | 2185/3973 [04:05<03:56,  7.56it/s]\u001b[A\n",
      " 55%|█████▌    | 2187/3973 [04:05<03:26,  8.64it/s]\u001b[A\n",
      " 55%|█████▌    | 2190/3973 [04:05<02:51, 10.43it/s]\u001b[A\n",
      " 55%|█████▌    | 2193/3973 [04:05<02:38, 11.22it/s]\u001b[A\n",
      " 55%|█████▌    | 2195/3973 [04:05<02:28, 11.99it/s]\u001b[A\n",
      " 55%|█████▌    | 2198/3973 [04:06<02:34, 11.49it/s]\u001b[A\n",
      " 55%|█████▌    | 2200/3973 [04:06<03:30,  8.43it/s]\u001b[A\n",
      " 55%|█████▌    | 2204/3973 [04:06<02:25, 12.13it/s]\u001b[A\n",
      " 56%|█████▌    | 2206/3973 [04:06<02:41, 10.92it/s]\u001b[A\n",
      " 56%|█████▌    | 2208/3973 [04:07<02:35, 11.37it/s]\u001b[A\n",
      " 56%|█████▌    | 2210/3973 [04:07<02:33, 11.49it/s]\u001b[A\n",
      " 56%|█████▌    | 2212/3973 [04:07<02:57,  9.93it/s]\u001b[A\n",
      " 56%|█████▌    | 2214/3973 [04:07<02:59,  9.80it/s]\u001b[A\n",
      " 56%|█████▌    | 2216/3973 [04:08<03:41,  7.93it/s]\u001b[A\n",
      " 56%|█████▌    | 2218/3973 [04:08<03:12,  9.12it/s]\u001b[A\n",
      " 56%|█████▌    | 2220/3973 [04:08<03:44,  7.81it/s]\u001b[A\n",
      " 56%|█████▌    | 2221/3973 [04:08<04:29,  6.51it/s]\u001b[A\n",
      " 56%|█████▌    | 2224/3973 [04:09<03:54,  7.46it/s]\u001b[A\n",
      " 56%|█████▌    | 2225/3973 [04:09<03:48,  7.66it/s]\u001b[A\n",
      " 56%|█████▌    | 2227/3973 [04:09<03:07,  9.32it/s]\u001b[A\n",
      " 56%|█████▌    | 2229/3973 [04:09<02:46, 10.50it/s]\u001b[A\n",
      " 56%|█████▌    | 2231/3973 [04:09<03:13,  9.01it/s]\u001b[A\n",
      " 56%|█████▌    | 2233/3973 [04:10<03:24,  8.51it/s]\u001b[A\n",
      " 56%|█████▋    | 2236/3973 [04:10<02:59,  9.69it/s]\u001b[A\n",
      " 56%|█████▋    | 2238/3973 [04:10<03:23,  8.53it/s]\u001b[A\n",
      " 56%|█████▋    | 2242/3973 [04:10<02:16, 12.65it/s]\u001b[A\n",
      " 56%|█████▋    | 2244/3973 [04:10<02:18, 12.45it/s]\u001b[A\n",
      " 57%|█████▋    | 2246/3973 [04:11<02:48, 10.28it/s]\u001b[A\n",
      " 57%|█████▋    | 2248/3973 [04:11<02:54,  9.91it/s]\u001b[A\n",
      " 57%|█████▋    | 2251/3973 [04:11<02:24, 11.94it/s]\u001b[A\n",
      " 57%|█████▋    | 2253/3973 [04:11<02:39, 10.77it/s]\u001b[A\n",
      " 57%|█████▋    | 2255/3973 [04:12<02:52,  9.95it/s]\u001b[A\n",
      " 57%|█████▋    | 2257/3973 [04:12<02:36, 10.95it/s]\u001b[A\n",
      " 57%|█████▋    | 2259/3973 [04:12<02:33, 11.15it/s]\u001b[A\n",
      " 57%|█████▋    | 2262/3973 [04:12<02:45, 10.32it/s]\u001b[A\n",
      " 57%|█████▋    | 2264/3973 [04:12<02:43, 10.48it/s]\u001b[A\n",
      " 57%|█████▋    | 2266/3973 [04:13<02:31, 11.24it/s]\u001b[A\n",
      " 57%|█████▋    | 2268/3973 [04:13<03:29,  8.13it/s]\u001b[A\n",
      " 57%|█████▋    | 2269/3973 [04:13<03:56,  7.22it/s]\u001b[A\n",
      " 57%|█████▋    | 2271/3973 [04:13<03:09,  9.00it/s]\u001b[A\n",
      " 57%|█████▋    | 2273/3973 [04:13<03:15,  8.68it/s]\u001b[A\n",
      " 57%|█████▋    | 2275/3973 [04:14<03:28,  8.15it/s]\u001b[A\n",
      " 57%|█████▋    | 2276/3973 [04:14<04:07,  6.87it/s]\u001b[A\n",
      " 57%|█████▋    | 2277/3973 [04:14<03:54,  7.22it/s]\u001b[A\n",
      " 57%|█████▋    | 2279/3973 [04:14<03:27,  8.15it/s]\u001b[A\n",
      " 57%|█████▋    | 2281/3973 [04:15<03:20,  8.43it/s]\u001b[A\n",
      " 57%|█████▋    | 2282/3973 [04:15<03:33,  7.91it/s]\u001b[A\n",
      " 57%|█████▋    | 2283/3973 [04:15<03:25,  8.24it/s]\u001b[A\n",
      " 58%|█████▊    | 2285/3973 [04:15<04:36,  6.11it/s]\u001b[A\n",
      " 58%|█████▊    | 2288/3973 [04:16<03:50,  7.32it/s]\u001b[A\n",
      " 58%|█████▊    | 2293/3973 [04:16<02:50,  9.83it/s]\u001b[A\n",
      " 58%|█████▊    | 2297/3973 [04:16<02:09, 12.90it/s]\u001b[A\n",
      " 58%|█████▊    | 2299/3973 [04:16<02:18, 12.10it/s]\u001b[A\n",
      " 58%|█████▊    | 2301/3973 [04:16<02:28, 11.24it/s]\u001b[A\n",
      " 58%|█████▊    | 2303/3973 [04:17<02:14, 12.44it/s]\u001b[A\n",
      " 58%|█████▊    | 2306/3973 [04:17<02:25, 11.42it/s]\u001b[A\n",
      " 58%|█████▊    | 2308/3973 [04:17<02:46,  9.99it/s]\u001b[A\n",
      " 58%|█████▊    | 2310/3973 [04:18<03:24,  8.12it/s]\u001b[A\n",
      " 58%|█████▊    | 2313/3973 [04:18<03:12,  8.63it/s]\u001b[A\n",
      " 58%|█████▊    | 2315/3973 [04:18<02:50,  9.70it/s]\u001b[A\n",
      " 58%|█████▊    | 2318/3973 [04:18<02:16, 12.12it/s]\u001b[A\n",
      " 58%|█████▊    | 2320/3973 [04:19<03:02,  9.04it/s]\u001b[A\n",
      " 58%|█████▊    | 2322/3973 [04:19<04:00,  6.87it/s]\u001b[A\n",
      " 58%|█████▊    | 2323/3973 [04:19<04:07,  6.68it/s]\u001b[A\n",
      " 58%|█████▊    | 2324/3973 [04:19<03:52,  7.11it/s]\u001b[A\n",
      " 59%|█████▊    | 2328/3973 [04:19<02:14, 12.25it/s]\u001b[A\n",
      " 59%|█████▊    | 2331/3973 [04:20<01:52, 14.63it/s]\u001b[A\n",
      " 59%|█████▊    | 2334/3973 [04:20<01:42, 15.97it/s]\u001b[A\n",
      " 59%|█████▉    | 2336/3973 [04:20<02:11, 12.40it/s]\u001b[A\n",
      " 59%|█████▉    | 2338/3973 [04:20<02:33, 10.65it/s]\u001b[A\n",
      " 59%|█████▉    | 2340/3973 [04:20<02:22, 11.49it/s]\u001b[A\n",
      " 59%|█████▉    | 2342/3973 [04:21<03:18,  8.23it/s]\u001b[A\n",
      " 59%|█████▉    | 2347/3973 [04:21<02:13, 12.18it/s]\u001b[A\n",
      " 59%|█████▉    | 2350/3973 [04:21<02:15, 11.93it/s]\u001b[A\n",
      " 59%|█████▉    | 2352/3973 [04:22<03:10,  8.51it/s]\u001b[A\n",
      " 59%|█████▉    | 2354/3973 [04:22<03:09,  8.54it/s]\u001b[A\n",
      " 59%|█████▉    | 2356/3973 [04:22<02:43,  9.87it/s]\u001b[A\n",
      " 59%|█████▉    | 2358/3973 [04:22<02:49,  9.53it/s]\u001b[A\n",
      " 59%|█████▉    | 2360/3973 [04:23<03:13,  8.34it/s]\u001b[A\n",
      " 59%|█████▉    | 2363/3973 [04:23<02:43,  9.84it/s]\u001b[A\n",
      " 60%|█████▉    | 2365/3973 [04:23<03:21,  7.98it/s]\u001b[A\n",
      " 60%|█████▉    | 2367/3973 [04:23<03:16,  8.17it/s]\u001b[A\n",
      " 60%|█████▉    | 2372/3973 [04:24<01:58, 13.47it/s]\u001b[A\n",
      " 60%|█████▉    | 2376/3973 [04:24<01:39, 16.09it/s]\u001b[A\n",
      " 60%|█████▉    | 2379/3973 [04:24<01:38, 16.11it/s]\u001b[A\n",
      " 60%|█████▉    | 2381/3973 [04:24<01:58, 13.45it/s]\u001b[A\n",
      " 60%|█████▉    | 2383/3973 [04:24<02:07, 12.46it/s]\u001b[A\n",
      " 60%|██████    | 2385/3973 [04:24<02:03, 12.81it/s]\u001b[A\n",
      " 60%|██████    | 2387/3973 [04:25<03:08,  8.42it/s]\u001b[A\n",
      " 60%|██████    | 2389/3973 [04:25<03:21,  7.88it/s]\u001b[A\n",
      " 60%|██████    | 2391/3973 [04:25<03:02,  8.65it/s]\u001b[A\n",
      " 60%|██████    | 2394/3973 [04:26<02:49,  9.32it/s]\u001b[A\n",
      " 60%|██████    | 2396/3973 [04:26<02:35, 10.11it/s]\u001b[A\n",
      " 60%|██████    | 2399/3973 [04:26<02:19, 11.30it/s]\u001b[A\n",
      " 60%|██████    | 2401/3973 [04:26<02:25, 10.81it/s]\u001b[A\n",
      " 61%|██████    | 2404/3973 [04:27<03:33,  7.33it/s]\u001b[A\n",
      " 61%|██████    | 2407/3973 [04:27<02:46,  9.39it/s]\u001b[A\n",
      " 61%|██████    | 2409/3973 [04:27<02:45,  9.45it/s]\u001b[A\n",
      " 61%|██████    | 2411/3973 [04:28<03:05,  8.44it/s]\u001b[A\n",
      " 61%|██████    | 2413/3973 [04:28<03:58,  6.54it/s]\u001b[A\n",
      " 61%|██████    | 2415/3973 [04:28<03:22,  7.68it/s]\u001b[A\n",
      " 61%|██████    | 2417/3973 [04:28<03:13,  8.06it/s]\u001b[A\n",
      " 61%|██████    | 2419/3973 [04:29<02:56,  8.80it/s]\u001b[A\n",
      " 61%|██████    | 2421/3973 [04:29<02:30, 10.28it/s]\u001b[A\n",
      " 61%|██████    | 2424/3973 [04:29<01:53, 13.64it/s]\u001b[A\n",
      " 61%|██████    | 2426/3973 [04:29<01:55, 13.38it/s]\u001b[A\n",
      " 61%|██████    | 2428/3973 [04:29<01:57, 13.18it/s]\u001b[A\n",
      " 61%|██████    | 2430/3973 [04:29<02:15, 11.38it/s]\u001b[A\n",
      " 61%|██████    | 2432/3973 [04:30<02:11, 11.69it/s]\u001b[A\n",
      " 61%|██████▏   | 2434/3973 [04:30<02:03, 12.42it/s]\u001b[A\n",
      " 61%|██████▏   | 2437/3973 [04:30<03:05,  8.27it/s]\u001b[A\n",
      " 61%|██████▏   | 2439/3973 [04:30<02:37,  9.76it/s]\u001b[A\n",
      " 61%|██████▏   | 2441/3973 [04:30<02:21, 10.80it/s]\u001b[A\n",
      " 61%|██████▏   | 2443/3973 [04:31<02:10, 11.71it/s]\u001b[A\n",
      " 62%|██████▏   | 2445/3973 [04:31<02:42,  9.39it/s]\u001b[A\n",
      " 62%|██████▏   | 2447/3973 [04:31<02:30, 10.14it/s]\u001b[A\n",
      " 62%|██████▏   | 2449/3973 [04:31<02:23, 10.65it/s]\u001b[A\n",
      " 62%|██████▏   | 2451/3973 [04:32<02:51,  8.88it/s]\u001b[A\n",
      " 62%|██████▏   | 2453/3973 [04:32<02:42,  9.35it/s]\u001b[A\n",
      " 62%|██████▏   | 2455/3973 [04:32<02:42,  9.33it/s]\u001b[A\n",
      " 62%|██████▏   | 2457/3973 [04:32<02:25, 10.44it/s]\u001b[A\n",
      " 62%|██████▏   | 2459/3973 [04:33<03:26,  7.33it/s]\u001b[A\n",
      " 62%|██████▏   | 2462/3973 [04:33<02:48,  8.94it/s]\u001b[A\n",
      " 62%|██████▏   | 2464/3973 [04:33<02:50,  8.85it/s]\u001b[A\n",
      " 62%|██████▏   | 2465/3973 [04:33<02:51,  8.82it/s]\u001b[A\n",
      " 62%|██████▏   | 2466/3973 [04:33<03:40,  6.83it/s]\u001b[A\n",
      " 62%|██████▏   | 2469/3973 [04:34<02:35,  9.67it/s]\u001b[A\n",
      " 62%|██████▏   | 2471/3973 [04:34<02:46,  9.03it/s]\u001b[A\n",
      " 62%|██████▏   | 2474/3973 [04:34<02:02, 12.22it/s]\u001b[A\n",
      " 62%|██████▏   | 2476/3973 [04:34<02:12, 11.33it/s]\u001b[A\n",
      " 62%|██████▏   | 2478/3973 [04:35<03:07,  7.99it/s]\u001b[A\n",
      " 62%|██████▏   | 2481/3973 [04:35<02:37,  9.48it/s]\u001b[A\n",
      " 62%|██████▏   | 2483/3973 [04:35<02:31,  9.85it/s]\u001b[A\n",
      " 63%|██████▎   | 2485/3973 [04:35<02:57,  8.39it/s]\u001b[A\n",
      " 63%|██████▎   | 2487/3973 [04:36<03:10,  7.81it/s]\u001b[A\n",
      " 63%|██████▎   | 2489/3973 [04:36<02:56,  8.42it/s]\u001b[A\n",
      " 63%|██████▎   | 2491/3973 [04:36<02:50,  8.69it/s]\u001b[A\n",
      " 63%|██████▎   | 2494/3973 [04:36<02:25, 10.19it/s]\u001b[A\n",
      " 63%|██████▎   | 2496/3973 [04:36<02:16, 10.82it/s]\u001b[A\n",
      " 63%|██████▎   | 2498/3973 [04:37<02:15, 10.86it/s]\u001b[A\n",
      " 63%|██████▎   | 2500/3973 [04:37<02:36,  9.42it/s]\u001b[A\n",
      " 63%|██████▎   | 2502/3973 [04:37<02:31,  9.73it/s]\u001b[A\n",
      " 63%|██████▎   | 2504/3973 [04:37<02:08, 11.42it/s]\u001b[A\n",
      " 63%|██████▎   | 2506/3973 [04:37<02:12, 11.10it/s]\u001b[A\n",
      " 63%|██████▎   | 2508/3973 [04:38<02:30,  9.74it/s]\u001b[A\n",
      " 63%|██████▎   | 2510/3973 [04:38<02:43,  8.96it/s]\u001b[A\n",
      " 63%|██████▎   | 2511/3973 [04:38<03:03,  7.96it/s]\u001b[A\n",
      " 63%|██████▎   | 2513/3973 [04:38<02:36,  9.32it/s]\u001b[A\n",
      " 63%|██████▎   | 2516/3973 [04:38<02:06, 11.55it/s]\u001b[A\n",
      " 63%|██████▎   | 2518/3973 [04:39<02:15, 10.74it/s]\u001b[A\n",
      " 63%|██████▎   | 2520/3973 [04:39<02:18, 10.47it/s]\u001b[A\n",
      " 63%|██████▎   | 2522/3973 [04:39<03:37,  6.68it/s]\u001b[A\n",
      " 64%|██████▎   | 2525/3973 [04:40<02:43,  8.85it/s]\u001b[A\n",
      " 64%|██████▎   | 2527/3973 [04:40<02:30,  9.62it/s]\u001b[A\n",
      " 64%|██████▎   | 2530/3973 [04:40<02:16, 10.54it/s]\u001b[A\n",
      " 64%|██████▎   | 2532/3973 [04:40<02:11, 10.98it/s]\u001b[A\n",
      " 64%|██████▍   | 2534/3973 [04:40<02:39,  9.01it/s]\u001b[A\n",
      " 64%|██████▍   | 2536/3973 [04:41<02:54,  8.25it/s]\u001b[A\n",
      " 64%|██████▍   | 2537/3973 [04:41<02:57,  8.09it/s]\u001b[A\n",
      " 64%|██████▍   | 2538/3973 [04:41<03:18,  7.24it/s]\u001b[A\n",
      " 64%|██████▍   | 2540/3973 [04:41<02:47,  8.57it/s]\u001b[A\n",
      " 64%|██████▍   | 2541/3973 [04:41<02:49,  8.42it/s]\u001b[A\n",
      " 64%|██████▍   | 2542/3973 [04:42<03:14,  7.36it/s]\u001b[A\n",
      " 64%|██████▍   | 2543/3973 [04:42<03:19,  7.15it/s]\u001b[A\n",
      " 64%|██████▍   | 2544/3973 [04:42<03:21,  7.10it/s]\u001b[A\n",
      " 64%|██████▍   | 2545/3973 [04:42<04:40,  5.09it/s]\u001b[A\n",
      " 64%|██████▍   | 2548/3973 [04:42<03:16,  7.24it/s]\u001b[A\n",
      " 64%|██████▍   | 2550/3973 [04:43<03:05,  7.69it/s]\u001b[A\n",
      " 64%|██████▍   | 2551/3973 [04:43<03:28,  6.82it/s]\u001b[A\n",
      " 64%|██████▍   | 2553/3973 [04:43<02:42,  8.72it/s]\u001b[A\n",
      " 64%|██████▍   | 2555/3973 [04:43<02:16, 10.39it/s]\u001b[A\n",
      " 64%|██████▍   | 2558/3973 [04:43<02:17, 10.29it/s]\u001b[A\n",
      " 64%|██████▍   | 2560/3973 [04:44<02:06, 11.19it/s]\u001b[A\n",
      " 64%|██████▍   | 2562/3973 [04:44<02:19, 10.15it/s]\u001b[A\n",
      " 65%|██████▍   | 2566/3973 [04:44<01:42, 13.68it/s]\u001b[A\n",
      " 65%|██████▍   | 2568/3973 [04:44<02:00, 11.70it/s]\u001b[A\n",
      " 65%|██████▍   | 2570/3973 [04:45<03:39,  6.40it/s]\u001b[A\n",
      " 65%|██████▍   | 2571/3973 [04:45<03:26,  6.78it/s]\u001b[A\n",
      " 65%|██████▍   | 2573/3973 [04:45<02:57,  7.88it/s]\u001b[A\n",
      " 65%|██████▍   | 2575/3973 [04:46<03:23,  6.86it/s]\u001b[A\n",
      " 65%|██████▍   | 2577/3973 [04:46<02:43,  8.55it/s]\u001b[A\n",
      " 65%|██████▍   | 2579/3973 [04:46<02:28,  9.40it/s]\u001b[A\n",
      " 65%|██████▍   | 2582/3973 [04:46<02:20,  9.89it/s]\u001b[A\n",
      " 65%|██████▌   | 2584/3973 [04:46<02:16, 10.19it/s]\u001b[A\n",
      " 65%|██████▌   | 2587/3973 [04:47<02:17, 10.12it/s]\u001b[A\n",
      " 65%|██████▌   | 2590/3973 [04:47<01:57, 11.79it/s]\u001b[A\n",
      " 65%|██████▌   | 2592/3973 [04:47<02:22,  9.66it/s]\u001b[A\n",
      " 65%|██████▌   | 2595/3973 [04:47<01:56, 11.85it/s]\u001b[A\n",
      " 65%|██████▌   | 2598/3973 [04:47<01:36, 14.25it/s]\u001b[A\n",
      " 65%|██████▌   | 2602/3973 [04:48<01:40, 13.58it/s]\u001b[A\n",
      " 66%|██████▌   | 2605/3973 [04:48<01:35, 14.33it/s]\u001b[A\n",
      " 66%|██████▌   | 2607/3973 [04:48<01:34, 14.53it/s]\u001b[A\n",
      " 66%|██████▌   | 2609/3973 [04:48<01:49, 12.40it/s]\u001b[A\n",
      " 66%|██████▌   | 2611/3973 [04:48<02:08, 10.57it/s]\u001b[A\n",
      " 66%|██████▌   | 2613/3973 [04:49<03:46,  6.01it/s]\u001b[A\n",
      " 66%|██████▌   | 2614/3973 [04:49<03:32,  6.38it/s]\u001b[A\n",
      " 66%|██████▌   | 2618/3973 [04:50<02:29,  9.04it/s]\u001b[A\n",
      " 66%|██████▌   | 2620/3973 [04:50<02:13, 10.17it/s]\u001b[A\n",
      " 66%|██████▌   | 2622/3973 [04:50<02:44,  8.19it/s]\u001b[A\n",
      " 66%|██████▌   | 2624/3973 [04:51<03:48,  5.90it/s]\u001b[A\n",
      " 66%|██████▌   | 2626/3973 [04:51<03:35,  6.26it/s]\u001b[A\n",
      " 66%|██████▌   | 2629/3973 [04:51<02:42,  8.25it/s]\u001b[A\n",
      " 66%|██████▌   | 2631/3973 [04:51<02:31,  8.85it/s]\u001b[A\n",
      " 66%|██████▋   | 2633/3973 [04:51<02:24,  9.27it/s]\u001b[A\n",
      " 66%|██████▋   | 2635/3973 [04:52<02:20,  9.55it/s]\u001b[A\n",
      " 66%|██████▋   | 2637/3973 [04:52<02:35,  8.59it/s]\u001b[A\n",
      " 66%|██████▋   | 2638/3973 [04:52<02:48,  7.94it/s]\u001b[A\n",
      " 66%|██████▋   | 2640/3973 [04:52<02:26,  9.08it/s]\u001b[A\n",
      " 66%|██████▋   | 2641/3973 [04:52<02:33,  8.67it/s]\u001b[A\n",
      " 67%|██████▋   | 2644/3973 [04:53<02:23,  9.27it/s]\u001b[A\n",
      " 67%|██████▋   | 2645/3973 [04:53<02:37,  8.46it/s]\u001b[A\n",
      " 67%|██████▋   | 2647/3973 [04:53<02:39,  8.31it/s]\u001b[A\n",
      " 67%|██████▋   | 2650/3973 [04:53<02:08, 10.27it/s]\u001b[A\n",
      " 67%|██████▋   | 2652/3973 [04:53<01:51, 11.87it/s]\u001b[A\n",
      " 67%|██████▋   | 2654/3973 [04:54<02:22,  9.23it/s]\u001b[A\n",
      " 67%|██████▋   | 2656/3973 [04:54<02:15,  9.72it/s]\u001b[A\n",
      " 67%|██████▋   | 2659/3973 [04:54<01:47, 12.26it/s]\u001b[A\n",
      " 67%|██████▋   | 2661/3973 [04:55<03:51,  5.66it/s]\u001b[A\n",
      " 67%|██████▋   | 2663/3973 [04:55<04:11,  5.20it/s]\u001b[A\n",
      " 67%|██████▋   | 2665/3973 [04:56<03:33,  6.13it/s]\u001b[A\n",
      " 67%|██████▋   | 2666/3973 [04:56<03:50,  5.67it/s]\u001b[A\n",
      " 67%|██████▋   | 2667/3973 [04:56<03:33,  6.11it/s]\u001b[A\n",
      " 67%|██████▋   | 2669/3973 [04:56<03:21,  6.48it/s]\u001b[A\n",
      " 67%|██████▋   | 2670/3973 [04:56<03:23,  6.39it/s]\u001b[A\n",
      " 67%|██████▋   | 2673/3973 [04:57<02:19,  9.30it/s]\u001b[A\n",
      " 67%|██████▋   | 2675/3973 [04:57<02:12,  9.77it/s]\u001b[A\n",
      " 67%|██████▋   | 2677/3973 [04:57<02:04, 10.45it/s]\u001b[A\n",
      " 67%|██████▋   | 2680/3973 [04:57<01:47, 12.02it/s]\u001b[A\n",
      " 68%|██████▊   | 2682/3973 [04:57<01:44, 12.41it/s]\u001b[A\n",
      " 68%|██████▊   | 2684/3973 [04:58<02:06, 10.17it/s]\u001b[A\n",
      " 68%|██████▊   | 2686/3973 [04:58<02:22,  9.01it/s]\u001b[A\n",
      " 68%|██████▊   | 2687/3973 [04:58<02:22,  9.00it/s]\u001b[A\n",
      " 68%|██████▊   | 2688/3973 [04:58<02:58,  7.20it/s]\u001b[A\n",
      " 68%|██████▊   | 2691/3973 [04:58<01:59, 10.76it/s]\u001b[A\n",
      " 68%|██████▊   | 2693/3973 [04:59<02:11,  9.74it/s]\u001b[A\n",
      " 68%|██████▊   | 2695/3973 [04:59<02:24,  8.87it/s]\u001b[A\n",
      " 68%|██████▊   | 2697/3973 [04:59<02:18,  9.25it/s]\u001b[A\n",
      " 68%|██████▊   | 2700/3973 [04:59<02:01, 10.46it/s]\u001b[A\n",
      " 68%|██████▊   | 2705/3973 [04:59<01:20, 15.82it/s]\u001b[A\n",
      " 68%|██████▊   | 2707/3973 [05:00<01:36, 13.07it/s]\u001b[A\n",
      " 68%|██████▊   | 2709/3973 [05:00<01:31, 13.77it/s]\u001b[A\n",
      " 68%|██████▊   | 2712/3973 [05:00<01:30, 13.98it/s]\u001b[A\n",
      " 68%|██████▊   | 2714/3973 [05:00<01:33, 13.46it/s]\u001b[A\n",
      " 68%|██████▊   | 2716/3973 [05:00<01:28, 14.13it/s]\u001b[A\n",
      " 68%|██████▊   | 2718/3973 [05:00<01:25, 14.76it/s]\u001b[A\n",
      " 68%|██████▊   | 2720/3973 [05:00<01:24, 14.83it/s]\u001b[A\n",
      " 69%|██████▊   | 2722/3973 [05:01<02:43,  7.63it/s]\u001b[A\n",
      " 69%|██████▊   | 2725/3973 [05:01<01:59, 10.48it/s]\u001b[A\n",
      " 69%|██████▊   | 2727/3973 [05:01<01:58, 10.53it/s]\u001b[A\n",
      " 69%|██████▊   | 2729/3973 [05:02<02:28,  8.40it/s]\u001b[A\n",
      " 69%|██████▊   | 2731/3973 [05:02<03:00,  6.88it/s]\u001b[A\n",
      " 69%|██████▉   | 2734/3973 [05:02<02:30,  8.25it/s]\u001b[A\n",
      " 69%|██████▉   | 2736/3973 [05:03<02:26,  8.42it/s]\u001b[A\n",
      " 69%|██████▉   | 2738/3973 [05:03<02:30,  8.22it/s]\u001b[A\n",
      " 69%|██████▉   | 2739/3973 [05:03<02:35,  7.92it/s]\u001b[A\n",
      " 69%|██████▉   | 2740/3973 [05:03<02:41,  7.65it/s]\u001b[A\n",
      " 69%|██████▉   | 2742/3973 [05:03<02:35,  7.94it/s]\u001b[A\n",
      " 69%|██████▉   | 2745/3973 [05:04<02:39,  7.68it/s]\u001b[A\n",
      " 69%|██████▉   | 2747/3973 [05:04<02:19,  8.80it/s]\u001b[A\n",
      " 69%|██████▉   | 2748/3973 [05:04<02:39,  7.69it/s]\u001b[A\n",
      " 69%|██████▉   | 2750/3973 [05:04<02:39,  7.68it/s]\u001b[A\n",
      " 69%|██████▉   | 2753/3973 [05:05<01:53, 10.78it/s]\u001b[A\n",
      " 69%|██████▉   | 2755/3973 [05:05<02:10,  9.34it/s]\u001b[A\n",
      " 69%|██████▉   | 2757/3973 [05:05<02:47,  7.26it/s]\u001b[A\n",
      " 69%|██████▉   | 2758/3973 [05:06<03:29,  5.79it/s]\u001b[A\n",
      " 70%|██████▉   | 2762/3973 [05:06<02:29,  8.10it/s]\u001b[A\n",
      " 70%|██████▉   | 2763/3973 [05:06<02:55,  6.90it/s]\u001b[A\n",
      " 70%|██████▉   | 2766/3973 [05:06<02:12,  9.12it/s]\u001b[A\n",
      " 70%|██████▉   | 2768/3973 [05:07<02:12,  9.09it/s]\u001b[A\n",
      " 70%|██████▉   | 2770/3973 [05:07<02:08,  9.40it/s]\u001b[A\n",
      " 70%|██████▉   | 2772/3973 [05:07<02:15,  8.88it/s]\u001b[A\n",
      " 70%|██████▉   | 2773/3973 [05:07<02:30,  7.97it/s]\u001b[A\n",
      " 70%|██████▉   | 2775/3973 [05:07<02:37,  7.60it/s]\u001b[A\n",
      " 70%|██████▉   | 2777/3973 [05:08<02:12,  9.04it/s]\u001b[A\n",
      " 70%|██████▉   | 2779/3973 [05:08<03:04,  6.47it/s]\u001b[A\n",
      " 70%|██████▉   | 2780/3973 [05:08<03:03,  6.50it/s]\u001b[A\n",
      " 70%|███████   | 2782/3973 [05:08<02:33,  7.77it/s]\u001b[A\n",
      " 70%|███████   | 2783/3973 [05:09<02:27,  8.08it/s]\u001b[A\n",
      " 70%|███████   | 2785/3973 [05:09<02:06,  9.36it/s]\u001b[A\n",
      " 70%|███████   | 2787/3973 [05:09<02:07,  9.30it/s]\u001b[A\n",
      " 70%|███████   | 2789/3973 [05:09<01:58, 10.01it/s]\u001b[A\n",
      " 70%|███████   | 2791/3973 [05:10<02:47,  7.07it/s]\u001b[A\n",
      " 70%|███████   | 2793/3973 [05:10<02:26,  8.08it/s]\u001b[A\n",
      " 70%|███████   | 2795/3973 [05:10<02:05,  9.41it/s]\u001b[A\n",
      " 70%|███████   | 2797/3973 [05:10<02:01,  9.64it/s]\u001b[A\n",
      " 70%|███████   | 2799/3973 [05:10<02:01,  9.64it/s]\u001b[A\n",
      " 71%|███████   | 2801/3973 [05:10<01:56, 10.07it/s]\u001b[A\n",
      " 71%|███████   | 2803/3973 [05:11<03:18,  5.89it/s]\u001b[A\n",
      " 71%|███████   | 2805/3973 [05:11<02:44,  7.09it/s]\u001b[A\n",
      " 71%|███████   | 2806/3973 [05:11<03:00,  6.48it/s]\u001b[A\n",
      " 71%|███████   | 2808/3973 [05:12<02:39,  7.31it/s]\u001b[A\n",
      " 71%|███████   | 2809/3973 [05:12<02:37,  7.38it/s]\u001b[A\n",
      " 71%|███████   | 2811/3973 [05:12<02:06,  9.18it/s]\u001b[A\n",
      " 71%|███████   | 2813/3973 [05:12<02:28,  7.83it/s]\u001b[A\n",
      " 71%|███████   | 2814/3973 [05:12<02:55,  6.59it/s]\u001b[A\n",
      " 71%|███████   | 2816/3973 [05:13<02:24,  8.02it/s]\u001b[A\n",
      " 71%|███████   | 2817/3973 [05:13<02:21,  8.17it/s]\u001b[A\n",
      " 71%|███████   | 2819/3973 [05:13<01:52, 10.26it/s]\u001b[A\n",
      " 71%|███████   | 2823/3973 [05:13<01:48, 10.57it/s]\u001b[A\n",
      " 71%|███████   | 2825/3973 [05:13<01:52, 10.24it/s]\u001b[A\n",
      " 71%|███████   | 2829/3973 [05:14<01:18, 14.63it/s]\u001b[A\n",
      " 71%|███████▏  | 2831/3973 [05:14<01:52, 10.14it/s]\u001b[A\n",
      " 71%|███████▏  | 2833/3973 [05:14<01:52, 10.18it/s]\u001b[A\n",
      " 71%|███████▏  | 2835/3973 [05:14<02:08,  8.87it/s]\u001b[A\n",
      " 71%|███████▏  | 2837/3973 [05:15<02:14,  8.46it/s]\u001b[A\n",
      " 71%|███████▏  | 2838/3973 [05:15<02:20,  8.10it/s]\u001b[A\n",
      " 72%|███████▏  | 2841/3973 [05:15<01:38, 11.53it/s]\u001b[A\n",
      " 72%|███████▏  | 2843/3973 [05:15<01:43, 10.97it/s]\u001b[A\n",
      " 72%|███████▏  | 2845/3973 [05:15<01:38, 11.47it/s]\u001b[A\n",
      " 72%|███████▏  | 2847/3973 [05:16<01:37, 11.54it/s]\u001b[A\n",
      " 72%|███████▏  | 2849/3973 [05:16<01:43, 10.87it/s]\u001b[A\n",
      " 72%|███████▏  | 2851/3973 [05:16<01:29, 12.56it/s]\u001b[A\n",
      " 72%|███████▏  | 2853/3973 [05:16<02:17,  8.13it/s]\u001b[A\n",
      " 72%|███████▏  | 2855/3973 [05:16<01:59,  9.36it/s]\u001b[A\n",
      " 72%|███████▏  | 2857/3973 [05:17<01:56,  9.57it/s]\u001b[A\n",
      " 72%|███████▏  | 2861/3973 [05:17<01:28, 12.62it/s]\u001b[A\n",
      " 72%|███████▏  | 2863/3973 [05:17<01:44, 10.61it/s]\u001b[A\n",
      " 72%|███████▏  | 2865/3973 [05:17<02:02,  9.02it/s]\u001b[A\n",
      " 72%|███████▏  | 2867/3973 [05:18<02:08,  8.60it/s]\u001b[A\n",
      " 72%|███████▏  | 2868/3973 [05:18<02:08,  8.58it/s]\u001b[A\n",
      " 72%|███████▏  | 2870/3973 [05:18<02:07,  8.64it/s]\u001b[A\n",
      " 72%|███████▏  | 2871/3973 [05:18<02:38,  6.94it/s]\u001b[A\n",
      " 72%|███████▏  | 2873/3973 [05:18<02:08,  8.55it/s]\u001b[A\n",
      " 72%|███████▏  | 2875/3973 [05:19<01:50,  9.92it/s]\u001b[A\n",
      " 72%|███████▏  | 2877/3973 [05:19<01:38, 11.15it/s]\u001b[A\n",
      " 72%|███████▏  | 2879/3973 [05:19<02:28,  7.36it/s]\u001b[A\n",
      " 73%|███████▎  | 2881/3973 [05:20<02:56,  6.20it/s]\u001b[A\n",
      " 73%|███████▎  | 2882/3973 [05:20<02:54,  6.25it/s]\u001b[A\n",
      " 73%|███████▎  | 2883/3973 [05:20<03:38,  5.00it/s]\u001b[A\n",
      " 73%|███████▎  | 2884/3973 [05:20<03:49,  4.75it/s]\u001b[A\n",
      " 73%|███████▎  | 2886/3973 [05:20<02:54,  6.23it/s]\u001b[A\n",
      " 73%|███████▎  | 2887/3973 [05:21<02:53,  6.25it/s]\u001b[A\n",
      " 73%|███████▎  | 2891/3973 [05:21<01:56,  9.26it/s]\u001b[A\n",
      " 73%|███████▎  | 2892/3973 [05:21<02:03,  8.78it/s]\u001b[A\n",
      " 73%|███████▎  | 2895/3973 [05:22<02:22,  7.56it/s]\u001b[A\n",
      " 73%|███████▎  | 2897/3973 [05:22<02:01,  8.85it/s]\u001b[A\n",
      " 73%|███████▎  | 2899/3973 [05:22<01:58,  9.04it/s]\u001b[A\n",
      " 73%|███████▎  | 2901/3973 [05:22<01:57,  9.10it/s]\u001b[A\n",
      " 73%|███████▎  | 2904/3973 [05:23<02:35,  6.88it/s]\u001b[A\n",
      " 73%|███████▎  | 2906/3973 [05:23<02:15,  7.90it/s]\u001b[A\n",
      " 73%|███████▎  | 2908/3973 [05:23<01:59,  8.90it/s]\u001b[A\n",
      " 73%|███████▎  | 2913/3973 [05:23<01:13, 14.52it/s]\u001b[A\n",
      " 73%|███████▎  | 2915/3973 [05:23<01:21, 12.94it/s]\u001b[A\n",
      " 73%|███████▎  | 2917/3973 [05:24<01:22, 12.74it/s]\u001b[A\n",
      " 73%|███████▎  | 2919/3973 [05:24<01:55,  9.12it/s]\u001b[A\n",
      " 74%|███████▎  | 2921/3973 [05:24<02:28,  7.10it/s]\u001b[A\n",
      " 74%|███████▎  | 2923/3973 [05:25<02:12,  7.93it/s]\u001b[A\n",
      " 74%|███████▎  | 2925/3973 [05:25<02:18,  7.55it/s]\u001b[A\n",
      " 74%|███████▎  | 2928/3973 [05:25<01:53,  9.24it/s]\u001b[A\n",
      " 74%|███████▎  | 2930/3973 [05:25<02:16,  7.66it/s]\u001b[A\n",
      " 74%|███████▍  | 2932/3973 [05:26<02:18,  7.54it/s]\u001b[A\n",
      " 74%|███████▍  | 2934/3973 [05:26<02:29,  6.95it/s]\u001b[A\n",
      " 74%|███████▍  | 2937/3973 [05:26<02:07,  8.11it/s]\u001b[A\n",
      " 74%|███████▍  | 2938/3973 [05:27<02:29,  6.95it/s]\u001b[A\n",
      " 74%|███████▍  | 2939/3973 [05:27<02:53,  5.96it/s]\u001b[A\n",
      " 74%|███████▍  | 2942/3973 [05:27<01:59,  8.63it/s]\u001b[A\n",
      " 74%|███████▍  | 2944/3973 [05:27<02:17,  7.50it/s]\u001b[A\n",
      " 74%|███████▍  | 2948/3973 [05:28<01:34, 10.83it/s]\u001b[A\n",
      " 74%|███████▍  | 2950/3973 [05:28<01:45,  9.73it/s]\u001b[A\n",
      " 74%|███████▍  | 2952/3973 [05:28<01:48,  9.44it/s]\u001b[A\n",
      " 74%|███████▍  | 2954/3973 [05:28<01:57,  8.70it/s]\u001b[A\n",
      " 74%|███████▍  | 2955/3973 [05:29<02:50,  5.98it/s]\u001b[A\n",
      " 74%|███████▍  | 2958/3973 [05:29<02:27,  6.90it/s]\u001b[A\n",
      " 75%|███████▍  | 2960/3973 [05:29<02:00,  8.40it/s]\u001b[A\n",
      " 75%|███████▍  | 2963/3973 [05:29<01:30, 11.12it/s]\u001b[A\n",
      " 75%|███████▍  | 2966/3973 [05:29<01:14, 13.55it/s]\u001b[A\n",
      " 75%|███████▍  | 2970/3973 [05:30<00:59, 16.77it/s]\u001b[A\n",
      " 75%|███████▍  | 2973/3973 [05:30<01:13, 13.63it/s]\u001b[A\n",
      " 75%|███████▍  | 2975/3973 [05:30<01:32, 10.85it/s]\u001b[A\n",
      " 75%|███████▍  | 2977/3973 [05:30<01:32, 10.71it/s]\u001b[A\n",
      " 75%|███████▌  | 2980/3973 [05:31<01:15, 13.16it/s]\u001b[A\n",
      " 75%|███████▌  | 2982/3973 [05:31<01:23, 11.84it/s]\u001b[A\n",
      " 75%|███████▌  | 2984/3973 [05:31<01:45,  9.35it/s]\u001b[A\n",
      " 75%|███████▌  | 2986/3973 [05:31<01:36, 10.20it/s]\u001b[A\n",
      " 75%|███████▌  | 2988/3973 [05:32<01:49,  8.99it/s]\u001b[A\n",
      " 75%|███████▌  | 2990/3973 [05:32<02:09,  7.57it/s]\u001b[A\n",
      " 75%|███████▌  | 2991/3973 [05:32<02:08,  7.62it/s]\u001b[A\n",
      " 75%|███████▌  | 2992/3973 [05:32<02:18,  7.11it/s]\u001b[A\n",
      " 75%|███████▌  | 2994/3973 [05:32<02:05,  7.81it/s]\u001b[A\n",
      " 75%|███████▌  | 2995/3973 [05:33<02:22,  6.85it/s]\u001b[A\n",
      " 75%|███████▌  | 2996/3973 [05:33<02:15,  7.22it/s]\u001b[A\n",
      " 75%|███████▌  | 2997/3973 [05:33<02:29,  6.54it/s]\u001b[A\n",
      " 76%|███████▌  | 3001/3973 [05:33<01:19, 12.21it/s]\u001b[A\n",
      " 76%|███████▌  | 3003/3973 [05:33<01:23, 11.67it/s]\u001b[A\n",
      " 76%|███████▌  | 3005/3973 [05:33<01:22, 11.76it/s]\u001b[A\n",
      " 76%|███████▌  | 3007/3973 [05:34<01:59,  8.07it/s]\u001b[A\n",
      " 76%|███████▌  | 3009/3973 [05:34<01:47,  8.95it/s]\u001b[A\n",
      " 76%|███████▌  | 3011/3973 [05:34<01:41,  9.47it/s]\u001b[A\n",
      " 76%|███████▌  | 3013/3973 [05:34<01:41,  9.45it/s]\u001b[A\n",
      " 76%|███████▌  | 3016/3973 [05:35<01:20, 11.82it/s]\u001b[A\n",
      " 76%|███████▌  | 3018/3973 [05:35<01:37,  9.84it/s]\u001b[A\n",
      " 76%|███████▌  | 3020/3973 [05:35<02:07,  7.50it/s]\u001b[A\n",
      " 76%|███████▌  | 3023/3973 [05:35<01:33, 10.21it/s]\u001b[A\n",
      " 76%|███████▌  | 3025/3973 [05:36<01:40,  9.39it/s]\u001b[A\n",
      " 76%|███████▌  | 3027/3973 [05:36<02:47,  5.66it/s]\u001b[A\n",
      " 76%|███████▌  | 3029/3973 [05:37<02:16,  6.91it/s]\u001b[A\n",
      " 76%|███████▋  | 3032/3973 [05:37<01:37,  9.67it/s]\u001b[A\n",
      " 76%|███████▋  | 3036/3973 [05:37<01:08, 13.64it/s]\u001b[A\n",
      " 76%|███████▋  | 3039/3973 [05:38<02:18,  6.73it/s]\u001b[A\n",
      " 77%|███████▋  | 3041/3973 [05:38<02:19,  6.68it/s]\u001b[A\n",
      " 77%|███████▋  | 3043/3973 [05:38<02:13,  6.99it/s]\u001b[A\n",
      " 77%|███████▋  | 3045/3973 [05:39<02:13,  6.93it/s]\u001b[A\n",
      " 77%|███████▋  | 3048/3973 [05:39<01:41,  9.07it/s]\u001b[A\n",
      " 77%|███████▋  | 3050/3973 [05:39<01:38,  9.40it/s]\u001b[A\n",
      " 77%|███████▋  | 3052/3973 [05:39<01:38,  9.34it/s]\u001b[A\n",
      " 77%|███████▋  | 3054/3973 [05:40<02:01,  7.58it/s]\u001b[A\n",
      " 77%|███████▋  | 3056/3973 [05:41<05:22,  2.84it/s]\u001b[A\n",
      " 77%|███████▋  | 3057/3973 [05:44<10:11,  1.50it/s]\u001b[A\n",
      " 77%|███████▋  | 3058/3973 [05:44<08:42,  1.75it/s]\u001b[A\n",
      " 77%|███████▋  | 3060/3973 [05:45<08:53,  1.71it/s]\u001b[A\n",
      " 77%|███████▋  | 3061/3973 [05:45<07:34,  2.01it/s]\u001b[A\n",
      " 77%|███████▋  | 3062/3973 [05:45<06:46,  2.24it/s]\u001b[A\n",
      " 77%|███████▋  | 3064/3973 [05:46<05:39,  2.68it/s]\u001b[A\n",
      " 77%|███████▋  | 3065/3973 [05:46<05:43,  2.64it/s]\u001b[A\n",
      " 77%|███████▋  | 3067/3973 [05:46<04:04,  3.70it/s]\u001b[A\n",
      " 77%|███████▋  | 3070/3973 [05:47<02:42,  5.57it/s]\u001b[A\n",
      " 77%|███████▋  | 3071/3973 [05:47<02:43,  5.52it/s]\u001b[A\n",
      " 77%|███████▋  | 3072/3973 [05:47<03:33,  4.23it/s]\u001b[A\n",
      " 77%|███████▋  | 3073/3973 [05:48<03:46,  3.98it/s]\u001b[A\n",
      " 77%|███████▋  | 3076/3973 [05:48<02:13,  6.74it/s]\u001b[A\n",
      " 77%|███████▋  | 3078/3973 [05:48<02:12,  6.74it/s]\u001b[A\n",
      " 77%|███████▋  | 3079/3973 [05:48<02:48,  5.31it/s]\u001b[A\n",
      " 78%|███████▊  | 3080/3973 [05:49<02:45,  5.41it/s]\u001b[A\n",
      " 78%|███████▊  | 3082/3973 [05:49<02:40,  5.56it/s]\u001b[A\n",
      " 78%|███████▊  | 3083/3973 [05:50<05:13,  2.84it/s]\u001b[A\n",
      " 78%|███████▊  | 3084/3973 [05:50<04:50,  3.06it/s]\u001b[A\n",
      " 78%|███████▊  | 3085/3973 [05:50<04:35,  3.22it/s]\u001b[A\n",
      " 78%|███████▊  | 3086/3973 [05:51<05:08,  2.87it/s]\u001b[A\n",
      " 78%|███████▊  | 3087/3973 [05:51<04:20,  3.41it/s]\u001b[A\n",
      " 78%|███████▊  | 3088/3973 [05:51<04:29,  3.29it/s]\u001b[A\n",
      " 78%|███████▊  | 3089/3973 [05:52<04:25,  3.33it/s]\u001b[A\n",
      " 78%|███████▊  | 3090/3973 [05:52<03:39,  4.03it/s]\u001b[A\n",
      " 78%|███████▊  | 3091/3973 [05:52<03:35,  4.10it/s]\u001b[A\n",
      " 78%|███████▊  | 3092/3973 [05:52<03:24,  4.30it/s]\u001b[A\n",
      " 78%|███████▊  | 3094/3973 [05:52<02:18,  6.37it/s]\u001b[A\n",
      " 78%|███████▊  | 3095/3973 [05:53<02:20,  6.26it/s]\u001b[A\n",
      " 78%|███████▊  | 3097/3973 [05:53<01:46,  8.20it/s]\u001b[A\n",
      " 78%|███████▊  | 3098/3973 [05:53<01:54,  7.62it/s]\u001b[A\n",
      " 78%|███████▊  | 3101/3973 [05:53<01:57,  7.40it/s]\u001b[A\n",
      " 78%|███████▊  | 3103/3973 [05:53<01:38,  8.86it/s]\u001b[A\n",
      " 78%|███████▊  | 3106/3973 [05:54<01:09, 12.41it/s]\u001b[A\n",
      " 78%|███████▊  | 3108/3973 [05:54<01:12, 12.00it/s]\u001b[A\n",
      " 78%|███████▊  | 3110/3973 [05:54<02:05,  6.88it/s]\u001b[A\n",
      " 78%|███████▊  | 3112/3973 [05:55<03:15,  4.41it/s]\u001b[A\n",
      " 78%|███████▊  | 3114/3973 [05:55<02:35,  5.53it/s]\u001b[A\n",
      " 78%|███████▊  | 3116/3973 [05:56<03:01,  4.72it/s]\u001b[A\n",
      " 78%|███████▊  | 3117/3973 [05:56<02:54,  4.92it/s]\u001b[A\n",
      " 78%|███████▊  | 3118/3973 [05:56<02:47,  5.10it/s]\u001b[A\n",
      " 79%|███████▊  | 3121/3973 [05:56<01:45,  8.09it/s]\u001b[A\n",
      " 79%|███████▊  | 3125/3973 [05:56<01:07, 12.55it/s]\u001b[A\n",
      " 79%|███████▊  | 3128/3973 [05:57<00:58, 14.36it/s]\u001b[A\n",
      " 79%|███████▉  | 3130/3973 [05:57<01:00, 13.86it/s]\u001b[A\n",
      " 79%|███████▉  | 3132/3973 [05:57<00:57, 14.73it/s]\u001b[A\n",
      " 79%|███████▉  | 3134/3973 [05:58<03:47,  3.69it/s]\u001b[A\n",
      " 79%|███████▉  | 3137/3973 [05:59<02:43,  5.11it/s]\u001b[A\n",
      " 79%|███████▉  | 3142/3973 [05:59<01:35,  8.74it/s]\u001b[A\n",
      " 79%|███████▉  | 3145/3973 [06:00<02:31,  5.47it/s]\u001b[A\n",
      " 79%|███████▉  | 3150/3973 [06:00<01:56,  7.06it/s]\u001b[A\n",
      " 79%|███████▉  | 3153/3973 [06:00<01:35,  8.61it/s]\u001b[A\n",
      " 79%|███████▉  | 3155/3973 [06:00<01:24,  9.64it/s]\u001b[A\n",
      " 79%|███████▉  | 3158/3973 [06:01<01:07, 11.99it/s]\u001b[A\n",
      " 80%|███████▉  | 3162/3973 [06:01<00:55, 14.57it/s]\u001b[A\n",
      " 80%|███████▉  | 3165/3973 [06:01<00:48, 16.62it/s]\u001b[A\n",
      " 80%|███████▉  | 3168/3973 [06:01<00:45, 17.68it/s]\u001b[A\n",
      " 80%|███████▉  | 3171/3973 [06:01<00:43, 18.37it/s]\u001b[A\n",
      " 80%|███████▉  | 3175/3973 [06:01<00:35, 22.21it/s]\u001b[A\n",
      " 80%|████████  | 3179/3973 [06:01<00:31, 25.58it/s]\u001b[A\n",
      " 80%|████████  | 3183/3973 [06:02<00:38, 20.57it/s]\u001b[A\n",
      " 80%|████████  | 3186/3973 [06:02<00:42, 18.71it/s]\u001b[A\n",
      " 80%|████████  | 3189/3973 [06:03<01:23,  9.38it/s]\u001b[A\n",
      " 80%|████████  | 3191/3973 [06:03<01:27,  8.96it/s]\u001b[A\n",
      " 80%|████████  | 3193/3973 [06:03<01:45,  7.39it/s]\u001b[A\n",
      " 80%|████████  | 3195/3973 [06:04<01:50,  7.02it/s]\u001b[A\n",
      " 80%|████████  | 3197/3973 [06:04<01:44,  7.42it/s]\u001b[A\n",
      " 81%|████████  | 3200/3973 [06:04<01:25,  9.03it/s]\u001b[A\n",
      " 81%|████████  | 3202/3973 [06:04<01:29,  8.57it/s]\u001b[A\n",
      " 81%|████████  | 3204/3973 [06:04<01:20,  9.60it/s]\u001b[A\n",
      " 81%|████████  | 3207/3973 [06:05<01:07, 11.30it/s]\u001b[A\n",
      " 81%|████████  | 3209/3973 [06:05<01:01, 12.43it/s]\u001b[A\n",
      " 81%|████████  | 3211/3973 [06:05<01:10, 10.79it/s]\u001b[A\n",
      " 81%|████████  | 3214/3973 [06:05<01:10, 10.77it/s]\u001b[A\n",
      " 81%|████████  | 3216/3973 [06:05<01:05, 11.53it/s]\u001b[A\n",
      " 81%|████████  | 3219/3973 [06:06<00:54, 13.83it/s]\u001b[A\n",
      " 81%|████████  | 3221/3973 [06:06<01:30,  8.29it/s]\u001b[A\n",
      " 81%|████████  | 3223/3973 [06:06<01:27,  8.53it/s]\u001b[A\n",
      " 81%|████████  | 3226/3973 [06:07<01:15,  9.89it/s]\u001b[A\n",
      " 81%|████████  | 3228/3973 [06:07<01:11, 10.43it/s]\u001b[A\n",
      " 81%|████████▏ | 3231/3973 [06:07<00:57, 12.99it/s]\u001b[A\n",
      " 81%|████████▏ | 3235/3973 [06:07<00:42, 17.19it/s]\u001b[A\n",
      " 82%|████████▏ | 3238/3973 [06:07<00:58, 12.55it/s]\u001b[A\n",
      " 82%|████████▏ | 3240/3973 [06:08<01:38,  7.44it/s]\u001b[A\n",
      " 82%|████████▏ | 3243/3973 [06:08<01:38,  7.40it/s]\u001b[A\n",
      " 82%|████████▏ | 3245/3973 [06:09<01:36,  7.51it/s]\u001b[A\n",
      " 82%|████████▏ | 3247/3973 [06:09<01:54,  6.36it/s]\u001b[A\n",
      " 82%|████████▏ | 3250/3973 [06:09<01:24,  8.56it/s]\u001b[A\n",
      " 82%|████████▏ | 3252/3973 [06:09<01:22,  8.76it/s]\u001b[A\n",
      " 82%|████████▏ | 3254/3973 [06:10<01:11, 10.01it/s]\u001b[A\n",
      " 82%|████████▏ | 3256/3973 [06:10<01:14,  9.68it/s]\u001b[A\n",
      " 82%|████████▏ | 3258/3973 [06:10<01:23,  8.53it/s]\u001b[A\n",
      " 82%|████████▏ | 3260/3973 [06:10<01:16,  9.36it/s]\u001b[A\n",
      " 82%|████████▏ | 3262/3973 [06:11<01:47,  6.61it/s]\u001b[A\n",
      " 82%|████████▏ | 3265/3973 [06:11<01:18,  8.97it/s]\u001b[A\n",
      " 82%|████████▏ | 3267/3973 [06:11<01:14,  9.46it/s]\u001b[A\n",
      " 82%|████████▏ | 3269/3973 [06:11<01:04, 10.98it/s]\u001b[A\n",
      " 82%|████████▏ | 3271/3973 [06:11<00:59, 11.71it/s]\u001b[A\n",
      " 82%|████████▏ | 3273/3973 [06:12<01:42,  6.80it/s]\u001b[A\n",
      " 82%|████████▏ | 3275/3973 [06:12<01:33,  7.48it/s]\u001b[A\n",
      " 82%|████████▏ | 3277/3973 [06:12<01:19,  8.78it/s]\u001b[A\n",
      " 83%|████████▎ | 3279/3973 [06:12<01:06, 10.47it/s]\u001b[A\n",
      " 83%|████████▎ | 3281/3973 [06:13<01:11,  9.65it/s]\u001b[A\n",
      " 83%|████████▎ | 3283/3973 [06:13<01:05, 10.58it/s]\u001b[A\n",
      " 83%|████████▎ | 3285/3973 [06:13<01:05, 10.43it/s]\u001b[A\n",
      " 83%|████████▎ | 3287/3973 [06:13<01:24,  8.10it/s]\u001b[A\n",
      " 83%|████████▎ | 3289/3973 [06:14<01:23,  8.18it/s]\u001b[A\n",
      " 83%|████████▎ | 3290/3973 [06:14<01:20,  8.44it/s]\u001b[A\n",
      " 83%|████████▎ | 3291/3973 [06:14<02:10,  5.21it/s]\u001b[A\n",
      " 83%|████████▎ | 3292/3973 [06:14<02:18,  4.93it/s]\u001b[A\n",
      " 83%|████████▎ | 3294/3973 [06:14<01:37,  6.96it/s]\u001b[A\n",
      " 83%|████████▎ | 3296/3973 [06:15<01:22,  8.16it/s]\u001b[A\n",
      " 83%|████████▎ | 3298/3973 [06:15<01:11,  9.50it/s]\u001b[A\n",
      " 83%|████████▎ | 3300/3973 [06:15<01:30,  7.45it/s]\u001b[A\n",
      " 83%|████████▎ | 3302/3973 [06:15<01:17,  8.60it/s]\u001b[A\n",
      " 83%|████████▎ | 3304/3973 [06:16<01:39,  6.70it/s]\u001b[A\n",
      " 83%|████████▎ | 3308/3973 [06:16<01:03, 10.45it/s]\u001b[A\n",
      " 83%|████████▎ | 3310/3973 [06:16<00:56, 11.81it/s]\u001b[A\n",
      " 83%|████████▎ | 3312/3973 [06:17<01:21,  8.16it/s]\u001b[A\n",
      " 83%|████████▎ | 3314/3973 [06:17<01:12,  9.04it/s]\u001b[A\n",
      " 83%|████████▎ | 3316/3973 [06:17<01:04, 10.11it/s]\u001b[A\n",
      " 84%|████████▎ | 3318/3973 [06:17<01:00, 10.80it/s]\u001b[A\n",
      " 84%|████████▎ | 3320/3973 [06:18<01:37,  6.72it/s]\u001b[A\n",
      " 84%|████████▎ | 3322/3973 [06:18<01:24,  7.73it/s]\u001b[A\n",
      " 84%|████████▎ | 3324/3973 [06:18<01:09,  9.29it/s]\u001b[A\n",
      " 84%|████████▎ | 3326/3973 [06:18<01:02, 10.33it/s]\u001b[A\n",
      " 84%|████████▍ | 3329/3973 [06:18<00:49, 13.03it/s]\u001b[A\n",
      " 84%|████████▍ | 3332/3973 [06:18<00:55, 11.56it/s]\u001b[A\n",
      " 84%|████████▍ | 3335/3973 [06:19<00:58, 10.90it/s]\u001b[A\n",
      " 84%|████████▍ | 3337/3973 [06:19<01:22,  7.70it/s]\u001b[A\n",
      " 84%|████████▍ | 3339/3973 [06:20<02:30,  4.20it/s]\u001b[A\n",
      " 84%|████████▍ | 3341/3973 [06:20<01:59,  5.31it/s]\u001b[A\n",
      " 84%|████████▍ | 3344/3973 [06:21<01:26,  7.31it/s]\u001b[A\n",
      " 84%|████████▍ | 3346/3973 [06:21<01:29,  7.03it/s]\u001b[A\n",
      " 84%|████████▍ | 3348/3973 [06:21<01:17,  8.03it/s]\u001b[A\n",
      " 84%|████████▍ | 3350/3973 [06:21<01:10,  8.78it/s]\u001b[A\n",
      " 84%|████████▍ | 3353/3973 [06:21<00:59, 10.48it/s]\u001b[A\n",
      " 84%|████████▍ | 3355/3973 [06:22<00:57, 10.70it/s]\u001b[A\n",
      " 84%|████████▍ | 3357/3973 [06:22<00:56, 10.81it/s]\u001b[A\n",
      " 85%|████████▍ | 3360/3973 [06:22<00:45, 13.35it/s]\u001b[A\n",
      " 85%|████████▍ | 3363/3973 [06:22<00:42, 14.34it/s]\u001b[A\n",
      " 85%|████████▍ | 3365/3973 [06:22<00:41, 14.76it/s]\u001b[A\n",
      " 85%|████████▍ | 3368/3973 [06:22<00:35, 17.26it/s]\u001b[A\n",
      " 85%|████████▍ | 3371/3973 [06:22<00:32, 18.49it/s]\u001b[A\n",
      " 85%|████████▍ | 3373/3973 [06:23<00:43, 13.88it/s]\u001b[A\n",
      " 85%|████████▍ | 3375/3973 [06:23<00:54, 10.89it/s]\u001b[A\n",
      " 85%|████████▍ | 3377/3973 [06:23<00:51, 11.54it/s]\u001b[A\n",
      " 85%|████████▌ | 3381/3973 [06:23<00:44, 13.18it/s]\u001b[A\n",
      " 85%|████████▌ | 3383/3973 [06:24<00:53, 10.94it/s]\u001b[A\n",
      " 85%|████████▌ | 3385/3973 [06:24<00:56, 10.48it/s]\u001b[A\n",
      " 85%|████████▌ | 3387/3973 [06:24<00:59,  9.91it/s]\u001b[A\n",
      " 85%|████████▌ | 3390/3973 [06:25<01:25,  6.85it/s]\u001b[A\n",
      " 85%|████████▌ | 3392/3973 [06:25<01:11,  8.08it/s]\u001b[A\n",
      " 85%|████████▌ | 3394/3973 [06:25<01:15,  7.68it/s]\u001b[A\n",
      " 85%|████████▌ | 3395/3973 [06:25<01:16,  7.56it/s]\u001b[A\n",
      " 86%|████████▌ | 3398/3973 [06:26<00:59,  9.70it/s]\u001b[A\n",
      " 86%|████████▌ | 3400/3973 [06:26<01:26,  6.59it/s]\u001b[A\n",
      " 86%|████████▌ | 3402/3973 [06:26<01:11,  8.01it/s]\u001b[A\n",
      " 86%|████████▌ | 3404/3973 [06:26<01:01,  9.32it/s]\u001b[A\n",
      " 86%|████████▌ | 3406/3973 [06:27<01:07,  8.37it/s]\u001b[A\n",
      " 86%|████████▌ | 3408/3973 [06:27<01:05,  8.63it/s]\u001b[A\n",
      " 86%|████████▌ | 3411/3973 [06:27<00:49, 11.39it/s]\u001b[A\n",
      " 86%|████████▌ | 3413/3973 [06:27<00:46, 11.92it/s]\u001b[A\n",
      " 86%|████████▌ | 3415/3973 [06:27<00:43, 12.94it/s]\u001b[A\n",
      " 86%|████████▌ | 3417/3973 [06:28<00:56,  9.86it/s]\u001b[A\n",
      " 86%|████████▌ | 3419/3973 [06:28<00:59,  9.27it/s]\u001b[A\n",
      " 86%|████████▌ | 3421/3973 [06:28<00:53, 10.33it/s]\u001b[A\n",
      " 86%|████████▌ | 3424/3973 [06:28<00:45, 12.10it/s]\u001b[A\n",
      " 86%|████████▌ | 3426/3973 [06:28<00:55,  9.94it/s]\u001b[A\n",
      " 86%|████████▋ | 3428/3973 [06:29<00:56,  9.61it/s]\u001b[A\n",
      " 86%|████████▋ | 3430/3973 [06:29<00:53, 10.11it/s]\u001b[A\n",
      " 86%|████████▋ | 3432/3973 [06:30<01:36,  5.62it/s]\u001b[A\n",
      " 86%|████████▋ | 3435/3973 [06:30<01:07,  7.94it/s]\u001b[A\n",
      " 87%|████████▋ | 3438/3973 [06:30<00:54,  9.90it/s]\u001b[A\n",
      " 87%|████████▋ | 3440/3973 [06:30<00:49, 10.74it/s]\u001b[A\n",
      " 87%|████████▋ | 3444/3973 [06:30<00:34, 15.13it/s]\u001b[A\n",
      " 87%|████████▋ | 3447/3973 [06:31<00:51, 10.20it/s]\u001b[A\n",
      " 87%|████████▋ | 3450/3973 [06:31<01:01,  8.46it/s]\u001b[A\n",
      " 87%|████████▋ | 3452/3973 [06:32<01:12,  7.16it/s]\u001b[A\n",
      " 87%|████████▋ | 3454/3973 [06:32<01:36,  5.40it/s]\u001b[A\n",
      " 87%|████████▋ | 3458/3973 [06:32<01:02,  8.18it/s]\u001b[A\n",
      " 87%|████████▋ | 3462/3973 [06:32<00:44, 11.42it/s]\u001b[A\n",
      " 87%|████████▋ | 3466/3973 [06:33<00:34, 14.50it/s]\u001b[A\n",
      " 87%|████████▋ | 3469/3973 [06:33<00:31, 15.85it/s]\u001b[A\n",
      " 87%|████████▋ | 3473/3973 [06:33<00:30, 16.18it/s]\u001b[A\n",
      " 87%|████████▋ | 3476/3973 [06:43<07:39,  1.08it/s]\u001b[A\n",
      " 88%|████████▊ | 3478/3973 [06:48<10:05,  1.22s/it]\u001b[A\n",
      " 88%|████████▊ | 3480/3973 [06:54<13:15,  1.61s/it]\u001b[A\n",
      " 88%|████████▊ | 3481/3973 [06:56<14:27,  1.76s/it]\u001b[A\n",
      " 88%|████████▊ | 3482/3973 [07:02<19:34,  2.39s/it]\u001b[A\n",
      " 88%|████████▊ | 3483/3973 [07:04<19:19,  2.37s/it]\u001b[A\n",
      " 88%|████████▊ | 3484/3973 [07:15<33:44,  4.14s/it]\u001b[A\n",
      " 88%|████████▊ | 3485/3973 [07:17<30:27,  3.74s/it]\u001b[A\n",
      " 88%|████████▊ | 3486/3973 [07:24<36:57,  4.55s/it]\u001b[A\n",
      " 88%|████████▊ | 3487/3973 [07:26<31:31,  3.89s/it]\u001b[A\n",
      " 88%|████████▊ | 3488/3973 [07:28<27:22,  3.39s/it]\u001b[A\n",
      " 88%|████████▊ | 3489/3973 [07:35<36:02,  4.47s/it]\u001b[A\n",
      " 88%|████████▊ | 3490/3973 [07:38<31:41,  3.94s/it]\u001b[A\n",
      " 88%|████████▊ | 3491/3973 [07:41<29:32,  3.68s/it]\u001b[A\n",
      " 88%|████████▊ | 3492/3973 [07:45<30:57,  3.86s/it]\u001b[A\n",
      " 88%|████████▊ | 3493/3973 [07:47<26:06,  3.26s/it]\u001b[A\n",
      " 88%|████████▊ | 3494/3973 [07:52<30:18,  3.80s/it]\u001b[A\n",
      " 88%|████████▊ | 3495/3973 [07:57<31:57,  4.01s/it]\u001b[A\n",
      " 88%|████████▊ | 3496/3973 [07:59<26:26,  3.33s/it]\u001b[A\n",
      " 88%|████████▊ | 3497/3973 [08:01<25:18,  3.19s/it]\u001b[A\n",
      " 88%|████████▊ | 3498/3973 [08:04<23:45,  3.00s/it]\u001b[A\n",
      " 88%|████████▊ | 3499/3973 [08:06<22:13,  2.81s/it]\u001b[A\n",
      " 88%|████████▊ | 3500/3973 [08:12<27:46,  3.52s/it]\u001b[A\n",
      " 88%|████████▊ | 3501/3973 [08:13<22:44,  2.89s/it]\u001b[A\n",
      " 88%|████████▊ | 3502/3973 [08:14<18:28,  2.35s/it]\u001b[A\n",
      " 88%|████████▊ | 3503/3973 [08:15<14:48,  1.89s/it]\u001b[A\n",
      " 88%|████████▊ | 3504/3973 [08:16<13:40,  1.75s/it]\u001b[A\n",
      " 88%|████████▊ | 3505/3973 [08:25<30:50,  3.95s/it]\u001b[A\n",
      " 88%|████████▊ | 3506/3973 [08:31<33:57,  4.36s/it]\u001b[A\n",
      " 88%|████████▊ | 3507/3973 [08:32<27:13,  3.51s/it]\u001b[A\n",
      " 88%|████████▊ | 3508/3973 [08:40<35:59,  4.64s/it]\u001b[A\n",
      " 88%|████████▊ | 3509/3973 [08:45<36:49,  4.76s/it]\u001b[A\n",
      " 88%|████████▊ | 3510/3973 [08:47<32:10,  4.17s/it]\u001b[A\n",
      " 88%|████████▊ | 3511/3973 [08:50<28:18,  3.68s/it]\u001b[A\n",
      " 88%|████████▊ | 3512/3973 [08:54<29:04,  3.78s/it]\u001b[A\n",
      " 88%|████████▊ | 3513/3973 [08:57<26:26,  3.45s/it]\u001b[A\n",
      " 88%|████████▊ | 3514/3973 [08:59<24:10,  3.16s/it]\u001b[A\n",
      " 88%|████████▊ | 3515/3973 [09:02<23:21,  3.06s/it]\u001b[A\n",
      " 88%|████████▊ | 3516/3973 [09:09<32:11,  4.23s/it]\u001b[A\n",
      " 89%|████████▊ | 3517/3973 [09:11<28:35,  3.76s/it]\u001b[A\n",
      " 89%|████████▊ | 3518/3973 [09:16<29:35,  3.90s/it]\u001b[A\n",
      " 89%|████████▊ | 3519/3973 [09:20<30:17,  4.00s/it]\u001b[A\n",
      " 89%|████████▊ | 3520/3973 [09:21<23:29,  3.11s/it]\u001b[A\n",
      " 89%|████████▊ | 3521/3973 [09:26<27:15,  3.62s/it]\u001b[A\n",
      " 89%|████████▊ | 3522/3973 [09:27<22:23,  2.98s/it]\u001b[A\n",
      " 89%|████████▊ | 3523/3973 [09:29<19:32,  2.60s/it]\u001b[A\n",
      " 89%|████████▊ | 3524/3973 [09:34<25:31,  3.41s/it]\u001b[A\n",
      " 89%|████████▊ | 3525/3973 [09:42<35:04,  4.70s/it]\u001b[A\n",
      " 89%|████████▊ | 3526/3973 [09:50<42:13,  5.67s/it]\u001b[A\n",
      " 89%|████████▉ | 3527/3973 [09:52<33:12,  4.47s/it]\u001b[A\n",
      " 89%|████████▉ | 3528/3973 [09:54<28:46,  3.88s/it]\u001b[A\n",
      " 89%|████████▉ | 3529/3973 [09:56<23:14,  3.14s/it]\u001b[A\n",
      " 89%|████████▉ | 3530/3973 [09:57<19:03,  2.58s/it]\u001b[A\n",
      " 89%|████████▉ | 3531/3973 [09:58<16:02,  2.18s/it]\u001b[A\n",
      " 89%|████████▉ | 3532/3973 [10:00<16:07,  2.19s/it]\u001b[A\n",
      " 89%|████████▉ | 3533/3973 [10:03<16:12,  2.21s/it]\u001b[A\n",
      " 89%|████████▉ | 3534/3973 [10:06<17:57,  2.45s/it]\u001b[A\n",
      " 89%|████████▉ | 3535/3973 [10:08<16:51,  2.31s/it]\u001b[A\n",
      " 89%|████████▉ | 3536/3973 [10:09<15:08,  2.08s/it]\u001b[A\n",
      " 89%|████████▉ | 3537/3973 [10:13<19:09,  2.64s/it]\u001b[A\n",
      " 89%|████████▉ | 3538/3973 [10:19<25:41,  3.54s/it]\u001b[A\n",
      " 89%|████████▉ | 3539/3973 [10:32<46:21,  6.41s/it]\u001b[A\n",
      " 89%|████████▉ | 3540/3973 [10:54<1:20:11, 11.11s/it]\u001b[A\n",
      " 89%|████████▉ | 3541/3973 [11:20<1:53:25, 15.75s/it]\u001b[A\n",
      " 89%|████████▉ | 3542/3973 [11:27<1:34:12, 13.12s/it]\u001b[A\n",
      " 89%|████████▉ | 3543/3973 [11:36<1:24:21, 11.77s/it]\u001b[A\n",
      " 89%|████████▉ | 3544/3973 [11:43<1:14:33, 10.43s/it]\u001b[A\n",
      " 89%|████████▉ | 3545/3973 [11:46<58:45,  8.24s/it]  \u001b[A\n",
      " 89%|████████▉ | 3546/3973 [11:49<46:37,  6.55s/it]\u001b[A\n",
      " 89%|████████▉ | 3547/3973 [11:55<45:23,  6.39s/it]\u001b[A\n",
      " 89%|████████▉ | 3548/3973 [11:57<35:21,  4.99s/it]\u001b[A\n",
      " 89%|████████▉ | 3549/3973 [12:00<30:39,  4.34s/it]\u001b[A\n",
      " 89%|████████▉ | 3550/3973 [12:02<26:49,  3.81s/it]\u001b[A\n",
      " 89%|████████▉ | 3551/3973 [12:08<31:21,  4.46s/it]\u001b[A\n",
      " 89%|████████▉ | 3552/3973 [12:11<27:07,  3.87s/it]\u001b[A\n",
      " 89%|████████▉ | 3553/3973 [12:23<44:25,  6.35s/it]\u001b[A\n",
      " 89%|████████▉ | 3554/3973 [12:28<42:40,  6.11s/it]\u001b[A\n",
      " 89%|████████▉ | 3555/3973 [12:32<37:03,  5.32s/it]\u001b[A\n",
      " 90%|████████▉ | 3556/3973 [12:40<43:20,  6.24s/it]\u001b[A\n",
      " 90%|████████▉ | 3557/3973 [12:42<33:22,  4.81s/it]\u001b[A\n",
      " 90%|████████▉ | 3558/3973 [12:46<32:41,  4.73s/it]\u001b[A\n",
      " 90%|████████▉ | 3559/3973 [12:49<28:41,  4.16s/it]\u001b[A\n",
      " 90%|████████▉ | 3560/3973 [12:52<26:44,  3.88s/it]\u001b[A\n",
      " 90%|████████▉ | 3561/3973 [12:55<24:31,  3.57s/it]\u001b[A\n",
      " 90%|████████▉ | 3562/3973 [13:01<29:08,  4.26s/it]\u001b[A\n",
      " 90%|████████▉ | 3563/3973 [13:04<27:33,  4.03s/it]\u001b[A\n",
      " 90%|████████▉ | 3564/3973 [13:07<23:25,  3.44s/it]\u001b[A\n",
      " 90%|████████▉ | 3565/3973 [13:13<30:22,  4.47s/it]\u001b[A\n",
      " 90%|████████▉ | 3566/3973 [13:17<28:15,  4.16s/it]\u001b[A\n",
      " 90%|████████▉ | 3567/3973 [13:18<22:37,  3.34s/it]\u001b[A\n",
      " 90%|████████▉ | 3568/3973 [13:21<21:48,  3.23s/it]\u001b[A\n",
      " 90%|████████▉ | 3569/3973 [13:23<18:54,  2.81s/it]\u001b[A\n",
      " 90%|████████▉ | 3570/3973 [13:26<19:01,  2.83s/it]\u001b[A\n",
      " 90%|████████▉ | 3571/3973 [13:37<34:34,  5.16s/it]\u001b[A\n",
      " 90%|████████▉ | 3572/3973 [13:43<37:35,  5.63s/it]\u001b[A\n",
      " 90%|████████▉ | 3573/3973 [13:45<30:10,  4.53s/it]\u001b[A\n",
      " 90%|████████▉ | 3574/3973 [13:51<32:31,  4.89s/it]\u001b[A\n",
      " 90%|████████▉ | 3575/3973 [13:53<26:13,  3.95s/it]\u001b[A\n",
      " 90%|█████████ | 3576/3973 [13:59<31:15,  4.72s/it]\u001b[A\n",
      " 90%|█████████ | 3577/3973 [14:03<29:30,  4.47s/it]\u001b[A\n",
      " 90%|█████████ | 3578/3973 [14:05<23:38,  3.59s/it]\u001b[A\n",
      " 90%|█████████ | 3579/3973 [14:06<19:33,  2.98s/it]\u001b[A\n",
      " 90%|█████████ | 3580/3973 [14:09<19:54,  3.04s/it]\u001b[A\n",
      " 90%|█████████ | 3581/3973 [14:17<28:57,  4.43s/it]\u001b[A\n",
      " 90%|█████████ | 3582/3973 [14:22<29:47,  4.57s/it]\u001b[A\n",
      " 90%|█████████ | 3583/3973 [14:24<23:58,  3.69s/it]\u001b[A\n",
      " 90%|█████████ | 3584/3973 [14:27<22:58,  3.54s/it]\u001b[A\n",
      " 90%|█████████ | 3585/3973 [14:29<20:26,  3.16s/it]\u001b[A\n",
      " 90%|█████████ | 3586/3973 [14:34<23:41,  3.67s/it]\u001b[A\n",
      " 90%|█████████ | 3587/3973 [14:39<26:18,  4.09s/it]\u001b[A\n",
      " 90%|█████████ | 3588/3973 [14:45<30:23,  4.74s/it]\u001b[A\n",
      " 90%|█████████ | 3589/3973 [14:48<26:21,  4.12s/it]\u001b[A\n",
      " 90%|█████████ | 3590/3973 [14:51<24:16,  3.80s/it]\u001b[A\n",
      " 90%|█████████ | 3591/3973 [14:53<20:49,  3.27s/it]\u001b[A\n",
      " 90%|█████████ | 3592/3973 [15:00<27:19,  4.30s/it]\u001b[A\n",
      " 90%|█████████ | 3593/3973 [15:05<28:13,  4.46s/it]\u001b[A\n",
      " 90%|█████████ | 3594/3973 [15:11<32:02,  5.07s/it]\u001b[A\n",
      " 90%|█████████ | 3595/3973 [15:15<29:14,  4.64s/it]\u001b[A\n",
      " 91%|█████████ | 3596/3973 [15:16<22:12,  3.54s/it]\u001b[A\n",
      " 91%|█████████ | 3597/3973 [15:24<31:55,  5.09s/it]\u001b[A\n",
      " 91%|█████████ | 3598/3973 [15:30<33:32,  5.37s/it]\u001b[A\n",
      " 91%|█████████ | 3599/3973 [15:32<27:10,  4.36s/it]\u001b[A\n",
      " 91%|█████████ | 3600/3973 [15:37<28:06,  4.52s/it]\u001b[A\n",
      " 91%|█████████ | 3601/3973 [15:40<24:14,  3.91s/it]\u001b[A\n",
      " 91%|█████████ | 3602/3973 [15:42<20:26,  3.30s/it]\u001b[A\n",
      " 91%|█████████ | 3603/3973 [15:45<20:12,  3.28s/it]\u001b[A\n",
      " 91%|█████████ | 3604/3973 [15:47<18:02,  2.93s/it]\u001b[A\n",
      " 91%|█████████ | 3605/3973 [15:55<28:05,  4.58s/it]\u001b[A\n",
      " 91%|█████████ | 3606/3973 [16:51<2:01:03, 19.79s/it]\u001b[A\n",
      " 91%|█████████ | 3607/3973 [17:04<1:49:37, 17.97s/it]\u001b[A\n",
      " 91%|█████████ | 3608/3973 [18:19<3:33:12, 35.05s/it]\u001b[A\n",
      " 91%|█████████ | 3609/3973 [18:45<3:14:47, 32.11s/it]\u001b[A\n",
      " 91%|█████████ | 3610/3973 [19:09<2:59:31, 29.67s/it]\u001b[A\n",
      " 91%|█████████ | 3611/3973 [19:23<2:32:04, 25.21s/it]\u001b[A\n",
      " 91%|█████████ | 3612/3973 [19:39<2:14:18, 22.32s/it]\u001b[A\n",
      " 91%|█████████ | 3613/3973 [19:54<2:01:36, 20.27s/it]\u001b[A\n",
      " 91%|█████████ | 3614/3973 [20:09<1:50:20, 18.44s/it]\u001b[A\n",
      " 91%|█████████ | 3615/3973 [20:26<1:48:37, 18.21s/it]\u001b[A\n",
      " 91%|█████████ | 3616/3973 [20:42<1:44:02, 17.49s/it]\u001b[A\n",
      " 91%|█████████ | 3617/3973 [20:59<1:43:19, 17.41s/it]\u001b[A\n",
      " 91%|█████████ | 3618/3973 [22:02<3:03:38, 31.04s/it]\u001b[A\n",
      " 91%|█████████ | 3619/3973 [22:45<3:23:27, 34.48s/it]\u001b[A\n",
      " 91%|█████████ | 3620/3973 [23:02<2:52:21, 29.30s/it]\u001b[A\n",
      " 91%|█████████ | 3621/3973 [23:21<2:33:54, 26.24s/it]\u001b[A\n",
      " 91%|█████████ | 3622/3973 [23:36<2:13:15, 22.78s/it]\u001b[A\n",
      " 91%|█████████ | 3623/3973 [23:51<1:59:54, 20.55s/it]\u001b[A\n",
      " 91%|█████████ | 3624/3973 [24:05<1:47:33, 18.49s/it]\u001b[A\n",
      " 91%|█████████ | 3625/3973 [24:16<1:34:58, 16.37s/it]\u001b[A\n",
      " 91%|█████████▏| 3626/3973 [24:28<1:27:31, 15.13s/it]\u001b[A\n",
      " 91%|█████████▏| 3627/3973 [25:29<2:45:36, 28.72s/it]\u001b[A\n",
      " 91%|█████████▏| 3628/3973 [25:45<2:23:37, 24.98s/it]\u001b[A\n",
      " 91%|█████████▏| 3629/3973 [26:11<2:24:45, 25.25s/it]\u001b[A\n",
      " 91%|█████████▏| 3630/3973 [26:22<1:59:26, 20.89s/it]\u001b[A\n",
      " 91%|█████████▏| 3631/3973 [26:49<2:10:24, 22.88s/it]\u001b[A\n",
      " 91%|█████████▏| 3632/3973 [27:20<2:23:04, 25.17s/it]\u001b[A\n",
      " 91%|█████████▏| 3633/3973 [27:34<2:03:44, 21.84s/it]\u001b[A\n",
      " 91%|█████████▏| 3634/3973 [28:22<2:48:41, 29.86s/it]\u001b[A\n",
      " 91%|█████████▏| 3635/3973 [28:47<2:39:44, 28.36s/it]\u001b[A\n",
      " 92%|█████████▏| 3636/3973 [29:03<2:17:23, 24.46s/it]\u001b[A\n",
      " 92%|█████████▏| 3637/3973 [29:30<2:21:49, 25.33s/it]\u001b[A\n",
      " 92%|█████████▏| 3638/3973 [29:42<1:58:40, 21.26s/it]\u001b[A\n",
      " 92%|█████████▏| 3639/3973 [29:50<1:36:00, 17.25s/it]\u001b[A\n",
      " 92%|█████████▏| 3640/3973 [29:51<1:10:10, 12.64s/it]\u001b[A\n",
      " 92%|█████████▏| 3641/3973 [30:01<1:04:23, 11.64s/it]\u001b[A\n",
      " 92%|█████████▏| 3642/3973 [30:03<49:16,  8.93s/it]  \u001b[A\n",
      " 92%|█████████▏| 3643/3973 [30:09<43:35,  7.93s/it]\u001b[A\n",
      " 92%|█████████▏| 3644/3973 [30:14<38:47,  7.07s/it]\u001b[A\n",
      " 92%|█████████▏| 3645/3973 [30:18<33:55,  6.21s/it]\u001b[A\n",
      " 92%|█████████▏| 3646/3973 [30:23<30:42,  5.64s/it]\u001b[A\n",
      " 92%|█████████▏| 3647/3973 [30:26<26:32,  4.89s/it]\u001b[A\n",
      " 92%|█████████▏| 3648/3973 [30:28<22:34,  4.17s/it]\u001b[A\n",
      " 92%|█████████▏| 3649/3973 [30:33<24:09,  4.47s/it]\u001b[A\n",
      " 92%|█████████▏| 3650/3973 [30:35<20:10,  3.75s/it]\u001b[A\n",
      " 92%|█████████▏| 3651/3973 [30:38<18:18,  3.41s/it]\u001b[A\n",
      " 92%|█████████▏| 3652/3973 [30:46<25:01,  4.68s/it]\u001b[A\n",
      " 92%|█████████▏| 3653/3973 [30:47<19:36,  3.68s/it]\u001b[A\n",
      " 92%|█████████▏| 3654/3973 [30:49<16:27,  3.09s/it]\u001b[A\n",
      " 92%|█████████▏| 3655/3973 [30:51<15:20,  2.89s/it]\u001b[A\n",
      " 92%|█████████▏| 3656/3973 [30:54<15:55,  3.01s/it]\u001b[A\n",
      " 92%|█████████▏| 3657/3973 [31:12<38:41,  7.35s/it]\u001b[A\n",
      " 92%|█████████▏| 3658/3973 [31:14<30:12,  5.75s/it]\u001b[A\n",
      " 92%|█████████▏| 3659/3973 [31:17<25:48,  4.93s/it]\u001b[A\n",
      " 92%|█████████▏| 3660/3973 [31:19<21:15,  4.08s/it]\u001b[A\n",
      " 92%|█████████▏| 3661/3973 [31:26<25:13,  4.85s/it]\u001b[A\n",
      " 92%|█████████▏| 3662/3973 [31:28<20:41,  3.99s/it]\u001b[A\n",
      " 92%|█████████▏| 3663/3973 [31:31<19:54,  3.85s/it]\u001b[A\n",
      " 92%|█████████▏| 3664/3973 [31:39<25:14,  4.90s/it]\u001b[A\n",
      " 92%|█████████▏| 3665/3973 [31:40<20:36,  4.01s/it]\u001b[A\n",
      " 92%|█████████▏| 3666/3973 [31:56<37:44,  7.38s/it]\u001b[A\n",
      " 92%|█████████▏| 3667/3973 [32:00<33:31,  6.57s/it]\u001b[A\n",
      " 92%|█████████▏| 3668/3973 [32:01<24:57,  4.91s/it]\u001b[A\n",
      " 92%|█████████▏| 3669/3973 [32:04<21:24,  4.23s/it]\u001b[A\n",
      " 92%|█████████▏| 3670/3973 [32:12<26:12,  5.19s/it]\u001b[A\n",
      " 92%|█████████▏| 3671/3973 [32:17<27:06,  5.39s/it]\u001b[A\n",
      " 92%|█████████▏| 3672/3973 [32:20<23:14,  4.63s/it]\u001b[A\n",
      " 92%|█████████▏| 3673/3973 [32:23<20:32,  4.11s/it]\u001b[A\n",
      " 92%|█████████▏| 3674/3973 [32:28<22:21,  4.49s/it]\u001b[A\n",
      " 92%|█████████▏| 3675/3973 [32:31<19:13,  3.87s/it]\u001b[A\n",
      " 93%|█████████▎| 3676/3973 [32:34<17:46,  3.59s/it]\u001b[A\n",
      " 93%|█████████▎| 3677/3973 [32:38<18:04,  3.66s/it]\u001b[A\n",
      " 93%|█████████▎| 3678/3973 [32:41<16:58,  3.45s/it]\u001b[A\n",
      " 93%|█████████▎| 3679/3973 [32:42<14:22,  2.93s/it]\u001b[A\n",
      " 93%|█████████▎| 3680/3973 [32:50<21:09,  4.33s/it]\u001b[A\n",
      " 93%|█████████▎| 3681/3973 [32:53<19:14,  3.95s/it]\u001b[A\n",
      " 93%|█████████▎| 3682/3973 [32:56<17:28,  3.60s/it]\u001b[A\n",
      " 93%|█████████▎| 3683/3973 [33:00<18:57,  3.92s/it]\u001b[A\n",
      " 93%|█████████▎| 3684/3973 [33:03<16:11,  3.36s/it]\u001b[A\n",
      " 93%|█████████▎| 3685/3973 [33:07<17:16,  3.60s/it]\u001b[A\n",
      " 93%|█████████▎| 3686/3973 [33:09<15:17,  3.20s/it]\u001b[A\n",
      " 93%|█████████▎| 3687/3973 [33:14<18:32,  3.89s/it]\u001b[A\n",
      " 93%|█████████▎| 3688/3973 [33:17<16:38,  3.50s/it]\u001b[A\n",
      " 93%|█████████▎| 3689/3973 [33:22<18:59,  4.01s/it]\u001b[A\n",
      " 93%|█████████▎| 3690/3973 [33:27<20:25,  4.33s/it]\u001b[A\n",
      " 93%|█████████▎| 3691/3973 [33:29<16:39,  3.55s/it]\u001b[A\n",
      " 93%|█████████▎| 3692/3973 [33:32<15:33,  3.32s/it]\u001b[A\n",
      " 93%|█████████▎| 3693/3973 [33:34<14:09,  3.03s/it]\u001b[A\n",
      " 93%|█████████▎| 3694/3973 [33:36<12:15,  2.64s/it]\u001b[A\n",
      " 93%|█████████▎| 3695/3973 [33:41<16:13,  3.50s/it]\u001b[A\n",
      " 93%|█████████▎| 3696/3973 [33:50<23:06,  5.01s/it]\u001b[A\n",
      " 93%|█████████▎| 3697/3973 [34:00<29:35,  6.43s/it]\u001b[A\n",
      " 93%|█████████▎| 3698/3973 [34:05<27:53,  6.08s/it]\u001b[A\n",
      " 93%|█████████▎| 3699/3973 [34:07<22:19,  4.89s/it]\u001b[A\n",
      " 93%|█████████▎| 3700/3973 [34:10<19:20,  4.25s/it]\u001b[A\n",
      " 93%|█████████▎| 3701/3973 [34:14<18:45,  4.14s/it]\u001b[A\n",
      " 93%|█████████▎| 3702/3973 [34:17<17:25,  3.86s/it]\u001b[A\n",
      " 93%|█████████▎| 3703/3973 [34:22<18:43,  4.16s/it]\u001b[A\n",
      " 93%|█████████▎| 3704/3973 [34:30<23:57,  5.34s/it]\u001b[A\n",
      " 93%|█████████▎| 3705/3973 [34:33<20:47,  4.65s/it]\u001b[A\n",
      " 93%|█████████▎| 3706/3973 [34:48<35:01,  7.87s/it]\u001b[A\n",
      " 93%|█████████▎| 3707/3973 [34:51<27:46,  6.27s/it]\u001b[A\n",
      " 93%|█████████▎| 3708/3973 [34:53<21:54,  4.96s/it]\u001b[A\n",
      " 93%|█████████▎| 3709/3973 [34:54<16:21,  3.72s/it]\u001b[A\n",
      " 93%|█████████▎| 3710/3973 [34:56<14:38,  3.34s/it]\u001b[A\n",
      " 93%|█████████▎| 3711/3973 [35:00<15:50,  3.63s/it]\u001b[A\n",
      " 93%|█████████▎| 3712/3973 [35:02<12:48,  2.95s/it]\u001b[A\n",
      " 93%|█████████▎| 3713/3973 [35:11<20:46,  4.80s/it]\u001b[A\n",
      " 93%|█████████▎| 3714/3973 [35:22<28:29,  6.60s/it]\u001b[A\n",
      " 94%|█████████▎| 3715/3973 [35:44<48:21, 11.25s/it]\u001b[A\n",
      " 94%|█████████▎| 3716/3973 [35:47<37:25,  8.74s/it]\u001b[A\n",
      " 94%|█████████▎| 3717/3973 [35:49<29:07,  6.83s/it]\u001b[A\n",
      " 94%|█████████▎| 3718/3973 [35:54<26:27,  6.22s/it]\u001b[A\n",
      " 94%|█████████▎| 3719/3973 [35:56<21:44,  5.14s/it]\u001b[A\n",
      " 94%|█████████▎| 3720/3973 [36:02<21:55,  5.20s/it]\u001b[A\n",
      " 94%|█████████▎| 3721/3973 [36:04<18:31,  4.41s/it]\u001b[A\n",
      " 94%|█████████▎| 3722/3973 [36:07<15:43,  3.76s/it]\u001b[A\n",
      " 94%|█████████▎| 3723/3973 [36:11<16:23,  3.94s/it]\u001b[A\n",
      " 94%|█████████▎| 3724/3973 [36:18<20:05,  4.84s/it]\u001b[A\n",
      " 94%|█████████▍| 3725/3973 [36:21<17:36,  4.26s/it]\u001b[A\n",
      " 94%|█████████▍| 3726/3973 [36:22<14:16,  3.47s/it]\u001b[A\n",
      " 94%|█████████▍| 3727/3973 [36:25<13:43,  3.35s/it]\u001b[A\n",
      " 94%|█████████▍| 3728/3973 [36:30<14:54,  3.65s/it]\u001b[A\n",
      " 94%|█████████▍| 3729/3973 [36:32<12:54,  3.18s/it]\u001b[A\n",
      " 94%|█████████▍| 3730/3973 [36:35<12:44,  3.15s/it]\u001b[A\n",
      " 94%|█████████▍| 3731/3973 [36:38<12:33,  3.11s/it]\u001b[A\n",
      " 94%|█████████▍| 3732/3973 [36:47<19:13,  4.78s/it]\u001b[A\n",
      " 94%|█████████▍| 3733/3973 [36:49<16:10,  4.05s/it]\u001b[A\n",
      " 94%|█████████▍| 3734/3973 [36:52<14:25,  3.62s/it]\u001b[A\n",
      " 94%|█████████▍| 3735/3973 [37:00<20:28,  5.16s/it]\u001b[A\n",
      " 94%|█████████▍| 3736/3973 [37:02<16:20,  4.14s/it]\u001b[A\n",
      " 94%|█████████▍| 3737/3973 [37:04<13:46,  3.50s/it]\u001b[A\n",
      " 94%|█████████▍| 3738/3973 [37:05<11:10,  2.86s/it]\u001b[A\n",
      " 94%|█████████▍| 3739/3973 [37:08<11:11,  2.87s/it]\u001b[A\n",
      " 94%|█████████▍| 3740/3973 [37:13<12:48,  3.30s/it]\u001b[A\n",
      " 94%|█████████▍| 3741/3973 [37:15<12:02,  3.12s/it]\u001b[A\n",
      " 94%|█████████▍| 3742/3973 [37:16<09:38,  2.51s/it]\u001b[A\n",
      " 94%|█████████▍| 3743/3973 [37:19<09:49,  2.56s/it]\u001b[A\n",
      " 94%|█████████▍| 3744/3973 [37:22<10:17,  2.69s/it]\u001b[A\n",
      " 94%|█████████▍| 3745/3973 [37:25<10:09,  2.67s/it]\u001b[A\n",
      " 94%|█████████▍| 3746/3973 [37:26<08:58,  2.37s/it]\u001b[A\n",
      " 94%|█████████▍| 3747/3973 [37:36<17:19,  4.60s/it]\u001b[A\n",
      " 94%|█████████▍| 3748/3973 [37:37<13:19,  3.55s/it]\u001b[A\n",
      " 94%|█████████▍| 3749/3973 [37:40<11:58,  3.21s/it]\u001b[A\n",
      " 94%|█████████▍| 3750/3973 [37:42<10:26,  2.81s/it]\u001b[A\n",
      " 94%|█████████▍| 3751/3973 [37:44<09:23,  2.54s/it]\u001b[A\n",
      " 94%|█████████▍| 3752/3973 [37:46<08:43,  2.37s/it]\u001b[A\n",
      " 94%|█████████▍| 3753/3973 [37:57<18:52,  5.15s/it]\u001b[A\n",
      " 94%|█████████▍| 3754/3973 [38:03<20:06,  5.51s/it]\u001b[A\n",
      " 95%|█████████▍| 3755/3973 [38:06<17:07,  4.71s/it]\u001b[A\n",
      " 95%|█████████▍| 3756/3973 [38:09<14:59,  4.14s/it]\u001b[A\n",
      " 95%|█████████▍| 3757/3973 [38:12<13:25,  3.73s/it]\u001b[A\n",
      " 95%|█████████▍| 3758/3973 [38:15<12:35,  3.51s/it]\u001b[A\n",
      " 95%|█████████▍| 3759/3973 [38:17<10:58,  3.08s/it]\u001b[A\n",
      " 95%|█████████▍| 3760/3973 [38:20<10:29,  2.95s/it]\u001b[A\n",
      " 95%|█████████▍| 3761/3973 [38:21<09:12,  2.60s/it]\u001b[A\n",
      " 95%|█████████▍| 3762/3973 [38:23<08:29,  2.42s/it]\u001b[A\n",
      " 95%|█████████▍| 3763/3973 [38:28<10:18,  2.95s/it]\u001b[A\n",
      " 95%|█████████▍| 3764/3973 [38:31<10:23,  2.99s/it]\u001b[A\n",
      " 95%|█████████▍| 3765/3973 [38:40<16:59,  4.90s/it]\u001b[A\n",
      " 95%|█████████▍| 3766/3973 [38:41<13:15,  3.84s/it]\u001b[A\n",
      " 95%|█████████▍| 3767/3973 [38:43<10:58,  3.20s/it]\u001b[A\n",
      " 95%|█████████▍| 3768/3973 [38:45<09:29,  2.78s/it]\u001b[A\n",
      " 95%|█████████▍| 3769/3973 [38:50<11:38,  3.43s/it]\u001b[A\n",
      " 95%|█████████▍| 3770/3973 [38:53<11:11,  3.31s/it]\u001b[A\n",
      " 95%|█████████▍| 3771/3973 [38:55<10:01,  2.98s/it]\u001b[A\n",
      " 95%|█████████▍| 3772/3973 [38:57<09:14,  2.76s/it]\u001b[A\n",
      " 95%|█████████▍| 3773/3973 [39:01<10:02,  3.01s/it]\u001b[A\n",
      " 95%|█████████▍| 3774/3973 [39:05<10:47,  3.26s/it]\u001b[A\n",
      " 95%|█████████▌| 3775/3973 [39:15<02:03,  1.60it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of 2T: 0.8744022149509187\n",
      "Measure Reciprocal Rank of 2T: 0.8379534356908968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## QUERY CORRETTA CON FUZZYYYY\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "        #print(f\"{name}: {GT_id_match[0]} NOT FOUND in {value[1]}\")\n",
    "        #print(\"___________________________\")\n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:            \n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of 2T: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query SOFT FILTERING\n",
    "Coverage of 2T: 0.8733954190787818\n",
    "\n",
    "Measure Reciprocal Rank of 2T: 0.841805940095622\n",
    "\n",
    "## Query HARD FILTERING\n",
    "Coverage of 2T: 0.8744022149509187\n",
    "\n",
    "Measure Reciprocal Rank of 2T: 0.83795343569089682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "#json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "json_file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Round4_sorted_mentions.json\"\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_sorted_mentions[:q1_idx]\n",
    "q2 = R4_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R4_sample_keys = []\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q1, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q2, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q3, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) | set(edInst_subclass) | set(govAgency_subclass) | set(intOrg_subclass))\n",
    "    geolocation_subclass = list(set(geolocation_subclass) | set(country_subclass) | set(city_subclass) | set(capitals_subclass) | set(admTerr_subclass))\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  29%|██▉       | 9377/31922 [07:06<14:49, 25.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q820655: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  29%|██▉       | 9403/31922 [07:09<33:30, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q838948: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9436/31922 [07:10<22:27, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q81935689: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9465/31922 [07:11<13:31, 27.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q842478: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9475/31922 [07:12<22:58, 16.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q845945: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9487/31922 [07:14<26:31, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q846662: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9493/31922 [07:14<31:14, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q846837: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9496/31922 [07:15<40:28,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q28060193: HTTP Error 429: Too Many Requests\n",
      "Error processing Q847017: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9520/31922 [07:18<44:15,  8.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q848330: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9527/31922 [07:19<56:40,  6.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q849706: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9541/31922 [07:21<50:40,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q851830: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9543/31922 [07:22<1:19:07,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q852231: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9550/31922 [07:23<54:39,  6.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q853854: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9560/31922 [07:24<45:44,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q193512: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|██▉       | 9563/31922 [07:25<58:25,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q191893: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9594/31922 [07:26<18:25, 20.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q856234: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9599/31922 [07:27<27:43, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q856713: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9610/31922 [07:27<26:17, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q858157: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9661/31922 [07:29<12:00, 30.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q860861: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9677/31922 [07:30<18:28, 20.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q861951: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9697/31922 [07:31<16:00, 23.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q862597: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9705/31922 [07:32<20:09, 18.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q863454: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9710/31922 [07:33<31:07, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q865493: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9713/31922 [07:33<40:29,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q865588: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9717/31922 [07:34<46:24,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q867143: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  30%|███       | 9723/31922 [07:35<51:44,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q868291: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9746/31922 [07:36<25:23, 14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q875157: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9752/31922 [07:37<31:04, 11.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q875538: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9757/31922 [07:38<40:25,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q877886: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9759/31922 [07:39<1:02:32,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q878123: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9767/31922 [07:40<49:39,  7.44it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q878130: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9769/31922 [07:41<1:04:36,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q878223: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9815/31922 [07:42<15:08, 24.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q891723: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9847/31922 [07:44<17:32, 20.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q894571: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9850/31922 [07:45<33:04, 11.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1076486: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9853/31922 [07:46<43:01,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q894986: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9868/31922 [07:47<27:51, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q19335303: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9874/31922 [07:48<39:32,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q898771: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9884/31922 [07:49<33:30, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q899192: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9899/31922 [07:50<29:35, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q899409: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9939/31922 [07:52<19:26, 18.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q907698: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███       | 9962/31922 [07:53<16:04, 22.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q912142: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███▏      | 9979/31922 [07:55<29:02, 12.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q917182: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  31%|███▏      | 10008/31922 [07:58<28:40, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q25377652: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  32%|███▏      | 10088/31922 [08:00<09:29, 38.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q936076: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  32%|███▏      | 10135/31922 [08:03<17:18, 20.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q950431: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  32%|███▏      | 10180/31922 [08:04<09:46, 37.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q954501: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  32%|███▏      | 10241/31922 [08:08<19:45, 18.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q967098: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  32%|███▏      | 10282/31922 [08:10<12:00, 30.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q976622: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10407/31922 [08:16<19:11, 18.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1040689: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10409/31922 [08:17<37:46,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1043939: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10488/31922 [08:21<30:12, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1059564: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10546/31922 [08:24<15:57, 22.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1065252: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10551/31922 [08:25<35:04, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1066997: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10602/31922 [08:28<14:20, 24.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1074523: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10659/31922 [08:30<14:02, 25.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1088652: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  33%|███▎      | 10681/31922 [08:32<13:58, 25.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q1092939: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▎  | 23539/31922 [17:13<09:40, 14.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q55687066: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23637/31922 [17:18<06:24, 21.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q63998451: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23640/31922 [17:19<11:44, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q64027488: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23696/31922 [17:21<06:25, 21.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q61702557: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23716/31922 [17:22<06:34, 20.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q61855877: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23719/31922 [17:23<10:19, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q62008942: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23729/31922 [17:24<10:05, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q62078547: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23739/31922 [17:25<10:23, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q58863414: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23772/31922 [17:26<05:38, 24.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q63100559: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  74%|███████▍  | 23778/31922 [17:27<07:55, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q63100584: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  75%|███████▍  | 23821/31922 [17:28<05:58, 22.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q63141557: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  75%|███████▍  | 23840/31922 [17:30<06:26, 20.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q65661087: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  75%|███████▍  | 23843/31922 [17:30<10:26, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q65963104: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs:  75%|███████▍  | 23871/31922 [17:32<08:01, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Q66619774: HTTP Error 429: Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs: 100%|██████████| 31922/31922 [18:21<00:00, 28.98it/s]  \n"
     ]
    }
   ],
   "source": [
    "#tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "#cta_file = './data/Dataset/Dataset/Round4_2020/gt/cta.csv'\n",
    "\n",
    "tables = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI//data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cta_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/Round4_2020/gt/cta.csv'\n",
    "\n",
    "\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    id_to_root_class = {}\n",
    "    \n",
    "    for el in tqdm(id_list, desc=\"Processing IDs\"):\n",
    "        if el not in id_to_root_class:\n",
    "            query = f\"\"\"\n",
    "            SELECT ?instanceClass ?instanceClassLabel WHERE {{\n",
    "              wd:{el} wdt:P31 ?instanceClass .\n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\" }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Set the query and request JSON response\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            #time.sleep(0.5)\n",
    "            \n",
    "            try:\n",
    "                results = sparql.query().convert()\n",
    "                if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "                    inst_item = int(results[\"results\"][\"bindings\"][0]['instanceClassLabel']['value'][1:])\n",
    "                    if inst_item in geolocation_subclass:\n",
    "                        id_to_root_class[el] = \"LOC\"\n",
    "                    elif inst_item in organization_subclass:\n",
    "                        id_to_root_class[el] = \"ORG\"\n",
    "                    elif inst_item == 5 or el == \"Q5\":\n",
    "                        id_to_root_class[el] = \"PERS\"\n",
    "                    else:\n",
    "                        id_to_root_class[el] = \"OTHERS\"\n",
    "                else:\n",
    "                    id_to_root_class[el] = \"None\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {el}: {e}\")\n",
    "                time.sleep(0.5)\n",
    "                id_to_root_class[el] = \"None\"          \n",
    "    \n",
    "    return id_to_root_class\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "ids = [url.split('/')[-1] for url in df[2]]\n",
    "\n",
    "root_classes = get_item_root(ids)\n",
    "\n",
    "# Map root classes to categories\n",
    "root_categories = []\n",
    "for el in ids:\n",
    "    try:\n",
    "        root_categories.append(root_classes[el])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22207/22207 [2:09:23<00:00,  2.86it/s]  \n"
     ]
    }
   ],
   "source": [
    "# probably this is the NERtype computation\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                if cta_keys[\"key\"][1].iloc[tmp_index] != \"None\":\n",
    "                    key_to_cell[f\"{table_name} {col}\"] = tmp_value\n",
    "                #print(f\"key: {key} -> key_to_cell[key]: {tmp_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13580"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_to_cell.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted mentions saved to ./data/R4_ner_type_new.json\n"
     ]
    }
   ],
   "source": [
    "json_file_path = \"./data/R4_ner_type_new.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(key_to_cell, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tables_path =  \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = (df[\"key\"].values, df[3])\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys[0]:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                print(f\"cell_value: {cell_value}, NERtype: {cea_keys[1][cea_keys[0] == key]}\")\n",
    "                ner_type[key] = (cell_value, cea_keys[1][cea_keys[0] == key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "with open('./R4_key_to_cell.json', 'w') as json_file:\n",
    "    json.dump(key_to_cell, json_file, indent=4,  cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Open and read the JSON file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 8\u001b[0m     ner_type \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/R4_key_to_cell.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     key_to_cell \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json\"\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)\n",
    "\n",
    "# Now key_to_cell contains the dictionary loaded from the JSON file\n",
    "print(\"Dictionary loaded from JSON file:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in case you want R4\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json', 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/475897 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9863/475897 [00:00<00:19, 24299.49it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "\n",
    "    if value is not None:\n",
    "        # Hard filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}},\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }    \n",
    "\n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        if NER_type is None:\n",
    "            print(f\"q_ids: {q_ids}, ner_type key: {new_key}\")\n",
    "        query = get_query(name, NER_type)\n",
    "\n",
    "\n",
    "        data = json.loads(query['query'])\n",
    "        queries.append((query, q_ids[0]))\n",
    "        if len(queries) == 4000:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 93>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries))\n\u001b[1;32m---> 97\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# For environments like Jupyter\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\nest_asyncio.py:83\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     81\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\site-packages\\nest_asyncio.py:106\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m     heappop(scheduled)\n\u001b[0;32m    101\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    104\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 106\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[0;32m    109\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\selectors.py:324\u001b[0m, in \u001b[0;36mSelectSelector.select\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     r, w, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "File \u001b[1;32mc:\\Users\\abelo\\anaconda3\\envs\\v_env\\lib\\selectors.py:315\u001b[0m, in \u001b[0;36mSelectSelector._select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 315\u001b[0m     r, w, x \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w \u001b[38;5;241m+\u001b[39m x, []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## QUERY CORRETTA CON FUZZYYYY\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "sample_size = 4000\n",
    "#queries = random.sample(queries, sample_size)\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, ssl=False, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "        \n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R4: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R4: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query\n",
    "Coverage of R4: 0.94025\n",
    "\n",
    "Measure Reciprocal Rank of R4: 0.9150119999999621"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardTableR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/HardTablesR3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HT3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(HT3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = HT3_sorted_mentions[:q1_idx]\n",
    "q2 = HT3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = HT3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = HT3_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "HT3_sample_keys = []\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q1, sample_size)\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q2, sample_size)\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q3, sample_size)\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in HT3_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100%|██████████| 7207/7207 [01:04<00:00, 111.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tables_path = \"./data/Dataset/Dataset/HardTablesR3/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR3/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "pattern = r'^\\.'\n",
    "\n",
    "# Create a list of file paths, excluding files that start with a dot\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10779/10779 [02:37<00:00, 68.36it/s]  \n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/HardTablesR3/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR3/gt/cea.csv'\n",
    "cta_file = './data/Dataset/Dataset/HardTablesR3/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    pattern = r'^\\.'\n",
    "    if re.match(pattern, table):\n",
    "        continue\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58949/58949 [00:01<00:00, 30698.13it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    if value is not None:\n",
    "        ### SOFT FILTERING CONSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''\n",
    "                {{\n",
    "                    \"query\": {{\n",
    "                        \"bool\": {{\n",
    "                            \"must\": [\n",
    "                                {{\n",
    "                                    \"match\": {{\n",
    "                                        \"name\": {{\n",
    "                                            \"query\": \"{name}\",\n",
    "                                            \"boost\": 2.0\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }}\n",
    "                            ],\n",
    "                            \"should\": [\n",
    "                                {{\n",
    "                                    \"term\": {{\n",
    "                                        \"NERtype\": \"{value[1]}\"\n",
    "                                    }}\n",
    "                                }}\n",
    "                            ]\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                ''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "        ### HARD FILTERING CONSTRAINT\n",
    "        #params = {\n",
    "        #    'name': name,\n",
    "        #    'token': 'lamapi_demo_2023',\n",
    "        #    'kg': 'wikidata',\n",
    "        #    'limit': 1000,\n",
    "        #    'query': f'''\n",
    "        #        {{\n",
    "        #            \"query\": {{\n",
    "        #                \"bool\": {{\n",
    "        #                    \"must\": [\n",
    "        #                        {{\n",
    "        #                            \"match\": {{\n",
    "        #                                \"name\": {{\n",
    "        #                                    \"query\": \"{name}\",\n",
    "        #                                    \"boost\": 2.0\n",
    "        #                                }}\n",
    "        #                            }}\n",
    "        #                        }},\n",
    "        #                        {{\n",
    "        #                            \"term\": {{\n",
    "        #                                \"NERtype\": \"{value[1]}\"\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ]\n",
    "        #                }}\n",
    "        #            }}\n",
    "        #        }}\n",
    "        #        ''',\n",
    "        #    'sort': [\n",
    "        #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        #    ]\n",
    "        #}\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_id = key_to_cell[key][1]\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        match = re.search(r'Q(\\d+)$', q_id)\n",
    "        if match:\n",
    "            queries.append((query, match[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 917/4000 [05:24<13:58,  3.67it/s]  2024-11-12 15:13:37,201 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      " 60%|██████    | 2401/4000 [13:32<12:33,  2.12it/s]2024-11-12 15:21:46,107 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      " 78%|███████▊  | 3124/4000 [17:26<02:57,  4.94it/s]2024-11-12 15:25:39,077 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      " 82%|████████▏ | 3273/4000 [18:16<09:09,  1.32it/s]2024-11-12 15:26:29,083 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      " 96%|█████████▌| 3838/4000 [21:16<08:55,  3.31s/it]2024-11-12 15:29:29,157 - ERROR - Giving up fetch(...) after 2 tries (TimeoutError)\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries))\n\u001b[0;32m--> 116\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# For environments like Jupyter\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[24], line 80\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(queries, url, pbar)\u001b[0m\n\u001b[1;32m     77\u001b[0m     param \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mint\u001b[39m(v) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (np\u001b[38;5;241m.\u001b[39mint64, np\u001b[38;5;241m.\u001b[39mint32)) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m param\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     78\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(process_item(session, url, \u001b[38;5;28mid\u001b[39m, headers, param, semaphore, pbar))\n\u001b[0;32m---> 80\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuzzy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (mrr_increment, count), (param, \u001b[38;5;28mid\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, queries):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 44\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(session, url, id, headers, params, semaphore, pbar)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_item\u001b[39m(session, url, \u001b[38;5;28mid\u001b[39m, headers, params, semaphore, pbar):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fetch(session, url, params, headers, semaphore)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ClientResponseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/backoff/_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    148\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    153\u001b[0m     giveup_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m giveup(e)\n",
      "Cell \u001b[0;32mIn[24], line 35\u001b[0m, in \u001b[0;36mfetch\u001b[0;34m(session, url, params, headers, semaphore)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@backoff\u001b[39m\u001b[38;5;241m.\u001b[39mon_exception(\n\u001b[1;32m     28\u001b[0m     backoff\u001b[38;5;241m.\u001b[39mexpo, \n\u001b[1;32m     29\u001b[0m     (aiohttp\u001b[38;5;241m.\u001b[39mClientError, aiohttp\u001b[38;5;241m.\u001b[39mhttp_exceptions\u001b[38;5;241m.\u001b[39mHttpProcessingError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(session, url, params, headers, semaphore):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m                 response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raises an exception for 4XX/5XX status codes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:1167\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:586\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    584\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client_reqrep.py:900\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m connection\n\u001b[0;32m--> 900\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# read response\u001b[39;49;00m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/helpers.py:725\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import backoff\n",
    "from tqdm import tqdm\n",
    "from aiohttp import ClientResponseError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# URL and sample size\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "sample_size = 4000\n",
    "\n",
    "# Generate sample queries\n",
    "queries = random.sample(queries, sample_size)\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{params}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:\n",
    "            # Convert numpy int64 to standard Python int\n",
    "            param = {k: int(v) if isinstance(v, (np.int64, np.int32)) else v for k, v in param.items()}\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(\"fuzzy\")\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"should\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of HT2: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of HT2: {m_mrr / len(queries)}\")\n",
    "\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./data/HT3_ner_type.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(ner_type, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# HardTableR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/HardTablesR2_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HT2_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(HT2_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = HT2_sorted_mentions[:q1_idx]\n",
    "q2 = HT2_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = HT2_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = HT2_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "HT2_sample_keys = []\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q1, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q2, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q3, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in HT2_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables_path = \"./data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "pattern = r'^\\.'\n",
    "\n",
    "# Create a list of file paths, excluding files that start with a dot\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "cta_file = './data/Dataset/Dataset/HardTablesR2/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    pattern = r'^\\.'\n",
    "    if re.match(pattern, table):\n",
    "        continue\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(name, value):\n",
    "    if value is not None:\n",
    "          ### SOFT FILTERING CONTSTRAINT\n",
    "        #params = {\n",
    "        #    'name': name,\n",
    "        #    'token': 'lamapi_demo_2023',\n",
    "        #    'kg': 'wikidata',\n",
    "        #    'limit': 1000,\n",
    "        #    'query': f'''\n",
    "        #        {{\n",
    "        #            \"query\": {{\n",
    "        #                \"bool\": {{\n",
    "        #                    \"must\": [\n",
    "        #                        {{\n",
    "        #                            \"match\": {{\n",
    "        #                                \"name\": {{\n",
    "        #                                    \"query\": \"{name}\",\n",
    "        #                                    \"boost\": 2.0\n",
    "        #                                }}\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ],\n",
    "        #                    \"should\": [\n",
    "        #                        {{\n",
    "        #                            \"term\": {{\n",
    "        #                                \"NERtype\": \"{value[1]}\"\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ]\n",
    "        #                }}\n",
    "        #            }}\n",
    "        #        }}\n",
    "        #        ''',\n",
    "        #    'sort': [\n",
    "        #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        #    ]\n",
    "        #}\n",
    "    \n",
    "        ### HARD FILTERING CONTSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''\n",
    "                {{\n",
    "                    \"query\": {{\n",
    "                        \"bool\": {{\n",
    "                            \"must\": [\n",
    "                                {{\n",
    "                                    \"match\": {{\n",
    "                                        \"name\": {{\n",
    "                                            \"query\": \"{name}\",\n",
    "                                            \"boost\": 2.0\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }},\n",
    "                                {{\n",
    "                                    \"term\": {{\n",
    "                                        \"NERtype\": \"{value[1]}\"\n",
    "                                    }}\n",
    "                                }}\n",
    "                            ]\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                ''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_id = key_to_cell[key][1]\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        match = re.search(r'Q(\\d+)$', q_id)\n",
    "        if match:\n",
    "            queries.append((query, match[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import backoff\n",
    "from tqdm import tqdm\n",
    "from aiohttp import ClientResponseError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# URL and sample size\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "sample_size = 4000\n",
    "\n",
    "# Generate sample queries\n",
    "queries = random.sample(queries, sample_size)\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{params}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:\n",
    "            # Convert numpy int64 to standard Python int\n",
    "            param = {k: int(v) if isinstance(v, (np.int64, np.int32)) else v for k, v in param.items()}\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(\"fuzzy\")\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"should\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of HT2: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of HT2: {m_mrr / len(queries)}\")\n",
    "\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
