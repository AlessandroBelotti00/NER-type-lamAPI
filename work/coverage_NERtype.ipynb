{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n",
      "Collecting SPARQLWrapper\n",
      "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
      "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
      "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: isodate, rdflib, SPARQLWrapper\n",
      "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-7.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install backoff\n",
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import requests\n",
    "from aiohttp import ClientResponseError\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/64 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 2/64 [00:00<00:03, 16.15it/s]\u001b[A\n",
      "  6%|▋         | 4/64 [00:00<00:04, 12.98it/s]\u001b[A\n",
      " 12%|█▎        | 8/64 [00:00<00:02, 20.10it/s]\u001b[A\n",
      " 17%|█▋        | 11/64 [00:00<00:02, 21.34it/s]\u001b[A\n",
      " 22%|██▏       | 14/64 [00:00<00:02, 19.90it/s]\u001b[A\n",
      " 27%|██▋       | 17/64 [00:00<00:02, 16.28it/s]\u001b[A\n",
      " 31%|███▏      | 20/64 [00:01<00:02, 18.23it/s]\u001b[A\n",
      " 36%|███▌      | 23/64 [00:01<00:01, 20.79it/s]\u001b[A\n",
      " 41%|████      | 26/64 [00:01<00:01, 20.67it/s]\u001b[A\n",
      " 45%|████▌     | 29/64 [00:01<00:01, 20.37it/s]\u001b[A\n",
      " 50%|█████     | 32/64 [00:01<00:01, 20.33it/s]\u001b[A\n",
      " 55%|█████▍    | 35/64 [00:01<00:01, 22.17it/s]\u001b[A\n",
      " 59%|█████▉    | 38/64 [00:01<00:01, 22.77it/s]\u001b[A\n",
      " 64%|██████▍   | 41/64 [00:02<00:01, 21.74it/s]\u001b[A\n",
      " 69%|██████▉   | 44/64 [00:02<00:01, 19.70it/s]\u001b[A\n",
      " 73%|███████▎  | 47/64 [00:02<00:00, 18.93it/s]\u001b[A\n",
      " 77%|███████▋  | 49/64 [00:02<00:00, 17.69it/s]\u001b[A\n",
      " 83%|████████▎ | 53/64 [00:02<00:00, 21.33it/s]\u001b[A\n",
      " 89%|████████▉ | 57/64 [00:02<00:00, 24.01it/s]\u001b[A\n",
      " 94%|█████████▍| 60/64 [00:02<00:00, 20.59it/s]\u001b[A\n",
      "100%|██████████| 64/64 [00:03<00:00, 20.29it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PERS\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'q_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m chunk_cea\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     17\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_to_cell\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[43mq_ids\u001b[49m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     19\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m         data \u001b[38;5;241m=\u001b[39m key_to_cell[key]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q_ids' is not defined"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "count = 0\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            count += 1\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    ### SOFT FILTERING CONTSTRAINT\n",
    "    #params = {\n",
    "    #    'name': name,\n",
    "    #    'token': 'lamapi_demo_2023',\n",
    "    #    'kg': 'wikidata',\n",
    "    #    'limit': 1000,\n",
    "    #    'query': f'''\n",
    "    #        {{\n",
    "    #            \"query\": {{\n",
    "    #                \"bool\": {{\n",
    "    #                    \"must\": [\n",
    "    #                        {{\n",
    "    #                            \"match\": {{\n",
    "    #                                \"name\": {{\n",
    "    #                                    \"query\": \"{name}\",\n",
    "    #                                    \"boost\": 2.0\n",
    "    #                                }}\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ],\n",
    "    #                    \"should\": [\n",
    "    #                        {{\n",
    "    #                            \"term\": {{\n",
    "    #                                \"NERtype\": \"{value[1]}\"\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ]\n",
    "    #                }}\n",
    "    #            }}\n",
    "    #        }}\n",
    "    #        ''',\n",
    "    #    'sort': [\n",
    "    #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "    #    ]\n",
    "    #}\n",
    "\n",
    "    ### HARD FILTERING CONTSTRAINT\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }},\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        pbar.update(1)  # No need to await here\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        print(f\"{name}: {GT_id_match[0]} NOT FOUND in {value[1]}\")\n",
    "        print(\"___________________________\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id[0])[0]\n",
    "\n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the soft filtering\n",
    "Coverage of R1: 0.9836745270795543\n",
    "\n",
    "Measure Reciprocal Rank of R1: 0.9616820419797453\n",
    "\n",
    "## Coverage with the hard filtering\n",
    "Coverage of R1: 0.8067357512953368\n",
    "\n",
    "Measure Reciprocal Rank of 13: 0.96043575129529763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R3_sorted_mentions[:q1_idx]\n",
    "q2 = R3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R3_sorted_mentions[q3_idx:]\n",
    "\n",
    "\n",
    "sample_size = 1000 \n",
    "R3_sample_keys = []\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q1, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q2, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q3, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R3_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round3_2019/gt/CTA_Round3_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1]            \n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PER\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = category_list\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    ### SOFT FILTERING CONTSTRAINT\n",
    "    #params = {\n",
    "    #    'name': name,\n",
    "    #    'token': 'lamapi_demo_2023',\n",
    "    #    'kg': 'wikidata',\n",
    "    #    'limit': 1000,\n",
    "    #    'query': f'''\n",
    "    #        {{\n",
    "    #            \"query\": {{\n",
    "    #                \"bool\": {{\n",
    "    #                    \"must\": [\n",
    "    #                        {{\n",
    "    #                            \"match\": {{\n",
    "    #                                \"name\": {{\n",
    "    #                                    \"query\": \"{name}\",\n",
    "    #                                    \"boost\": 2.0\n",
    "    #                                }}\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ],\n",
    "    #                    \"should\": [\n",
    "    #                        {{\n",
    "    #                            \"term\": {{\n",
    "    #                                \"NERtype\": \"{value[1]}\"\n",
    "    #                            }}\n",
    "    #                        }}\n",
    "    #                    ]\n",
    "    #                }}\n",
    "    #            }}\n",
    "    #        }}\n",
    "    #        ''',\n",
    "    #    'sort': [\n",
    "    #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "    #    ]\n",
    "    #}\n",
    "\n",
    "    ### HARD FILTERING CONTSTRAINT\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }},\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            ''',\n",
    "        'sort': [\n",
    "            f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (name, url_id) in zip(results, string_name_list.items()):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                params = {\n",
    "                    'name': name,\n",
    "                    'token': 'lamapi_demo_2023',\n",
    "                    'kg': 'wikidata',\n",
    "                    'limit': 1000,\n",
    "                    'query':  f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}''',\n",
    "                    'sort': [\n",
    "                        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "                    ]\n",
    "                }\n",
    "                id = re.search(r'Q(\\d+)$', url_id[0])[0]\n",
    "                \n",
    "                response = requests.get(url, params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R3: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R3: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the soft filtering\n",
    "Coverage of R3: 0.9634817408704353\n",
    "\n",
    "Measure Reciprocal Rank of R3: 0.9472711355677341\n",
    "\n",
    "## Coverage with the hard filtering\n",
    "Coverage of R3: 0.5406758448060075\n",
    "\n",
    "Measure Reciprocal Rank of R3: 0.96075719649556936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100%|██████████| 180/180 [00:37<00:00,  4.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tables_path = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:51<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "cta_file = './data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 194438/194438 [00:04<00:00, 43766.14it/s] \n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    if value is not None:\n",
    "          ### SOFT FILTERING CONTSTRAINT\n",
    "        #params = {\n",
    "        #    'name': name,\n",
    "        #    'token': 'lamapi_demo_2023',\n",
    "        #    'kg': 'wikidata',\n",
    "        #    'limit': 1000,\n",
    "        #    'query': f'''\n",
    "        #        {{\n",
    "        #            \"query\": {{\n",
    "        #                \"bool\": {{\n",
    "        #                    \"must\": [\n",
    "        #                        {{\n",
    "        #                            \"match\": {{\n",
    "        #                                \"name\": {{\n",
    "        #                                    \"query\": \"{name}\",\n",
    "        #                                    \"boost\": 2.0\n",
    "        #                                }}\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ],\n",
    "        #                    \"should\": [\n",
    "        #                        {{\n",
    "        #                            \"term\": {{\n",
    "        #                                \"NERtype\": \"{value[1]}\"\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ]\n",
    "        #                }}\n",
    "        #            }}\n",
    "        #        }}\n",
    "        #        ''',\n",
    "        #    'sort': [\n",
    "        #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        #    ]\n",
    "        #}\n",
    "    \n",
    "        ### HARD FILTERING CONTSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}, {{\"term\": {{\"NERtype\": \"{value}\"}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_id = key_to_cell[key][1]\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        match = re.search(r'Q(\\d+)$', q_id)\n",
    "        if match:\n",
    "            queries.append((query, match[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 60/4000 [00:50<2:02:05,  1.86s/it]2024-08-02 11:32:34,280 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "  2%|▏         | 71/4000 [00:58<38:08,  1.72it/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries))\n\u001b[0;32m---> 98\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# For environments like Jupyter\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:116\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m     heappop(scheduled)\n\u001b[1;32m    111\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    114\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    119\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## QUERY CORRETTA CON FUZZYYYY\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "sample_size = 4000\n",
    "queries = random.sample(queries, sample_size)\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "        #print(f\"{name}: {GT_id_match[0]} NOT FOUND in {value[1]}\")\n",
    "        #print(\"___________________________\")\n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:            \n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of 2T: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query\n",
    "Coverage of R4: 0.48625\n",
    "\n",
    "Measure Reciprocal Rank of R4: 0.55204350000000288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_sorted_mentions[:q1_idx]\n",
    "q2 = R4_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R4_sample_keys = []\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q1, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q2, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q3, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing IDs: 100%|██████████| 31922/31922 [19:12<00:00, 27.70it/s]  \n",
      "100%|██████████| 22207/22207 [1:40:59<00:00,  3.67it/s]  \n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round4_2020/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):    \n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    id_to_root_class = {}\n",
    "    \n",
    "    for el in tqdm(id_list, desc=\"Processing IDs\"):\n",
    "        if el not in id_to_root_class:\n",
    "            query = f\"\"\"\n",
    "            SELECT ?instanceClass ?instanceClassLabel WHERE {{\n",
    "              wd:{el} wdt:P31 ?instanceClass .\n",
    "              SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\" }}\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Set the query and request JSON response\n",
    "            sparql.setQuery(query)\n",
    "            sparql.setReturnFormat(JSON)\n",
    "            #time.sleep(0.5)\n",
    "            \n",
    "            try:\n",
    "                results = sparql.query().convert()\n",
    "                if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "                    inst_item = int(results[\"results\"][\"bindings\"][0]['instanceClassLabel']['value'][1:])\n",
    "                    if inst_item in geolocation_subclass:\n",
    "                        id_to_root_class[el] = \"LOC\"\n",
    "                    elif inst_item in organization_subclass:\n",
    "                        id_to_root_class[el] = \"ORG\"\n",
    "                    elif inst_item == 5 or el == \"Q5\":\n",
    "                        id_to_root_class[el] = \"PERS\"\n",
    "                    else:\n",
    "                        id_to_root_class[el] = \"OTHERS\"\n",
    "                else:\n",
    "                    id_to_root_class[el] = \"OTHERS\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {el}: {e}\")\n",
    "                time.sleep(0.5)\n",
    "                id_to_root_class[el] = None          \n",
    "    \n",
    "    return id_to_root_class\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "ids = [url.split('/')[-1] for url in df[2]]\n",
    "\n",
    "root_classes = get_item_root(ids)\n",
    "\n",
    "# Map root classes to categories\n",
    "root_categories = []\n",
    "for el in ids:\n",
    "    root_categories.append(root_classes[el])\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[f\"{table_name} {row} {col}\"] = tmp_value\n",
    "                #print(f\"key: {key} -> key_to_cell[key]: tmp_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100%|██████████| 22207/22207 [10:30<00:00, 35.22it/s]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tables_path =  \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/22207 [01:04<17:21:13,  2.82s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "os.listdir(tables)\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype('str') + \" \" + df[2].astype('str')\n",
    "cea_keys = (df[\"key\"].values, df[3])\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "break\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {row+1} {col}\"\n",
    "            if key in cea_keys[0]:\n",
    "                cell_value = df.iloc[row, col]\n",
    "                ner_type[key] = (cell_value, cea_keys[1][cea_keys[0] == key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "with open('./R4_key_to_cell.json', 'w') as json_file:\n",
    "    json.dump(key_to_cell, json_file, indent=4,  cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from JSON file:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = \"./R4_ner_type.json\"\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('./R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)\n",
    "\n",
    "# Now key_to_cell contains the dictionary loaded from the JSON file\n",
    "print(\"Dictionary loaded from JSON file:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 475897/475897 [01:11<00:00, 6678.51it/s] \n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    if value is not None:\n",
    "          ### SOFT FILTERING CONTSTRAINT\n",
    "        #params = {\n",
    "        #    'name': name,\n",
    "        #    'token': 'lamapi_demo_2023',\n",
    "        #    'kg': 'wikidata',\n",
    "        #    'limit': 1000,\n",
    "        #    'query': f'''\n",
    "        #        {{\n",
    "        #            \"query\": {{\n",
    "        #                \"bool\": {{\n",
    "        #                    \"must\": [\n",
    "        #                        {{\n",
    "        #                            \"match\": {{\n",
    "        #                                \"name\": {{\n",
    "        #                                    \"query\": \"{name}\",\n",
    "        #                                    \"boost\": 2.0\n",
    "        #                                }}\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ],\n",
    "        #                    \"should\": [\n",
    "        #                        {{\n",
    "        #                            \"term\": {{\n",
    "        #                                \"NERtype\": \"{value[1]}\"\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ]\n",
    "        #                }}\n",
    "        #            }}\n",
    "        #        }}\n",
    "        #        ''',\n",
    "        #    'sort': [\n",
    "        #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        #    ]\n",
    "        #}\n",
    "    \n",
    "        ### HARD FILTERING CONTSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''\n",
    "                {{\n",
    "                    \"query\": {{\n",
    "                        \"bool\": {{\n",
    "                            \"must\": [\n",
    "                                {{\n",
    "                                    \"match\": {{\n",
    "                                        \"name\": {{\n",
    "                                            \"query\": \"{name}\",\n",
    "                                            \"boost\": 2.0\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }},\n",
    "                                {{\n",
    "                                    \"term\": {{\n",
    "                                        \"NERtype\": \"{value}\"\n",
    "                                    }}\n",
    "                                }}\n",
    "                            ]\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                ''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key]\n",
    "    q_id = None\n",
    "    try:\n",
    "        q_id = cea_values_dict[key]\n",
    "        \n",
    "        new_key = f\"{id_table} {id_col}\"\n",
    "        if new_key in ner_type:\n",
    "            NER_type = ner_type[new_key]\n",
    "            query = get_query(name, NER_type)\n",
    "            queries.append((query, re.search(r'Q(\\d+)$', q_id)[0]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 19%|█▊        | 747/4000 [27:30<1:59:46,  2.21s/it]\n",
      "2024-08-01 12:28:25,666 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,673 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:28:25,675 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,677 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:28:25,691 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,694 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,699 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,704 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,708 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,713 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:28:25,716 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:28:25,733 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:28:25,737 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:28:25,739 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,742 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,745 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "2024-08-01 12:28:25,756 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:28:25,773 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:28:25,777 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,780 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,782 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:28:25,786 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:28:25,788 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:28:25,795 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,797 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:28:25,800 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:28:25,804 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:28:25,809 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:28:25,812 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,817 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,822 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,824 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "2024-08-01 12:28:25,825 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,829 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:28:25,830 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,832 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,841 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:28:25,844 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,847 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:28:25,853 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,855 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,858 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,862 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:28:25,864 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,866 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:28:25,869 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:28:25,871 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:28:25,874 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:28:25,881 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:28:25,883 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "\n",
      "\n",
      "  0%|          | 1/4000 [00:45<50:09:47, 45.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/4000 [00:45<20:57:24, 18.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 3/4000 [00:45<11:27:57, 10.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/4000 [00:47<7:45:00,  6.98s/it] \u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 5/4000 [00:53<7:08:03,  6.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 6/4000 [00:57<6:16:42,  5.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/4000 [01:03<6:36:35,  5.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/4000 [01:05<3:55:09,  3.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 10/4000 [01:06<3:03:48,  2.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/4000 [01:07<2:33:37,  2.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 12/4000 [01:09<2:40:14,  2.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/4000 [01:10<2:01:36,  1.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 14/4000 [01:10<1:33:48,  1.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/4000 [01:13<2:03:51,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 16/4000 [01:15<1:54:59,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 17/4000 [01:17<2:04:41,  1.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 18/4000 [01:17<1:29:59,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 19/4000 [01:18<1:15:23,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 20/4000 [01:22<2:16:16,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 22/4000 [01:27<2:26:31,  2.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 23/4000 [01:29<2:36:00,  2.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 24/4000 [01:32<2:39:30,  2.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 25/4000 [01:32<1:59:49,  1.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 26/4000 [01:33<1:34:05,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 27/4000 [01:34<1:30:11,  1.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 28/4000 [01:36<1:42:45,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 29/4000 [01:36<1:17:57,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 30/4000 [01:38<1:24:43,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 31/4000 [01:39<1:34:30,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 32/4000 [01:40<1:24:22,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 33/4000 [01:41<1:06:30,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 34/4000 [01:44<1:45:25,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 35/4000 [01:44<1:16:14,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 36/4000 [01:44<56:33,  1.17it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 38/4000 [01:45<46:10,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 39/4000 [01:47<1:05:59,  1.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 40/4000 [01:49<1:26:55,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 41/4000 [01:52<1:55:55,  1.76s/it]\u001b[A\u001b[A2024-08-01 12:29:47,172 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "\n",
      "\n",
      "  1%|          | 42/4000 [01:54<2:02:35,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 43/4000 [01:56<2:06:14,  1.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 44/4000 [01:59<2:22:14,  2.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 45/4000 [02:05<3:37:36,  3.30s/it]\u001b[A\u001b[A2024-08-01 12:30:01,148 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:30:01,152 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "\n",
      "\n",
      "  1%|          | 46/4000 [02:09<3:47:58,  3.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 47/4000 [02:09<2:47:26,  2.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 48/4000 [02:10<2:09:50,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 49/4000 [02:11<2:04:13,  1.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 50/4000 [02:14<2:20:23,  2.13s/it]\u001b[A\u001b[A2024-08-01 12:30:10,147 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "\n",
      "\n",
      "  1%|▏         | 51/4000 [02:17<2:31:29,  2.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 52/4000 [02:24<3:57:29,  3.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 53/4000 [02:24<2:57:36,  2.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 54/4000 [02:24<2:07:54,  1.94s/it]\u001b[A\u001b[A2024-08-01 12:30:19,167 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "\n",
      "\n",
      "  1%|▏         | 55/4000 [02:25<1:44:05,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 56/4000 [02:26<1:37:02,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 57/4000 [02:27<1:18:29,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 58/4000 [02:27<1:02:59,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 59/4000 [02:28<57:29,  1.14it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 60/4000 [02:30<1:16:50,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 61/4000 [02:35<2:34:43,  2.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 62/4000 [02:39<3:05:51,  2.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 63/4000 [02:42<3:19:18,  3.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 64/4000 [02:45<3:03:46,  2.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 65/4000 [02:46<2:42:08,  2.47s/it]\u001b[A\u001b[A2024-08-01 12:30:42,146 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "\n",
      "\n",
      "  2%|▏         | 66/4000 [02:48<2:19:23,  2.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 67/4000 [02:48<1:45:57,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 68/4000 [02:51<2:21:24,  2.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 69/4000 [02:55<2:55:26,  2.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 70/4000 [02:56<2:15:32,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 71/4000 [02:57<1:57:59,  1.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 72/4000 [02:59<1:48:06,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 73/4000 [02:59<1:30:48,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 74/4000 [03:01<1:38:10,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 75/4000 [03:02<1:19:17,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 76/4000 [03:03<1:32:25,  1.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 77/4000 [03:04<1:06:52,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 78/4000 [03:04<53:44,  1.22it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 79/4000 [03:04<47:55,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 80/4000 [03:07<1:20:20,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 81/4000 [03:12<2:32:14,  2.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 82/4000 [03:13<2:02:15,  1.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 83/4000 [03:14<2:00:51,  1.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 84/4000 [03:16<1:50:35,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 85/4000 [03:19<2:16:38,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 86/4000 [03:21<2:16:10,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 87/4000 [03:21<1:38:45,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 88/4000 [03:22<1:39:07,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 89/4000 [03:25<2:03:34,  1.90s/it]\u001b[A\u001b[A2024-08-01 12:31:21,145 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "\n",
      "\n",
      "  2%|▏         | 90/4000 [03:27<2:09:14,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 91/4000 [03:30<2:26:02,  2.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 92/4000 [03:31<1:57:02,  1.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 93/4000 [03:32<1:40:28,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 94/4000 [03:32<1:19:03,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 96/4000 [03:33<46:16,  1.41it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 97/4000 [03:36<1:35:20,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 99/4000 [03:38<1:11:11,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▎         | 100/4000 [03:39<1:11:22,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 102/4000 [03:39<47:06,  1.38it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 103/4000 [03:43<1:29:47,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 104/4000 [03:46<2:06:11,  1.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 105/4000 [03:47<1:36:39,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 106/4000 [03:48<1:35:08,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 107/4000 [03:50<1:43:08,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 108/4000 [03:50<1:22:49,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 109/4000 [03:52<1:26:44,  1.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 110/4000 [03:52<1:04:44,  1.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 111/4000 [03:55<1:32:18,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 112/4000 [03:56<1:31:44,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 113/4000 [03:57<1:23:18,  1.29s/it]\u001b[A\u001b[A2024-08-01 12:31:52,142 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "\n",
      "\n",
      "  3%|▎         | 114/4000 [04:00<2:03:08,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 115/4000 [04:00<1:30:02,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 116/4000 [04:01<1:13:51,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 117/4000 [04:02<1:07:15,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 118/4000 [04:04<1:20:36,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 119/4000 [04:04<1:06:17,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 120/4000 [04:08<1:54:47,  1.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 121/4000 [04:08<1:25:39,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 122/4000 [04:09<1:30:35,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 123/4000 [04:13<2:18:27,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 124/4000 [04:14<1:55:07,  1.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 125/4000 [04:15<1:38:42,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 126/4000 [04:16<1:21:10,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 127/4000 [04:17<1:28:46,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 128/4000 [04:18<1:21:25,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 129/4000 [04:20<1:18:50,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 131/4000 [04:24<1:46:58,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 132/4000 [04:25<1:41:43,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 133/4000 [04:26<1:20:25,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 134/4000 [04:26<1:01:44,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 135/4000 [04:28<1:29:13,  1.39s/it]\u001b[A\u001b[A2024-08-01 12:32:25,141 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "\n",
      "\n",
      "  3%|▎         | 136/4000 [04:38<4:06:43,  3.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 137/4000 [04:41<3:50:15,  3.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 138/4000 [04:44<3:37:50,  3.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 139/4000 [04:45<2:44:48,  2.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 140/4000 [04:45<2:04:53,  1.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 141/4000 [04:46<1:42:51,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 142/4000 [04:48<1:55:00,  1.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 143/4000 [04:50<2:01:11,  1.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 145/4000 [04:50<1:08:01,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 146/4000 [04:52<1:17:45,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 147/4000 [04:52<1:00:23,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 148/4000 [04:54<1:12:28,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 149/4000 [04:54<1:01:29,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 150/4000 [04:56<1:09:02,  1.08s/it]\u001b[A\u001b[A2024-08-01 12:32:52,138 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "\n",
      "\n",
      "  4%|▍         | 151/4000 [04:57<1:20:48,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 152/4000 [04:58<59:59,  1.07it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 153/4000 [04:59<1:08:09,  1.06s/it]\u001b[A\u001b[A2024-08-01 12:32:54,140 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  4%|▍         | 155/4000 [05:01<1:00:08,  1.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 156/4000 [05:01<58:03,  1.10it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 157/4000 [05:04<1:19:34,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 158/4000 [05:04<1:11:54,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 159/4000 [05:05<1:07:11,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 160/4000 [05:07<1:23:39,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 161/4000 [05:09<1:25:46,  1.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 162/4000 [05:10<1:24:58,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 163/4000 [05:12<1:30:24,  1.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 164/4000 [05:12<1:18:52,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 165/4000 [05:13<1:02:38,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 166/4000 [05:15<1:20:39,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 167/4000 [05:16<1:24:02,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 168/4000 [05:18<1:29:15,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 170/4000 [05:19<1:07:20,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 172/4000 [05:26<2:06:10,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 173/4000 [05:27<1:52:05,  1.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 174/4000 [05:28<1:51:44,  1.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 175/4000 [05:29<1:28:26,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 176/4000 [05:33<2:11:53,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 177/4000 [05:35<2:10:09,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 179/4000 [05:35<1:18:08,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 180/4000 [05:38<1:36:59,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 181/4000 [05:42<2:18:51,  2.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 182/4000 [05:42<1:45:12,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 184/4000 [05:45<1:38:35,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 185/4000 [05:47<1:42:53,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 186/4000 [05:48<1:44:50,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 187/4000 [05:49<1:33:07,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 188/4000 [05:52<2:03:07,  1.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 189/4000 [05:57<2:58:25,  2.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 190/4000 [05:58<2:17:53,  2.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 191/4000 [06:00<2:19:26,  2.20s/it]\u001b[A\u001b[A2024-08-01 12:33:55,169 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "\n",
      "\n",
      "  5%|▍         | 192/4000 [06:01<1:47:33,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 193/4000 [06:02<1:35:40,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 194/4000 [06:03<1:30:29,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 195/4000 [06:04<1:17:30,  1.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 196/4000 [06:05<1:09:41,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 197/4000 [06:05<55:21,  1.14it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 198/4000 [06:07<1:21:25,  1.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 199/4000 [06:09<1:33:24,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 200/4000 [06:11<1:36:31,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 201/4000 [06:11<1:10:09,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 202/4000 [06:15<2:05:00,  1.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 203/4000 [06:15<1:35:14,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 204/4000 [06:16<1:16:08,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 205/4000 [06:16<57:05,  1.11it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 206/4000 [06:18<1:14:26,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 207/4000 [06:20<1:23:52,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 208/4000 [06:21<1:21:44,  1.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 209/4000 [06:21<1:02:20,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 210/4000 [06:24<1:48:27,  1.72s/it]\u001b[A\u001b[A2024-08-01 12:34:23,130 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  5%|▌         | 211/4000 [06:31<3:19:47,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 213/4000 [06:31<1:49:08,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 214/4000 [06:32<1:38:15,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 215/4000 [06:33<1:20:18,  1.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 216/4000 [06:36<1:55:02,  1.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 217/4000 [06:36<1:28:20,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 219/4000 [06:39<1:24:02,  1.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 220/4000 [06:42<1:49:12,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 221/4000 [06:43<1:38:03,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 222/4000 [06:43<1:19:14,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 223/4000 [06:45<1:31:20,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 224/4000 [06:45<1:11:54,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 225/4000 [06:47<1:15:24,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 226/4000 [06:49<1:41:34,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 227/4000 [06:51<1:31:09,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 228/4000 [06:52<1:37:02,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 229/4000 [06:53<1:27:47,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 230/4000 [06:54<1:11:17,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 231/4000 [06:54<1:01:09,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 232/4000 [06:55<56:36,  1.11it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 233/4000 [06:57<1:10:17,  1.12s/it]\u001b[A\u001b[A2024-08-01 12:34:54,130 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 235/4000 [07:00<1:24:36,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 236/4000 [07:05<2:22:55,  2.28s/it]\u001b[A\u001b[A2024-08-01 12:35:04,124 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 237/4000 [07:11<3:18:09,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 238/4000 [07:13<3:01:54,  2.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 239/4000 [07:14<2:27:24,  2.35s/it]\u001b[A\u001b[A2024-08-01 12:35:09,125 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 240/4000 [07:16<2:25:48,  2.33s/it]\u001b[A\u001b[A2024-08-01 12:35:14,124 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 241/4000 [07:19<2:40:33,  2.56s/it]\u001b[A\u001b[A2024-08-01 12:35:16,123 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 242/4000 [07:24<3:26:25,  3.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 243/4000 [07:25<2:35:37,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 244/4000 [07:26<2:02:41,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 245/4000 [07:27<1:43:15,  1.65s/it]\u001b[A\u001b[A2024-08-01 12:35:25,125 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 246/4000 [07:35<3:47:30,  3.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 247/4000 [07:35<2:44:52,  2.64s/it]\u001b[A\u001b[A2024-08-01 12:35:30,124 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▌         | 248/4000 [07:39<3:05:03,  2.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 249/4000 [07:43<3:17:21,  3.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 250/4000 [07:43<2:33:03,  2.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 251/4000 [07:43<1:50:21,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 252/4000 [07:44<1:25:19,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 253/4000 [07:44<1:02:27,  1.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 254/4000 [07:46<1:12:35,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 255/4000 [07:46<53:39,  1.16it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 256/4000 [07:47<53:04,  1.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 257/4000 [07:47<44:08,  1.41it/s]\u001b[A\u001b[A2024-08-01 12:35:42,121 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▋         | 258/4000 [07:47<37:52,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 259/4000 [07:50<1:10:17,  1.13s/it]\u001b[A\u001b[A2024-08-01 12:35:45,122 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "\n",
      "\n",
      "  6%|▋         | 260/4000 [07:54<2:11:35,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 261/4000 [07:57<2:33:57,  2.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 262/4000 [07:57<1:49:40,  1.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 263/4000 [07:58<1:26:52,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 264/4000 [07:58<1:09:18,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 265/4000 [07:59<1:01:56,  1.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 266/4000 [08:00<53:33,  1.16it/s]  \u001b[A\u001b[A2024-08-01 12:35:55,121 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  7%|▋         | 267/4000 [08:02<1:25:36,  1.38s/it]\u001b[A\u001b[A2024-08-01 12:36:00,121 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "\n",
      "\n",
      "  7%|▋         | 268/4000 [08:06<2:02:51,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 269/4000 [08:11<3:04:09,  2.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 270/4000 [08:11<2:10:49,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 271/4000 [08:15<2:48:07,  2.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 272/4000 [08:16<2:12:51,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 273/4000 [08:18<2:08:11,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 274/4000 [08:18<1:39:17,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 275/4000 [08:19<1:24:49,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 276/4000 [08:20<1:15:05,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 277/4000 [08:21<1:04:02,  1.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 278/4000 [08:24<1:44:07,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 279/4000 [08:24<1:23:10,  1.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 280/4000 [08:26<1:23:15,  1.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 281/4000 [08:26<1:07:18,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 282/4000 [08:27<55:28,  1.12it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 283/4000 [08:28<1:04:09,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 285/4000 [08:28<38:48,  1.60it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 286/4000 [08:31<1:07:16,  1.09s/it]\u001b[A\u001b[A2024-08-01 12:36:26,132 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "\n",
      "\n",
      "  7%|▋         | 287/4000 [08:32<1:09:12,  1.12s/it]\u001b[A\u001b[A2024-08-01 12:36:31,115 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  7%|▋         | 288/4000 [08:41<3:13:55,  3.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 290/4000 [08:46<3:08:37,  3.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 291/4000 [08:48<2:51:43,  2.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 292/4000 [08:49<2:16:12,  2.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 293/4000 [08:51<2:15:27,  2.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 294/4000 [08:52<2:01:09,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 295/4000 [08:53<1:37:19,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 296/4000 [08:54<1:20:46,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 297/4000 [08:59<2:33:14,  2.48s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 298/4000 [08:59<1:52:04,  1.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 299/4000 [09:04<2:41:54,  2.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 300/4000 [09:06<2:31:04,  2.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 301/4000 [09:07<2:14:05,  2.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 302/4000 [09:08<1:41:23,  1.65s/it]\u001b[A\u001b[A2024-08-01 12:37:04,114 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      "  8%|▊         | 303/4000 [09:33<8:55:15,  8.69s/it]\u001b[A\u001b[A2024-08-01 12:37:28,257 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:37:28,262 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:37:28,264 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:37:28,267 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:37:28,269 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:37:28,273 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:37:28,275 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:37:28,278 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:37:28,285 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:37:28,295 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:37:28,297 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:37:28,301 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:37:28,303 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "2024-08-01 12:37:28,308 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:37:28,313 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:37:28,316 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:37:28,325 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:37:28,328 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:37:28,331 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:37:28,333 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:37:28,335 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:37:28,341 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "2024-08-01 12:37:28,346 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:37:28,349 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:37:28,351 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:37:28,360 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "2024-08-01 12:37:28,363 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:37:28,368 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "\n",
      "\n",
      "  8%|▊         | 304/4000 [09:34<6:36:16,  6.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 305/4000 [09:34<4:41:36,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 306/4000 [09:34<3:19:40,  3.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 308/4000 [09:45<4:13:12,  4.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 309/4000 [09:47<3:39:25,  3.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 310/4000 [09:47<2:48:18,  2.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 311/4000 [09:48<2:24:40,  2.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 312/4000 [09:49<1:47:14,  1.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 313/4000 [09:50<1:46:16,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 314/4000 [09:51<1:35:49,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 315/4000 [09:55<2:07:07,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 316/4000 [09:58<2:38:33,  2.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 317/4000 [09:59<2:01:33,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 318/4000 [09:59<1:28:42,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 319/4000 [10:02<2:01:25,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 320/4000 [10:03<1:44:12,  1.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 321/4000 [10:04<1:15:07,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 322/4000 [10:04<55:55,  1.10it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 323/4000 [10:05<1:08:29,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 324/4000 [10:08<1:39:45,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 326/4000 [10:10<1:15:42,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 327/4000 [10:12<1:31:57,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 328/4000 [10:14<1:31:54,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 329/4000 [10:15<1:31:41,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 330/4000 [10:15<1:09:30,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 331/4000 [10:16<54:47,  1.12it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 332/4000 [10:16<47:18,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 333/4000 [10:17<52:13,  1.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 334/4000 [10:18<51:18,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 335/4000 [10:19<51:17,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 336/4000 [10:22<1:40:47,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 337/4000 [10:23<1:27:18,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 338/4000 [10:24<1:15:51,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 339/4000 [10:25<1:16:57,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 340/4000 [10:27<1:16:17,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 341/4000 [10:27<1:08:44,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 344/4000 [10:28<32:19,  1.89it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 345/4000 [10:30<53:50,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 346/4000 [10:32<1:08:10,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 347/4000 [10:32<1:01:13,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 348/4000 [10:33<51:13,  1.19it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 349/4000 [10:33<50:34,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 350/4000 [10:34<46:26,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 351/4000 [10:35<42:15,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 352/4000 [10:37<1:09:13,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 353/4000 [10:39<1:28:09,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 354/4000 [10:41<1:31:45,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 355/4000 [10:41<1:06:27,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 356/4000 [10:41<51:27,  1.18it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 358/4000 [10:42<34:12,  1.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 359/4000 [10:42<29:58,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 360/4000 [10:44<57:43,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 361/4000 [10:46<1:10:45,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 362/4000 [10:48<1:27:32,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 363/4000 [10:48<1:08:39,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 364/4000 [10:49<1:05:08,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 366/4000 [10:53<1:29:23,  1.48s/it]\u001b[A\u001b[A2024-08-01 12:38:48,118 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "\n",
      "\n",
      "  9%|▉         | 367/4000 [10:55<1:37:58,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 369/4000 [10:56<1:04:31,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 370/4000 [10:57<1:05:43,  1.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 371/4000 [10:57<56:09,  1.08it/s]  \u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 372/4000 [10:58<57:20,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 373/4000 [10:59<48:42,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 375/4000 [11:00<44:29,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 376/4000 [11:01<43:35,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 377/4000 [11:02<47:09,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 379/4000 [11:02<33:35,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 380/4000 [11:02<29:24,  2.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 381/4000 [11:04<43:13,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 382/4000 [11:04<37:35,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 383/4000 [11:05<38:53,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 384/4000 [11:05<35:47,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 385/4000 [11:06<28:46,  2.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 386/4000 [11:06<25:29,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 387/4000 [11:07<34:37,  1.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 388/4000 [11:08<43:24,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 389/4000 [11:08<36:53,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 391/4000 [11:09<36:08,  1.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 392/4000 [11:11<47:56,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 394/4000 [11:11<30:02,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 395/4000 [11:11<27:26,  2.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 396/4000 [11:12<37:43,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 397/4000 [11:13<29:39,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 398/4000 [11:13<26:12,  2.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 399/4000 [11:14<37:33,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 400/4000 [11:16<57:58,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 401/4000 [11:16<43:25,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 402/4000 [11:17<56:29,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 404/4000 [11:18<45:12,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 405/4000 [11:19<43:45,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 406/4000 [11:19<37:12,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 407/4000 [11:20<30:33,  1.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 408/4000 [11:21<43:18,  1.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 409/4000 [11:21<32:47,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 410/4000 [11:22<46:31,  1.29it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 411/4000 [11:23<44:45,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 412/4000 [11:24<42:00,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 413/4000 [11:24<44:09,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 414/4000 [11:25<36:49,  1.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 415/4000 [11:25<28:26,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 416/4000 [11:26<35:30,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 417/4000 [11:26<27:04,  2.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 418/4000 [11:28<48:00,  1.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 419/4000 [11:28<36:38,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 420/4000 [11:29<46:40,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 422/4000 [11:30<35:34,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 423/4000 [11:31<53:17,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 424/4000 [11:32<43:25,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 425/4000 [11:32<37:17,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 426/4000 [11:33<34:46,  1.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 427/4000 [11:33<34:02,  1.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 428/4000 [11:33<26:47,  2.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 429/4000 [11:33<24:02,  2.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 430/4000 [11:35<42:45,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 431/4000 [11:35<35:01,  1.70it/s]\u001b[A\u001b[A2024-08-01 12:39:30,103 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "\n",
      "\n",
      " 11%|█         | 432/4000 [11:36<34:42,  1.71it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 433/4000 [11:38<1:05:09,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 434/4000 [11:39<59:16,  1.00it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 436/4000 [11:40<45:17,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 437/4000 [11:42<1:07:51,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 438/4000 [11:44<1:14:43,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 439/4000 [11:44<1:03:44,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 441/4000 [11:45<47:06,  1.26it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 442/4000 [11:46<44:47,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 443/4000 [11:46<37:07,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 444/4000 [11:47<41:55,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 445/4000 [11:48<38:56,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 446/4000 [11:48<34:56,  1.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 447/4000 [11:48<28:00,  2.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 448/4000 [11:49<26:54,  2.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 449/4000 [11:49<33:23,  1.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 450/4000 [11:50<28:33,  2.07it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 451/4000 [11:51<35:46,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 452/4000 [11:51<40:29,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 453/4000 [11:52<46:42,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 454/4000 [11:54<1:06:13,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 455/4000 [11:54<49:02,  1.20it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 456/4000 [11:55<48:09,  1.23it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 457/4000 [11:56<49:40,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 458/4000 [11:56<39:16,  1.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 459/4000 [11:57<33:06,  1.78it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 460/4000 [11:57<25:27,  2.32it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 461/4000 [11:58<32:04,  1.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 462/4000 [11:58<26:36,  2.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 463/4000 [11:58<20:35,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 465/4000 [12:00<42:52,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 467/4000 [12:01<29:31,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 468/4000 [12:02<36:00,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 469/4000 [12:02<33:36,  1.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 470/4000 [12:02<28:02,  2.10it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 471/4000 [12:04<40:28,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 473/4000 [12:05<42:16,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 474/4000 [12:06<48:16,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 475/4000 [12:06<39:21,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 476/4000 [12:07<36:27,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 477/4000 [12:08<41:02,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 478/4000 [12:08<32:33,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 479/4000 [12:09<35:06,  1.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 480/4000 [12:09<35:27,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 481/4000 [12:12<1:02:43,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 483/4000 [12:13<51:45,  1.13it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 484/4000 [12:14<56:09,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 485/4000 [12:14<44:39,  1.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 486/4000 [12:15<49:23,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 487/4000 [12:16<51:23,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 488/4000 [12:17<40:13,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 489/4000 [12:17<33:42,  1.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 490/4000 [12:18<37:11,  1.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 491/4000 [12:18<34:53,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 492/4000 [12:18<30:44,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 493/4000 [12:19<36:52,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 494/4000 [12:20<31:56,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 495/4000 [12:21<45:30,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 496/4000 [12:22<50:14,  1.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 498/4000 [12:23<42:05,  1.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 499/4000 [12:24<38:13,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▎        | 500/4000 [12:24<31:12,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 502/4000 [12:24<19:58,  2.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 503/4000 [12:25<25:14,  2.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 504/4000 [12:26<37:34,  1.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 505/4000 [12:26<30:21,  1.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 506/4000 [12:27<27:26,  2.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 508/4000 [12:28<31:50,  1.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 509/4000 [12:30<51:25,  1.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 510/4000 [12:31<59:06,  1.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 512/4000 [12:32<38:11,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 513/4000 [12:32<32:46,  1.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 514/4000 [12:32<26:39,  2.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 516/4000 [12:32<21:43,  2.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 517/4000 [12:33<23:32,  2.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 518/4000 [12:34<37:51,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 519/4000 [12:35<38:23,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 520/4000 [12:36<40:06,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 521/4000 [12:36<31:15,  1.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 522/4000 [12:36<26:50,  2.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 524/4000 [12:37<29:36,  1.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 526/4000 [12:38<23:11,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 527/4000 [12:40<41:08,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 529/4000 [12:40<30:22,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 530/4000 [12:42<44:32,  1.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 532/4000 [12:43<40:36,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 533/4000 [12:43<33:29,  1.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 534/4000 [12:44<35:31,  1.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 535/4000 [12:45<39:46,  1.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 537/4000 [12:45<26:19,  2.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 539/4000 [12:45<19:21,  2.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 540/4000 [12:47<35:03,  1.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 541/4000 [12:47<30:06,  1.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 543/4000 [12:48<24:03,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 544/4000 [12:48<20:09,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 545/4000 [12:48<22:57,  2.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 546/4000 [12:49<21:02,  2.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 547/4000 [12:49<19:44,  2.92it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 548/4000 [12:50<30:40,  1.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 549/4000 [12:51<36:31,  1.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 551/4000 [12:51<27:02,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 553/4000 [12:52<21:51,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 554/4000 [12:53<33:16,  1.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 555/4000 [12:54<38:25,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 556/4000 [12:55<42:28,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 557/4000 [12:55<37:45,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 558/4000 [12:59<1:27:15,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 559/4000 [13:02<1:41:16,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 560/4000 [13:02<1:17:26,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 561/4000 [13:03<1:11:24,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 562/4000 [13:03<57:52,  1.01s/it]  \u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 563/4000 [13:04<45:50,  1.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 564/4000 [13:04<36:16,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 565/4000 [13:05<48:13,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 566/4000 [13:06<51:03,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 567/4000 [13:08<1:00:34,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 568/4000 [13:08<46:03,  1.24it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 569/4000 [13:08<41:35,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 570/4000 [13:09<36:16,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 571/4000 [13:09<28:17,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 572/4000 [13:10<31:46,  1.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 573/4000 [13:10<24:09,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 574/4000 [13:11<34:42,  1.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 575/4000 [13:11<29:23,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 576/4000 [13:12<28:37,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 577/4000 [13:12<22:41,  2.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 578/4000 [13:13<32:17,  1.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 579/4000 [13:13<32:41,  1.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 580/4000 [13:14<30:30,  1.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 581/4000 [13:15<48:01,  1.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 582/4000 [13:16<47:29,  1.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 583/4000 [13:17<47:02,  1.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 584/4000 [13:17<39:52,  1.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 585/4000 [13:18<37:18,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 586/4000 [13:21<1:10:28,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 587/4000 [13:21<56:36,  1.00it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 588/4000 [13:22<57:33,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 589/4000 [13:22<44:48,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 590/4000 [13:23<44:42,  1.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 591/4000 [13:24<40:16,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 592/4000 [13:24<39:20,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 593/4000 [13:25<32:39,  1.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 594/4000 [13:26<49:51,  1.14it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 595/4000 [13:27<57:18,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 596/4000 [13:30<1:22:50,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 597/4000 [13:34<2:11:38,  2.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 598/4000 [13:36<2:02:19,  2.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 599/4000 [13:37<1:41:56,  1.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 600/4000 [13:38<1:20:26,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 601/4000 [13:39<1:17:26,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 602/4000 [13:44<2:22:59,  2.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 603/4000 [13:44<1:41:52,  1.80s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 604/4000 [13:45<1:18:31,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 605/4000 [13:46<1:25:44,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 606/4000 [13:47<1:05:46,  1.16s/it]\u001b[A\u001b[A2024-08-01 12:41:42,103 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "\n",
      "\n",
      " 15%|█▌        | 607/4000 [13:47<57:37,  1.02s/it]  \u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 608/4000 [13:49<1:05:07,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 609/4000 [13:51<1:14:22,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 610/4000 [13:51<1:05:11,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 611/4000 [13:53<1:06:59,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 612/4000 [13:55<1:21:05,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 613/4000 [13:58<1:59:33,  2.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 614/4000 [14:00<1:47:15,  1.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 616/4000 [14:00<1:00:35,  1.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 617/4000 [14:00<47:00,  1.20it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 618/4000 [14:01<46:08,  1.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 619/4000 [14:02<54:20,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 620/4000 [14:02<41:04,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 621/4000 [14:03<44:03,  1.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 622/4000 [14:03<33:23,  1.69it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 623/4000 [14:04<26:42,  2.11it/s]\u001b[A\u001b[A2024-08-01 12:42:00,093 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "\n",
      "\n",
      " 16%|█▌        | 624/4000 [14:06<58:19,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 625/4000 [14:07<59:17,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 626/4000 [14:07<48:54,  1.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 627/4000 [14:08<45:27,  1.24it/s]\u001b[A\u001b[A2024-08-01 12:42:04,090 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "\n",
      "\n",
      " 16%|█▌        | 628/4000 [14:09<52:57,  1.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 629/4000 [14:13<1:33:46,  1.67s/it]\u001b[A\u001b[A2024-08-01 12:42:08,090 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "\n",
      "\n",
      " 16%|█▌        | 630/4000 [14:15<1:45:29,  1.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 632/4000 [14:16<1:10:46,  1.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 633/4000 [14:20<1:45:54,  1.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 634/4000 [14:21<1:29:49,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 635/4000 [14:22<1:24:38,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 636/4000 [14:26<2:09:29,  2.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 637/4000 [14:27<1:35:57,  1.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 638/4000 [14:27<1:09:52,  1.25s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 639/4000 [14:30<1:44:15,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 640/4000 [14:33<1:55:42,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 641/4000 [14:35<2:02:08,  2.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 642/4000 [14:36<1:34:33,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 643/4000 [14:37<1:37:14,  1.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 644/4000 [14:41<2:12:06,  2.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 645/4000 [14:43<1:54:45,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 646/4000 [14:43<1:31:40,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 647/4000 [14:45<1:28:05,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 648/4000 [14:46<1:19:41,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 649/4000 [14:46<59:45,  1.07s/it]  \u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 650/4000 [14:48<1:08:49,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 651/4000 [14:48<52:26,  1.06it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 652/4000 [14:49<53:58,  1.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 653/4000 [14:50<54:51,  1.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 655/4000 [14:50<36:38,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 656/4000 [14:55<1:29:53,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 658/4000 [14:55<57:32,  1.03s/it]  \u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 659/4000 [15:00<1:43:20,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 660/4000 [15:01<1:31:17,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 661/4000 [15:03<1:37:20,  1.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 662/4000 [15:06<1:56:33,  2.10s/it]\u001b[A\u001b[A2024-08-01 12:43:01,090 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      " 17%|█▋        | 664/4000 [15:06<1:08:45,  1.24s/it]\u001b[A\u001b[A2024-08-01 12:43:02,087 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "2024-08-01 12:43:03,086 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "\n",
      "\n",
      " 17%|█▋        | 665/4000 [15:08<1:18:16,  1.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 666/4000 [15:10<1:14:54,  1.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 667/4000 [15:11<1:20:21,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 668/4000 [15:12<1:09:51,  1.26s/it]\u001b[A\u001b[A2024-08-01 12:43:09,088 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:43:09,092 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:43:09,100 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:43:13,087 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "\n",
      "\n",
      " 17%|█▋        | 669/4000 [15:20<2:52:55,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 670/4000 [15:21<2:14:11,  2.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 671/4000 [15:25<2:52:26,  3.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 672/4000 [15:26<2:20:31,  2.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 673/4000 [15:27<1:45:54,  1.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 674/4000 [15:28<1:28:41,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 675/4000 [15:29<1:17:27,  1.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 676/4000 [15:30<1:15:50,  1.37s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 677/4000 [15:31<1:01:59,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 678/4000 [15:34<1:39:26,  1.80s/it]\u001b[A\u001b[A2024-08-01 12:43:29,087 - INFO - Backing off fetch(...) for 0.1s (TimeoutError)\n",
      "\n",
      "\n",
      " 17%|█▋        | 679/4000 [15:35<1:23:34,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 680/4000 [15:39<2:17:11,  2.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 681/4000 [15:40<1:44:11,  1.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 682/4000 [15:40<1:18:59,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 683/4000 [15:42<1:16:59,  1.39s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 684/4000 [15:43<1:23:29,  1.51s/it]\u001b[A\u001b[A2024-08-01 12:43:43,085 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:43:45,085 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:43:45,087 - INFO - Backing off fetch(...) for 0.8s (TimeoutError)\n",
      "\n",
      "\n",
      " 17%|█▋        | 685/4000 [15:52<3:21:16,  3.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 686/4000 [15:52<2:26:42,  2.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 687/4000 [15:53<1:46:05,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 688/4000 [15:53<1:22:56,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 689/4000 [15:56<1:50:36,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 690/4000 [15:57<1:29:46,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 691/4000 [15:58<1:12:49,  1.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 692/4000 [15:58<55:44,  1.01s/it]  \u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 694/4000 [15:58<33:11,  1.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 695/4000 [16:01<1:05:41,  1.19s/it]\u001b[A\u001b[A2024-08-01 12:43:57,086 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "\n",
      "\n",
      " 17%|█▋        | 696/4000 [16:04<1:35:58,  1.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 697/4000 [16:05<1:12:11,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 698/4000 [16:07<1:22:44,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 699/4000 [16:07<1:01:54,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 700/4000 [16:09<1:21:18,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 701/4000 [16:13<2:06:59,  2.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 702/4000 [16:15<1:51:17,  2.02s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 703/4000 [16:17<1:47:11,  1.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 704/4000 [16:17<1:23:10,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 705/4000 [16:18<1:17:36,  1.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 706/4000 [16:20<1:17:48,  1.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 707/4000 [16:22<1:36:58,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 708/4000 [16:44<6:57:55,  7.62s/it]\u001b[A\u001b[A2024-08-01 12:44:38,878 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:44:38,882 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:44:38,886 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:44:38,893 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:44:38,896 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:44:38,898 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "2024-08-01 12:44:38,901 - INFO - Backing off fetch(...) for 0.0s (TimeoutError)\n",
      "2024-08-01 12:44:38,903 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:44:38,910 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:44:38,914 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "2024-08-01 12:44:38,916 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:44:38,918 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "2024-08-01 12:44:38,923 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "\n",
      "\n",
      " 18%|█▊        | 709/4000 [16:44<5:01:09,  5.49s/it]\u001b[A\u001b[A2024-08-01 12:44:38,958 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:44:38,978 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:44:38,986 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "\n",
      "\n",
      " 18%|█▊        | 711/4000 [16:45<2:53:32,  3.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 713/4000 [16:45<1:47:08,  1.96s/it]\u001b[A\u001b[A2024-08-01 12:44:40,284 - INFO - Backing off fetch(...) for 0.7s (TimeoutError)\n",
      "\n",
      "\n",
      " 18%|█▊        | 715/4000 [16:57<3:05:58,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 716/4000 [16:57<2:31:26,  2.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 717/4000 [17:03<3:06:01,  3.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 719/4000 [17:03<1:55:48,  2.12s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 720/4000 [17:05<1:48:41,  1.99s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 721/4000 [17:05<1:31:26,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 722/4000 [17:07<1:24:20,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 724/4000 [17:07<50:18,  1.09it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 725/4000 [17:08<57:13,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 726/4000 [17:12<1:34:53,  1.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 727/4000 [17:15<1:54:21,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 728/4000 [17:16<1:41:01,  1.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 729/4000 [17:19<1:54:26,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 730/4000 [17:21<1:52:16,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 731/4000 [17:22<1:30:21,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 733/4000 [17:23<1:05:03,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 734/4000 [17:23<53:50,  1.01it/s]  \u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 736/4000 [17:24<38:46,  1.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 737/4000 [17:24<32:43,  1.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 738/4000 [17:25<35:35,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 739/4000 [17:26<40:37,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 740/4000 [17:29<1:11:11,  1.31s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 741/4000 [17:31<1:18:03,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 742/4000 [17:35<2:09:11,  2.38s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 743/4000 [17:38<2:15:02,  2.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 744/4000 [17:41<2:25:01,  2.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 745/4000 [17:43<2:11:29,  2.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 746/4000 [17:46<2:14:12,  2.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 747/4000 [17:46<1:36:05,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 748/4000 [17:48<1:50:19,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 749/4000 [17:53<2:26:44,  2.71s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 750/4000 [17:57<2:50:48,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 751/4000 [17:58<2:21:46,  2.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 752/4000 [17:59<1:48:49,  2.01s/it]\u001b[A\u001b[A2024-08-01 12:45:55,087 - INFO - Backing off fetch(...) for 1.6s (TimeoutError)\n",
      "2024-08-01 12:45:56,078 - INFO - Backing off fetch(...) for 1.9s (TimeoutError)\n",
      "2024-08-01 12:45:57,080 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:45:57,084 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "2024-08-01 12:45:57,087 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "\n",
      "\n",
      " 19%|█▉        | 753/4000 [18:04<2:41:23,  2.98s/it]\u001b[A\u001b[A2024-08-01 12:45:59,078 - INFO - Backing off fetch(...) for 1.8s (TimeoutError)\n",
      "\n",
      "\n",
      " 19%|█▉        | 755/4000 [18:05<1:43:11,  1.91s/it]\u001b[A\u001b[A2024-08-01 12:46:01,076 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "2024-08-01 12:46:01,079 - INFO - Backing off fetch(...) for 1.5s (TimeoutError)\n",
      "2024-08-01 12:46:02,077 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:46:02,082 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:46:02,086 - INFO - Backing off fetch(...) for 0.6s (TimeoutError)\n",
      "2024-08-01 12:46:02,090 - INFO - Backing off fetch(...) for 1.4s (TimeoutError)\n",
      "2024-08-01 12:46:02,094 - INFO - Backing off fetch(...) for 0.2s (TimeoutError)\n",
      "2024-08-01 12:46:03,076 - INFO - Backing off fetch(...) for 1.2s (TimeoutError)\n",
      "2024-08-01 12:46:03,078 - INFO - Backing off fetch(...) for 0.4s (TimeoutError)\n",
      "2024-08-01 12:46:03,080 - INFO - Backing off fetch(...) for 1.2s (TimeoutError)\n",
      "2024-08-01 12:46:06,075 - INFO - Backing off fetch(...) for 1.2s (TimeoutError)\n",
      "2024-08-01 12:46:07,077 - INFO - Backing off fetch(...) for 1.5s (TimeoutError)\n",
      "2024-08-01 12:46:07,080 - INFO - Backing off fetch(...) for 1.4s (TimeoutError)\n",
      "2024-08-01 12:46:11,076 - INFO - Backing off fetch(...) for 1.4s (TimeoutError)\n",
      "2024-08-01 12:46:14,076 - INFO - Backing off fetch(...) for 0.5s (TimeoutError)\n",
      "2024-08-01 12:46:14,078 - INFO - Backing off fetch(...) for 1.6s (TimeoutError)\n",
      "2024-08-01 12:46:17,075 - INFO - Backing off fetch(...) for 1.0s (TimeoutError)\n",
      "2024-08-01 12:46:18,079 - INFO - Backing off fetch(...) for 0.9s (TimeoutError)\n",
      "2024-08-01 12:46:19,075 - INFO - Backing off fetch(...) for 1.2s (TimeoutError)\n",
      "2024-08-01 12:46:21,075 - INFO - Backing off fetch(...) for 0.3s (TimeoutError)\n",
      "2024-08-01 12:46:22,077 - INFO - Backing off fetch(...) for 1.6s (TimeoutError)\n",
      "2024-08-01 12:46:24,076 - ERROR - Giving up fetch(...) after 2 tries (TimeoutError)\n",
      "2024-08-01 12:46:24,081 - ERROR - Giving up fetch(...) after 2 tries (TimeoutError)\n",
      "2024-08-01 12:46:24,086 - ERROR - Giving up fetch(...) after 2 tries (TimeoutError)\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(queries))\n\u001b[0;32m---> 97\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# For environments like Jupyter\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[22], line 62\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(queries, url, pbar)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m     60\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(process_item(session, url, \u001b[38;5;28mid\u001b[39m, headers, param, semaphore, pbar))\n\u001b[0;32m---> 62\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (mrr_increment, count), (param, \u001b[38;5;28mid\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, queries):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mrr_increment \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(session, url, id, headers, params, semaphore, pbar)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_item\u001b[39m(session, url, \u001b[38;5;28mid\u001b[39m, headers, params, semaphore, pbar):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fetch(session, url, params, headers, semaphore)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ClientResponseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/backoff/_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    148\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    153\u001b[0m     giveup_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m giveup(e)\n",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m, in \u001b[0;36mfetch\u001b[0;34m(session, url, params, headers, semaphore)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@backoff\u001b[39m\u001b[38;5;241m.\u001b[39mon_exception(\n\u001b[1;32m      9\u001b[0m     backoff\u001b[38;5;241m.\u001b[39mexpo, \n\u001b[1;32m     10\u001b[0m     (aiohttp\u001b[38;5;241m.\u001b[39mClientError, aiohttp\u001b[38;5;241m.\u001b[39mhttp_exceptions\u001b[38;5;241m.\u001b[39mHttpProcessingError, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(session, url, params, headers, semaphore):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, ssl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m                 response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raises an exception for 4XX/5XX status codes\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:1167\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client.py:586\u001b[0m, in \u001b[0;36mClientSession._request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[1;32m    584\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m req\u001b[38;5;241m.\u001b[39msend(conn)\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstart(conn)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     resp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/client_reqrep.py:900\u001b[0m, in \u001b[0;36mClientResponse.start\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mprotocol\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;241m=\u001b[39m connection\n\u001b[0;32m--> 900\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# read response\u001b[39;49;00m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/aiohttp/helpers.py:725\u001b[0m, in \u001b[0;36mTimerContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 12:46:24,915 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,936 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,937 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,939 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,940 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,943 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,947 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,949 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,950 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,954 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,955 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,957 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,959 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,961 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,962 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,964 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,965 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,966 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,967 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,969 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,971 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,972 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,974 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,979 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,981 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,982 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,984 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,990 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,992 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,995 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,998 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:24,999 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,000 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,001 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,005 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,007 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,010 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,016 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,017 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,018 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,019 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,021 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,022 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,024 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,028 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,030 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,031 - ERROR - Giving up fetch(...) after 2 tries (aiohttp.client_exceptions.ClientOSError: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2706))\n",
      "2024-08-01 12:46:25,137 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientConnectionError: Connector is closed.)\n",
      "2024-08-01 12:46:25,171 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientConnectionError: Connector is closed.)\n",
      "2024-08-01 12:46:25,173 - ERROR - Giving up fetch(...) after 3 tries (aiohttp.client_exceptions.ClientConnectionError: Connector is closed.)\n"
     ]
    }
   ],
   "source": [
    "## QUERY CORRETTA CON FUZZYYYY\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "sample_size = 4000\n",
    "queries = random.sample(queries, sample_size)\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=10, \n",
    "    max_time=600\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, ssl=False, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "        \n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    #print(\"after call\")\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R4: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R4: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query\n",
    "Coverage of R4: 0.94025\n",
    "\n",
    "Measure Reciprocal Rank of R4: 0.9150119999999621"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardTableR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/HardTablesR3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HT3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(HT3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = HT3_sorted_mentions[:q1_idx]\n",
    "q2 = HT3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = HT3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = HT3_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "HT3_sample_keys = []\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q1, sample_size)\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q2, sample_size)\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q3, sample_size)\n",
    "HT3_sample_keys = HT3_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in HT3_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "cta_file = './data/Dataset/Dataset/HardTablesR2/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    pattern = r'^\\.'\n",
    "    if re.match(pattern, table):\n",
    "        continue\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./data/HT3_ner_type.json\"\n",
    "\n",
    "# Save the sorted_mentions dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(ner_type, json_file, indent=4)\n",
    "\n",
    "print(f\"Sorted mentions saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HardTableR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/HardTablesR2_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HT2_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(HT2_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = HT2_sorted_mentions[:q1_idx]\n",
    "q2 = HT2_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = HT2_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = HT2_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "HT2_sample_keys = []\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q1, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q2, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q3, sample_size)\n",
    "HT2_sample_keys = HT2_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in HT2_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tables_path = \"./data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "pattern = r'^\\.'\n",
    "\n",
    "# Create a list of file paths, excluding files that start with a dot\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"./data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = './data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "cta_file = './data/Dataset/Dataset/HardTablesR2/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    pattern = r'^\\.'\n",
    "    if re.match(pattern, table):\n",
    "        continue\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(name, value):\n",
    "    if value is not None:\n",
    "          ### SOFT FILTERING CONTSTRAINT\n",
    "        #params = {\n",
    "        #    'name': name,\n",
    "        #    'token': 'lamapi_demo_2023',\n",
    "        #    'kg': 'wikidata',\n",
    "        #    'limit': 1000,\n",
    "        #    'query': f'''\n",
    "        #        {{\n",
    "        #            \"query\": {{\n",
    "        #                \"bool\": {{\n",
    "        #                    \"must\": [\n",
    "        #                        {{\n",
    "        #                            \"match\": {{\n",
    "        #                                \"name\": {{\n",
    "        #                                    \"query\": \"{name}\",\n",
    "        #                                    \"boost\": 2.0\n",
    "        #                                }}\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ],\n",
    "        #                    \"should\": [\n",
    "        #                        {{\n",
    "        #                            \"term\": {{\n",
    "        #                                \"NERtype\": \"{value[1]}\"\n",
    "        #                            }}\n",
    "        #                        }}\n",
    "        #                    ]\n",
    "        #                }}\n",
    "        #            }}\n",
    "        #        }}\n",
    "        #        ''',\n",
    "        #    'sort': [\n",
    "        #        f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "        #    ]\n",
    "        #}\n",
    "    \n",
    "        ### HARD FILTERING CONTSTRAINT\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'''\n",
    "                {{\n",
    "                    \"query\": {{\n",
    "                        \"bool\": {{\n",
    "                            \"must\": [\n",
    "                                {{\n",
    "                                    \"match\": {{\n",
    "                                        \"name\": {{\n",
    "                                            \"query\": \"{name}\",\n",
    "                                            \"boost\": 2.0\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }},\n",
    "                                {{\n",
    "                                    \"term\": {{\n",
    "                                        \"NERtype\": \"{value[1]}\"\n",
    "                                    }}\n",
    "                                }}\n",
    "                            ]\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                ''',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}',\n",
    "            'sort': [\n",
    "                f'''{{\"popularity\": {{\"order\": \"desc\"}}}}'''\n",
    "            ]\n",
    "        }\n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_id = key_to_cell[key][1]\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        match = re.search(r'Q(\\d+)$', q_id)\n",
    "        if match:\n",
    "            queries.append((query, match[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "import nest_asyncio\n",
    "import backoff\n",
    "from tqdm import tqdm\n",
    "from aiohttp import ClientResponseError\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# URL and sample size\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "sample_size = 4000\n",
    "\n",
    "# Generate sample queries\n",
    "queries = random.sample(queries, sample_size)\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{params}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)  # No need to await here\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id in queries:\n",
    "            # Convert numpy int64 to standard Python int\n",
    "            param = {k: int(v) if isinstance(v, (np.int64, np.int32)) else v for k, v in param.items()}\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(\"fuzzy\")\n",
    "        \n",
    "        for (mrr_increment, count), (param, id) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                name = param['name']\n",
    "                param['query'] = f'{{\"query\": {{\"bool\": {{\"should\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0, \"fuzziness\": \"AUTO\"}}}}}}]}}}}}}'\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)  # No need to await here\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                            \n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of HT2: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of HT2: {m_mrr / len(queries)}\")\n",
    "\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
