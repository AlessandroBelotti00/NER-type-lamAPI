{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = int(len(R1_sorted_mentions)/40)  \n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:01<00:00, 32.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PER\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "        {{\n",
    "            \"query\": {{\n",
    "                \"bool\": {{\n",
    "                    \"must\": [\n",
    "                        {{\n",
    "                            \"match\": {{\n",
    "                                \"name\": {{\n",
    "                                    \"query\": \"{name}\",\n",
    "                                    \"boost\": 2.0\n",
    "                                }}\n",
    "                            }}\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"term\": {{\n",
    "                                \"NERtype\": \"{value[1]}\"\n",
    "                            }}\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        '''\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the filtering\n",
    "Coverage of R1: 0.828099173553719\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.7993801652892593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = int(len(R4_2T_sorted_mentions)/40) \n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SPARQLWrapper\n",
      "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
      "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
      "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: isodate, rdflib, SPARQLWrapper\n",
      "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-7.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPARQL queries: 100%|██████████| 540/540 [00:08<00:00, 60.92it/s]\n",
      "100%|██████████| 180/180 [00:33<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Q17334923\", # Place\n",
    "        \"Q486972\",   # PopulatedPlace\n",
    "        \"Q515\",      # City\n",
    "        \"Q6256\",     # Country\n",
    "        \"Q82794\",    # Region\n",
    "        \"Q8502\",     # Mountain\n",
    "        \"Q23442\",    # Island\n",
    "        \"Q23397\",    # Lake\n",
    "        \"Q4022\",     # River\n",
    "        \"Q22698\",    # Park\n",
    "        \"Q41176\",    # Building\n",
    "        \"Q839954\",   # HistoricPlace\n",
    "        \"Q4989906\",  # Monument\n",
    "        \"Q12280\",    # Bridge\n",
    "        \"Q34442\",    # Road\n",
    "        \"Q1248784\"   # Airport\n",
    "    ],\n",
    "    \"PER\": [\n",
    "        \"Q215627\",   # Person\n",
    "        \"Q483501\",   # Artist\n",
    "        \"Q2066131\",  # Athlete\n",
    "        \"Q82955\",    # Politician\n",
    "        \"Q901\",      # Scientist\n",
    "        \"Q36180\",    # Writer\n",
    "        \"Q33999\",    # Actor\n",
    "        \"Q639669\",   # Musician\n",
    "        \"Q6581097\",  # MilitaryPerson\n",
    "        \"Q947873\",   # Religious\n",
    "        \"Q12078\",    # Royalty\n",
    "        \"Q1456951\"   # Criminal\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Q43229\",    # Organisation\n",
    "        \"Q783794\",   # Company\n",
    "        \"Q3918\",     # EducationalInstitution\n",
    "        \"Q7278\",     # PoliticalParty\n",
    "        \"Q12973014\", # SportsTeam\n",
    "        \"Q163740\",   # Non-ProfitOrganisation\n",
    "        \"Q327333\",   # GovernmentAgency\n",
    "        \"Q1969448\",  # ReligiousOrganisation\n",
    "        \"Q215380\",   # Band\n",
    "        \"Q7075\",     # Library\n",
    "        \"Q33506\",    # Museum\n",
    "        \"Q16917\",    # Hospital\n",
    "        \"Q3918\",     # University (same as EducationalInstitution)\n",
    "        \"Q216052\"    # TradeUnion\n",
    "    ]\n",
    "}\n",
    "\n",
    "def get_item_root(mapping, id_list):\n",
    "    # Flatten the list of target IDs\n",
    "    target_ids = [item for sublist in mapping.values() for item in sublist]\n",
    "    \n",
    "    # Define the SPARQL endpoint\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    # Define the SPARQL query\n",
    "    id_list_str = \" \".join(\"wd:\" + id_ for id_ in id_list)\n",
    "    target_ids_str = \" \".join(\"wd:\" + tid for tid in target_ids)\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?class WHERE {\n",
    "      VALUES ?item { %s }\n",
    "      ?item wdt:P279* ?class.\n",
    "      VALUES ?class { %s }\n",
    "    }\n",
    "    \"\"\" % (id_list_str, target_ids_str)\n",
    "    \n",
    "    # Set the query and request JSON response\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    # Execute the query and convert the result to JSON\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Extract the classes from the results\n",
    "    id_to_root_class = {}\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        item_id = result[\"item\"][\"value\"].split('/')[-1]\n",
    "        root_class = result[\"class\"][\"value\"].split('/')[-1]\n",
    "        id_to_root_class[item_id] = root_class\n",
    "    \n",
    "    return id_to_root_class\n",
    "\n",
    "def get_item_root_batch(mapping, ids, batch_size=50):\n",
    "    root_classes = {}\n",
    "    with tqdm(total=len(ids), desc=\"SPARQL queries\") as pbar:\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            batch_ids = ids[i:i+batch_size]\n",
    "            batch_root_classes = get_item_root(mapping, batch_ids)\n",
    "            root_classes.update(batch_root_classes)\n",
    "            pbar.update(len(batch_ids))\n",
    "    return [root_classes.get(id_, None) for id_ in ids]\n",
    "\n",
    "# Assuming mapping is defined elsewhere in your code\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "types = df[2].astype(str).str.split(' ')\n",
    "ids = []\n",
    "for el in types:\n",
    "    ids.append(el[0].split('/')[-1])\n",
    "root_classes = get_item_root_batch(mapping, ids)\n",
    "\n",
    "# Map root classes to categories\n",
    "root_categories = [map_class_to_category(root_id) for root_id in root_classes]\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668/668 [01:53<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"].split(\" \")[0] in q_ids.values():\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
