{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n",
      "Collecting SPARQLWrapper\n",
      "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
      "  Downloading rdflib-7.0.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n",
      "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: isodate, rdflib, SPARQLWrapper\n",
      "Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-7.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install backoff\n",
    "! pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round1_T2D_f3_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R1_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R1_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R1_sorted_mentions[:q1_idx]\n",
    "q2 = R1_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R1_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R1_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = int(len(R1_sorted_mentions)/40)  \n",
    "R1_sample_keys = []\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q1, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q2, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q3, sample_size)\n",
    "R1_sample_keys = R1_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R1_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:01<00:00, 57.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round1_T2D/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round1_T2D/gt/CTA_Round1_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Place\", \"PopulatedPlace\", \"City\", \"Country\", \"Region\", \"Mountain\", \"Island\", \"Lake\", \"River\",\n",
    "        \"Park\", \"Building\", \"HistoricPlace\", \"Monument\", \"Bridge\", \"Road\", \"Airport\"\n",
    "    ],\n",
    "    \"PER\": [\n",
    "        \"Person\", \"Artist\", \"Athlete\", \"Politician\", \"Scientist\", \"Writer\", \"Actor\", \"Musician\", \"MilitaryPerson\",\n",
    "        \"Religious\", \"Royalty\", \"Criminal\"\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Organisation\", \"Company\", \"EducationalInstitution\", \"PoliticalParty\", \"SportsTeam\", \"Non-ProfitOrganisation\",\n",
    "        \"GovernmentAgency\", \"ReligiousOrganisation\", \"Band\", \"Library\", \"Museum\", \"Hospital\", \"University\", \"TradeUnion\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n",
      "607 elements and founds: 853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round1_T2D/gt/CEA_Round1_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "count = 0\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            count += 1\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 14864/15222 [13:52<00:20, 17.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.9764814084877151\n",
      "Measure Reciprocal Rank of R1: 0.9306191696227784\n"
     ]
    }
   ],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    \n",
    "    if value[1] != \"OTHERS\":\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query' : f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        ],\n",
    "                        \"should\": [\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        '''\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}'\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "        #print(f\"{name} NOT FOUND-->t{item}\")\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the filtering\n",
    "Coverage of R1: 0.9764814084877151\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.930619169622778443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round3_2019_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R3_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R3_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R3_sorted_mentions[:q1_idx]\n",
    "q2 = R3_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R3_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R3_sorted_mentions[q3_idx:]\n",
    "\n",
    "\n",
    "sample_size = int(len(R3_sorted_mentions)/40) \n",
    "R3_sample_keys = []\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q1, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q2, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q3, sample_size)\n",
    "R3_sample_keys = R3_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R3_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2162/2162 [05:32<00:00,  6.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round3_2019/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round3_2019/gt/CTA_Round3_gt.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1]            \n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PER\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = category_list\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [07:15<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round3_2019/gt/CEA_Round3_gt_WD.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 14836/15233 [23:25<00:37, 10.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of R1: 0.9739381605724414\n",
      "Measure Reciprocal Rank of R1: 0.9310733932907487\n"
     ]
    }
   ],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    if value[1] != \"OTHERS\":\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query' : f'''\n",
    "            {{\n",
    "                \"query\": {{\n",
    "                    \"bool\": {{\n",
    "                        \"must\": [\n",
    "                            {{\n",
    "                                \"match\": {{\n",
    "                                    \"name\": {{\n",
    "                                        \"query\": \"{name}\",\n",
    "                                        \"boost\": 2.0\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        ],\n",
    "                        \"should\": [\n",
    "                            {{\n",
    "                                \"term\": {{\n",
    "                                    \"NERtype\": \"{value[1]}\"\n",
    "                                }}\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        '''\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': f'{{\"query\": {{\"bool\": {{\"must\": [{{\"match\": {{\"name\": {{\"query\": \"{name}\", \"boost\": 2.0}}}}}}]}}}}}}'\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R1: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R1: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage with the filtering\r",
    "Coverage of R1: 0.9739381605724414\n",
    "\r\n",
    "Measure Reciprocal Rank of R1: 0.9310733932907487\r\n",
    "6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2T_Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = int(len(R4_2T_sorted_mentions)/40) \n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Q17334923\",  # Place\n",
    "        \"Q486972\",    # PopulatedPlace\n",
    "        \"Q618123\",    # Geographical Feature\n",
    "        \"Q515\",       # City\n",
    "        \"Q6256\",      # Country\n",
    "        \"Q82794\",     # Region\n",
    "        \"Q8502\",      # Mountain\n",
    "        \"Q23442\",     # Island\n",
    "        \"Q23397\",     # Lake\n",
    "        \"Q4022\",      # River\n",
    "        \"Q22698\",     # Park\n",
    "        \"Q41176\",     # Building\n",
    "        \"Q839954\",    # HistoricPlace\n",
    "        \"Q4989906\",   # Monument\n",
    "        \"Q12280\",     # Bridge\n",
    "        \"Q34442\",     # Road\n",
    "        \"Q1248784\",   # Airport\n",
    "        \"Q13226383\",  # Residential area\n",
    "        \"Q13221722\",  # Industrial area\n",
    "        \"Q164238\",    # Forest\n",
    "        \"Q55488\",     # Desert\n",
    "        \"Q3065569\",   # Archaeological site\n",
    "        \"Q14752696\",  # Campus\n",
    "        \"Q1022083\",   # Tourist attraction\n",
    "        \"Q207694\",    # World Heritage Site\n",
    "        \"Q4989906\",   # Monument\n",
    "        \"Q2065736\",   # Memorial\n",
    "        \"Q10864048\",  # Castle\n",
    "        \"Q23413\",     # Palace\n",
    "        \"Q838948\",    # Estate\n",
    "    ],\n",
    "    \"PER\": [\n",
    "        \"Q215627\",    # Person\n",
    "        \"Q5\",         # Human\n",
    "        \"Q483501\",    # Artist\n",
    "        \"Q2066131\",   # Athlete\n",
    "        \"Q82955\",     # Politician\n",
    "        \"Q901\",       # Scientist\n",
    "        \"Q36180\",     # Writer\n",
    "        \"Q33999\",     # Actor\n",
    "        \"Q639669\",    # Musician\n",
    "        \"Q6581097\",   # MilitaryPerson\n",
    "        \"Q947873\",    # Religious\n",
    "        \"Q12078\",     # Royalty\n",
    "        \"Q1456951\",   # Criminal\n",
    "        \"Q36834\",     # Philosopher\n",
    "        \"Q49757\",     # Director\n",
    "        \"Q622425\",    # Producer\n",
    "        \"Q333634\",    # Businessperson\n",
    "        \"Q937857\",    # Entrepreneur\n",
    "        \"Q3282637\",   # Inventor\n",
    "        \"Q10871364\",  # Fashion designer\n",
    "        \"Q4834541\",   # Journalist\n",
    "        \"Q36180\",     # Poet\n",
    "        \"Q170790\",    # Diplomat\n",
    "        \"Q376799\",    # Chef\n",
    "        \"Q24238356\",  # Influencer\n",
    "        \"Q207694\",    # Celebrity\n",
    "        \"Q10864048\",  # Explorer\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Q43229\",     # Organisation\n",
    "        \"Q783794\",    # Company\n",
    "        \"Q3918\",      # EducationalInstitution\n",
    "        \"Q7278\",      # PoliticalParty\n",
    "        \"Q12973014\",  # SportsTeam\n",
    "        \"Q163740\",    # Non-ProfitOrganisation\n",
    "        \"Q327333\",    # GovernmentAgency\n",
    "        \"Q1969448\",   # ReligiousOrganisation\n",
    "        \"Q215380\",    # Band\n",
    "        \"Q7075\",      # Library\n",
    "        \"Q33506\",     # Museum\n",
    "        \"Q16917\",     # Hospital\n",
    "        \"Q216052\",    # TradeUnion\n",
    "        \"Q6501447\",   # Research Institute\n",
    "        \"Q163740\",    # Foundation\n",
    "        \"Q200695\",    # Bank\n",
    "        \"Q19317\",     # Corporation\n",
    "        \"Q875538\",    # Law firm\n",
    "        \"Q2234766\",   # Professional association\n",
    "        \"Q2088357\",   # Charity\n",
    "        \"Q163740\",    # NGO (Non-Governmental Organization)\n",
    "        \"Q484652\",    # Think tank\n",
    "        \"Q507619\",    # Multinational corporation\n",
    "        \"Q4438121\",   # Media company\n",
    "        \"Q156537\",    # Publishing company\n",
    "        \"Q11426\",     # Airline\n",
    "        \"Q980447\",    # Hotel chain\n",
    "        \"Q1131088\",   # Technology company\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "cta_keys = {}\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "\n",
    "category_list = []\n",
    "\n",
    "for row_idx in range(df.shape[0]):\n",
    "    col_idx = 2\n",
    "    while True:\n",
    "        try:\n",
    "            if pd.isna(df.iloc[row_idx,col_idx]):\n",
    "                category_list.append(\"OTHERS\")\n",
    "                break\n",
    "            urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        except IndexError as e:\n",
    "            category_list.append(\"OTHERS\")\n",
    "            break\n",
    "        \n",
    "        #print(f\"{df.iloc[row_idx,0]}->{cell_urls} @ {row_idx},{col_idx}\")\n",
    "        urls = df.iloc[row_idx,col_idx].split(' ')\n",
    "        find = False\n",
    "        for url in urls:\n",
    "            type = url.split('/')[-1]     \n",
    "            map_class_to_category(type)\n",
    "            if type == \"Person\":\n",
    "                category_list.append(\"PER\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Location\":\n",
    "                category_list.append(\"LOC\")\n",
    "                find = True\n",
    "                break\n",
    "            elif type == \"Organisation\":\n",
    "                category_list.append(\"ORG\")\n",
    "                find = True\n",
    "                break\n",
    "        if find:\n",
    "            break\n",
    "        \n",
    "        col_idx += 1\n",
    "\n",
    "type = df[2].astype(str).str.split('/').str[-1]\n",
    "df[\"category\"] = type.apply(map_class_to_category)\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPARQL queries: 100%|██████████| 540/540 [00:07<00:00, 76.41it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (46) does not match length of index (540)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 172\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Map root classes to categories\u001b[39;00m\n\u001b[1;32m    169\u001b[0m root_categories \u001b[38;5;241m=\u001b[39m [(map_class_to_category(root_id), id_) \u001b[38;5;28;01mfor\u001b[39;00m root_id, id_ \u001b[38;5;129;01min\u001b[39;00m root_classes]\n\u001b[0;32m--> 172\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m root_categories\n\u001b[1;32m    173\u001b[0m cta_keys \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    174\u001b[0m cta_keys[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m), df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4094\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4093\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4303\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4294\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4295\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4301\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4303\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4306\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4307\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4308\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4309\u001b[0m     ):\n\u001b[1;32m   4310\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:5042\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5042\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (46) does not match length of index (540)"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Q17334923\",  # Place\n",
    "        \"Q486972\",    # PopulatedPlace\n",
    "        \"Q618123\",    # Geographical Feature\n",
    "        \"Q515\",       # City\n",
    "        \"Q6256\",      # Country\n",
    "        \"Q82794\",     # Region\n",
    "        \"Q8502\",      # Mountain\n",
    "        \"Q23442\",     # Island\n",
    "        \"Q23397\",     # Lake\n",
    "        \"Q4022\",      # River\n",
    "        \"Q22698\",     # Park\n",
    "        \"Q41176\",     # Building\n",
    "        \"Q839954\",    # HistoricPlace\n",
    "        \"Q4989906\",   # Monument\n",
    "        \"Q12280\",     # Bridge\n",
    "        \"Q34442\",     # Road\n",
    "        \"Q1248784\",   # Airport\n",
    "        \"Q13226383\",  # Residential area\n",
    "        \"Q13221722\",  # Industrial area\n",
    "        \"Q164238\",    # Forest\n",
    "        \"Q55488\",     # Desert\n",
    "        \"Q3065569\",   # Archaeological site\n",
    "        \"Q14752696\",  # Campus\n",
    "        \"Q1022083\",   # Tourist attraction\n",
    "        \"Q207694\",    # World Heritage Site\n",
    "        \"Q4989906\",   # Monument\n",
    "        \"Q2065736\",   # Memorial\n",
    "        \"Q10864048\",  # Castle\n",
    "        \"Q23413\",     # Palace\n",
    "        \"Q838948\",    # Estate\n",
    "    ],\n",
    "    \"PER\": [\n",
    "        \"Q215627\",    # Person\n",
    "        \"Q5\",         # Human\n",
    "        \"Q483501\",    # Artist\n",
    "        \"Q2066131\",   # Athlete\n",
    "        \"Q82955\",     # Politician\n",
    "        \"Q901\",       # Scientist\n",
    "        \"Q36180\",     # Writer\n",
    "        \"Q33999\",     # Actor\n",
    "        \"Q639669\",    # Musician\n",
    "        \"Q6581097\",   # MilitaryPerson\n",
    "        \"Q947873\",    # Religious\n",
    "        \"Q12078\",     # Royalty\n",
    "        \"Q1456951\",   # Criminal\n",
    "        \"Q36834\",     # Philosopher\n",
    "        \"Q49757\",     # Director\n",
    "        \"Q622425\",    # Producer\n",
    "        \"Q333634\",    # Businessperson\n",
    "        \"Q937857\",    # Entrepreneur\n",
    "        \"Q3282637\",   # Inventor\n",
    "        \"Q10871364\",  # Fashion designer\n",
    "        \"Q4834541\",   # Journalist\n",
    "        \"Q36180\",     # Poet\n",
    "        \"Q170790\",    # Diplomat\n",
    "        \"Q376799\",    # Chef\n",
    "        \"Q24238356\",  # Influencer\n",
    "        \"Q207694\",    # Celebrity\n",
    "        \"Q10864048\",  # Explorer\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Q43229\",     # Organisation\n",
    "        \"Q783794\",    # Company\n",
    "        \"Q3918\",      # EducationalInstitution\n",
    "        \"Q7278\",      # PoliticalParty\n",
    "        \"Q12973014\",  # SportsTeam\n",
    "        \"Q163740\",    # Non-ProfitOrganisation\n",
    "        \"Q327333\",    # GovernmentAgency\n",
    "        \"Q1969448\",   # ReligiousOrganisation\n",
    "        \"Q215380\",    # Band\n",
    "        \"Q7075\",      # Library\n",
    "        \"Q33506\",     # Museum\n",
    "        \"Q16917\",     # Hospital\n",
    "        \"Q216052\",    # TradeUnion\n",
    "        \"Q6501447\",   # Research Institute\n",
    "        \"Q163740\",    # Foundation\n",
    "        \"Q200695\",    # Bank\n",
    "        \"Q19317\",     # Corporation\n",
    "        \"Q875538\",    # Law firm\n",
    "        \"Q2234766\",   # Professional association\n",
    "        \"Q2088357\",   # Charity\n",
    "        \"Q163740\",    # NGO (Non-Governmental Organization)\n",
    "        \"Q484652\",    # Think tank\n",
    "        \"Q507619\",    # Multinational corporation\n",
    "        \"Q4438121\",   # Media company\n",
    "        \"Q156537\",    # Publishing company\n",
    "        \"Q11426\",     # Airline\n",
    "        \"Q980447\",    # Hotel chain\n",
    "        \"Q1131088\",   # Technology company\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def get_item_root(mapping, id_list):\n",
    "    # Flatten the list of target IDs\n",
    "    target_ids = [item for sublist in mapping.values() for item in sublist]\n",
    "    \n",
    "    # Define the SPARQL endpoint\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    # Define the SPARQL query\n",
    "    id_list_str = \" \".join(\"wd:\" + id_ for id_ in id_list)\n",
    "    target_ids_str = \" \".join(\"wd:\" + tid for tid in target_ids)\n",
    "    query = \"\"\"\n",
    "        SELECT ?item ?class WHERE {\n",
    "          VALUES ?item { %s }\n",
    "          ?item wdt:P279* ?class.\n",
    "          VALUES ?class { %s }\n",
    "        }\n",
    "    \"\"\" % (id_list_str, target_ids_str)\n",
    "    \n",
    "    # Set the query and request JSON response\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    try:\n",
    "        # Execute the query and convert the result to JSON\n",
    "        results = sparql.query().convert()\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SPARQL query: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "    # Extract the classes from the results\n",
    "    id_to_root_class = {}\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        item_id = result[\"item\"][\"value\"].split('/')[-1]\n",
    "        root_class = result[\"class\"][\"value\"].split('/')[-1]\n",
    "        id_to_root_class[item_id] = root_class\n",
    "    \n",
    "    return id_to_root_class\n",
    "\n",
    "def get_item_root_batch(mapping, ids, batch_size=50):\n",
    "    root_classes = []\n",
    "    with tqdm(total=len(ids), desc=\"SPARQL queries\") as pbar:\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            batch_ids = ids[i:i+batch_size]\n",
    "            batch_root_classes = get_item_root(mapping, batch_ids)\n",
    "            root_classes.extend([(root_class, id_) for id_, root_class in batch_root_classes.items()])\n",
    "            pbar.update(len(batch_ids))\n",
    "    return root_classes\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "types = df[2].astype(str).str.split(' ')\n",
    "\n",
    "\n",
    "ids = []\n",
    "for el in types:\n",
    "    ids.append(el[0].split('/')[-1])\n",
    "root_classes = get_item_root_batch(mapping, list(ids))\n",
    "\n",
    "# Map root classes to categories\n",
    "root_categories = [(map_class_to_category(root_id), id_) for root_id, id_ in root_classes]\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                print(tmp_value)\n",
    "                break\n",
    "                key_to_cell[key] = (tmp_value, cta_keys[\"key\"][2].iloc[tmp_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00E2H310 0': 'OTHERS',\n",
       " '00E2H310 1': 'OTHERS',\n",
       " '00E2H310 2': 'OTHERS',\n",
       " '0AJSJYAL 0': 'OTHERS',\n",
       " '0D70DN48 0': 'OTHERS',\n",
       " '0D70DN48 2': 'OTHERS',\n",
       " '0D70DN48 3': 'OTHERS',\n",
       " '0H1C2CNE 0': 'OTHERS',\n",
       " '0IR0XIUW 0': 'OTHERS',\n",
       " '0IR0XIUW 1': 'ORG',\n",
       " '0IR0XIUW 2': 'OTHERS',\n",
       " '1C7N45JA 0': 'ORG',\n",
       " '1C7N45JA 1': 'OTHERS',\n",
       " '1C7N45JA 2': 'OTHERS',\n",
       " '1C7N45JA 3': 'OTHERS',\n",
       " '1C7N45JA 4': 'OTHERS',\n",
       " '1C7N45JA 5': 'OTHERS',\n",
       " '1C9LFOKN 0': 'OTHERS',\n",
       " '1C9LFOKN 1': 'OTHERS',\n",
       " '1MQL5T7F 0': 'OTHERS',\n",
       " '1MQL5T7F 1': 'OTHERS',\n",
       " '1MQL5T7F 5': 'OTHERS',\n",
       " '1MQL5T7F 7': 'OTHERS',\n",
       " '24W5SSRB 0': 'OTHERS',\n",
       " '24W5SSRB 2': 'OTHERS',\n",
       " '24W5SSRB 3': 'OTHERS',\n",
       " '24W5SSRB 4': 'OTHERS',\n",
       " '29BNEL1Q 0': 'OTHERS',\n",
       " '29BNEL1Q 1': 'OTHERS',\n",
       " '29BNEL1Q 2': 'ORG',\n",
       " '29BNEL1Q 3': 'LOC',\n",
       " '2BEBH437 0': 'OTHERS',\n",
       " '2BEBH437 1': 'OTHERS',\n",
       " '2BEBH437 2': 'OTHERS',\n",
       " '2EZKB5RU 0': 'OTHERS',\n",
       " '2EZKB5RU 1': 'OTHERS',\n",
       " '2EZKB5RU 2': 'OTHERS',\n",
       " '2VRS31OV 0': 'OTHERS',\n",
       " '2VRS31OV 1': 'OTHERS',\n",
       " '2VRS31OV 2': 'OTHERS',\n",
       " '2XHMV76G 1': 'OTHERS',\n",
       " '2XHMV76G 2': 'OTHERS',\n",
       " '3C3INQLM 0': 'OTHERS',\n",
       " '3C3INQLM 1': 'OTHERS',\n",
       " '3C3INQLM 2': 'OTHERS',\n",
       " '3C3INQLM 3': 'OTHERS',\n",
       " '3DOM5NIW 0': 'OTHERS',\n",
       " '3DOM5NIW 1': 'OTHERS',\n",
       " '3DOM5NIW 4': 'ORG',\n",
       " '3DOM5NIW 5': 'OTHERS',\n",
       " '3DOM5NIW 6': 'OTHERS',\n",
       " '3JXMPC7N 0': 'OTHERS',\n",
       " '3JXMPC7N 3': 'OTHERS',\n",
       " '3LG8J4MX 0': 'OTHERS',\n",
       " '3LG8J4MX 2': 'OTHERS',\n",
       " '3LG8J4MX 3': 'OTHERS',\n",
       " '3LG8J4MX 4': 'OTHERS',\n",
       " '3N6S2FCX 0': 'OTHERS',\n",
       " '3N6S2FCX 1': 'OTHERS',\n",
       " '3N6S2FCX 2': 'OTHERS',\n",
       " '3X6QY8Q0 0': 'OTHERS',\n",
       " '4EGV2QNO 0': 'OTHERS',\n",
       " '4EGV2QNO 1': 'OTHERS',\n",
       " '4EGV2QNO 3': 'OTHERS',\n",
       " '4FFI99VQ 0': 'OTHERS',\n",
       " '4FFI99VQ 3': 'OTHERS',\n",
       " '4FW66XNV 0': 'OTHERS',\n",
       " '4HP08FY9 0': 'OTHERS',\n",
       " '4HP08FY9 2': 'OTHERS',\n",
       " '4HP08FY9 3': 'OTHERS',\n",
       " '4J75OL3W 0': 'OTHERS',\n",
       " '4J75OL3W 2': 'ORG',\n",
       " '4J75OL3W 3': 'OTHERS',\n",
       " '4WMC1B70 0': 'OTHERS',\n",
       " '51MYHYDF 0': 'ORG',\n",
       " '51MYHYDF 1': 'LOC',\n",
       " '51MYHYDF 2': 'LOC',\n",
       " '54S3KOBO 0': 'OTHERS',\n",
       " '54S3KOBO 1': 'LOC',\n",
       " '54S3KOBO 2': 'OTHERS',\n",
       " '5DKX42VB 0': 'OTHERS',\n",
       " '5DKX42VB 1': 'OTHERS',\n",
       " '5HD27KI3 0': 'LOC',\n",
       " '5HD27KI3 3': 'OTHERS',\n",
       " '5HD27KI3 4': 'OTHERS',\n",
       " '5OLOTSKC 0': 'ORG',\n",
       " '5OLOTSKC 1': 'OTHERS',\n",
       " '5OLOTSKC 2': 'OTHERS',\n",
       " '5OLOTSKC 3': 'OTHERS',\n",
       " '5OLOTSKC 4': 'OTHERS',\n",
       " '5OLOTSKC 5': 'OTHERS',\n",
       " '5RV7WRO1 1': 'LOC',\n",
       " '5RV7WRO1 2': 'OTHERS',\n",
       " '60G94POT 0': 'OTHERS',\n",
       " '60G94POT 1': 'OTHERS',\n",
       " '60G94POT 4': 'ORG',\n",
       " '60G94POT 5': 'OTHERS',\n",
       " '60G94POT 6': 'OTHERS',\n",
       " '64OJP4G3 0': 'OTHERS',\n",
       " '64OJP4G3 1': 'OTHERS',\n",
       " '64OJP4G3 2': 'OTHERS',\n",
       " '6I7ET24J 0': 'OTHERS',\n",
       " '6I7ET24J 1': 'OTHERS',\n",
       " '6I7ET24J 2': 'OTHERS',\n",
       " '71SY0Z5S 1': 'OTHERS',\n",
       " '71SY0Z5S 3': 'OTHERS',\n",
       " '7EVLH0DV 0': 'ORG',\n",
       " '7EVLH0DV 1': 'OTHERS',\n",
       " '7EVLH0DV 4': 'OTHERS',\n",
       " '7GOG4IKF 0': 'OTHERS',\n",
       " '7QNYGYI7 0': 'OTHERS',\n",
       " '7QNYGYI7 1': 'OTHERS',\n",
       " '7QNYGYI7 2': 'OTHERS',\n",
       " '7VVP9YIF 0': 'OTHERS',\n",
       " '7VVP9YIF 1': 'OTHERS',\n",
       " '7VVP9YIF 2': 'OTHERS',\n",
       " '8D8CVBT0 0': 'OTHERS',\n",
       " '8D8CVBT0 1': 'OTHERS',\n",
       " '8D8CVBT0 2': 'LOC',\n",
       " '8MKQXO18 0': 'OTHERS',\n",
       " '8MKQXO18 1': 'LOC',\n",
       " '8MKQXO18 3': 'OTHERS',\n",
       " '8MKQXO18 4': 'OTHERS',\n",
       " '8QA9EYPI 0': 'OTHERS',\n",
       " '8QA9EYPI 1': 'OTHERS',\n",
       " '8QA9EYPI 2': 'OTHERS',\n",
       " '8R6ZM8HE 0': 'OTHERS',\n",
       " '8R6ZM8HE 1': 'OTHERS',\n",
       " '8R6ZM8HE 2': 'OTHERS',\n",
       " '8YINKYDR 0': 'OTHERS',\n",
       " '8YINKYDR 1': 'OTHERS',\n",
       " '8YINKYDR 2': 'OTHERS',\n",
       " '9BGE5Y4L 0': 'OTHERS',\n",
       " '9BGE5Y4L 1': 'OTHERS',\n",
       " '9BGE5Y4L 2': 'LOC',\n",
       " '9CSNI7FS 0': 'OTHERS',\n",
       " '9CSNI7FS 2': 'OTHERS',\n",
       " '9CSNI7FS 3': 'OTHERS',\n",
       " '9CSNI7FS 4': 'OTHERS',\n",
       " '9HIRS45A 0': 'OTHERS',\n",
       " '9HIRS45A 1': 'OTHERS',\n",
       " '9HIRS45A 2': 'OTHERS',\n",
       " '9JA43FJL 1': 'OTHERS',\n",
       " '9JA43FJL 2': 'OTHERS',\n",
       " '9JA43FJL 6': 'OTHERS',\n",
       " '9XAJBIJV 0': 'OTHERS',\n",
       " '9XAJBIJV 2': 'OTHERS',\n",
       " '9XAJBIJV 3': 'OTHERS',\n",
       " '9XAJBIJV 4': 'OTHERS',\n",
       " '9Y8MPU2Q 0': 'OTHERS',\n",
       " '9Y8MPU2Q 1': 'OTHERS',\n",
       " '9Y8MPU2Q 4': 'ORG',\n",
       " '9Y8MPU2Q 5': 'OTHERS',\n",
       " '9Y8MPU2Q 6': 'OTHERS',\n",
       " 'A5TITKIN 0': 'OTHERS',\n",
       " 'AN306FAH 0': 'OTHERS',\n",
       " 'AN306FAH 2': 'OTHERS',\n",
       " 'AN306FAH 3': 'OTHERS',\n",
       " 'AN306FAH 4': 'OTHERS',\n",
       " 'ASNHTAOS 1': 'LOC',\n",
       " 'ASNHTAOS 2': 'OTHERS',\n",
       " 'AUU9A6KL 0': 'OTHERS',\n",
       " 'AUU9A6KL 1': 'OTHERS',\n",
       " 'AUU9A6KL 5': 'OTHERS',\n",
       " 'AUU9A6KL 7': 'OTHERS',\n",
       " 'B20WIQKU 0': 'OTHERS',\n",
       " 'B20WIQKU 1': 'OTHERS',\n",
       " 'B20WIQKU 2': 'OTHERS',\n",
       " 'B38A9Q5R 0': 'OTHERS',\n",
       " 'B38A9Q5R 3': 'OTHERS',\n",
       " 'B7ILPH0Y 0': 'OTHERS',\n",
       " 'B7ILPH0Y 1': 'OTHERS',\n",
       " 'B7ILPH0Y 3': 'OTHERS',\n",
       " 'BDH3WFGJ 0': 'ORG',\n",
       " 'BDH3WFGJ 1': 'LOC',\n",
       " 'BDH3WFGJ 2': 'LOC',\n",
       " 'BID0NRU0 0': 'OTHERS',\n",
       " 'BID0NRU0 1': 'OTHERS',\n",
       " 'BLBITS6S 0': 'OTHERS',\n",
       " 'BLBITS6S 2': 'ORG',\n",
       " 'BLBITS6S 3': 'OTHERS',\n",
       " 'BN8FOG7F 0': 'OTHERS',\n",
       " 'CAF0E2JH 0': 'OTHERS',\n",
       " 'CAF0E2JH 2': 'OTHERS',\n",
       " 'CAF0E2JH 3': 'OTHERS',\n",
       " 'CAF0E2JH 4': 'OTHERS',\n",
       " 'CAF0E2JH 5': 'OTHERS',\n",
       " 'CAF0E2JH 6': 'OTHERS',\n",
       " 'CHZGO92A 0': 'OTHERS',\n",
       " 'CHZGO92A 2': 'OTHERS',\n",
       " 'CHZGO92A 3': 'OTHERS',\n",
       " 'CHZGO92A 4': 'OTHERS',\n",
       " 'CHZGO92A 5': 'OTHERS',\n",
       " 'CHZGO92A 6': 'OTHERS',\n",
       " 'CNQ5Z0BG 0': 'OTHERS',\n",
       " 'CNQ5Z0BG 2': 'OTHERS',\n",
       " 'CNQ5Z0BG 3': 'LOC',\n",
       " 'CNQ5Z0BG 4': 'OTHERS',\n",
       " 'CQH26T15 0': 'OTHERS',\n",
       " 'CQH26T15 1': 'OTHERS',\n",
       " 'CQH26T15 3': 'OTHERS',\n",
       " 'CXBCCWXI 0': 'OTHERS',\n",
       " 'CXBCCWXI 1': 'LOC',\n",
       " 'CXBCCWXI 2': 'OTHERS',\n",
       " 'DRSJ0SOG 0': 'OTHERS',\n",
       " 'DRSJ0SOG 2': 'OTHERS',\n",
       " 'DRSJ0SOG 3': 'OTHERS',\n",
       " 'DRSJ0SOG 4': 'OTHERS',\n",
       " 'DRSJ0SOG 5': 'OTHERS',\n",
       " 'DRSJ0SOG 6': 'OTHERS',\n",
       " 'DXTA1MV8 0': 'OTHERS',\n",
       " 'DXTA1MV8 1': 'LOC',\n",
       " 'DXTA1MV8 3': 'OTHERS',\n",
       " 'DXTA1MV8 4': 'OTHERS',\n",
       " 'E22XXKVQ 0': 'OTHERS',\n",
       " 'E22XXKVQ 1': 'OTHERS',\n",
       " 'E22XXKVQ 2': 'OTHERS',\n",
       " 'E22XXKVQ 3': 'OTHERS',\n",
       " 'E3ZK744Q 0': 'OTHERS',\n",
       " 'E3ZK744Q 1': 'OTHERS',\n",
       " 'E3ZK744Q 2': 'OTHERS',\n",
       " 'E3ZK744Q 4': 'OTHERS',\n",
       " 'E45F8QW2 0': 'OTHERS',\n",
       " 'E45F8QW2 1': 'OTHERS',\n",
       " 'E45F8QW2 2': 'ORG',\n",
       " 'E45F8QW2 3': 'LOC',\n",
       " 'EQZ21058 0': 'OTHERS',\n",
       " 'EQZ21058 2': 'OTHERS',\n",
       " 'EQZ21058 3': 'OTHERS',\n",
       " 'ET9REW9Y 0': 'OTHERS',\n",
       " 'ET9REW9Y 1': 'OTHERS',\n",
       " 'ET9REW9Y 3': 'OTHERS',\n",
       " 'EV6LDIB8 0': 'OTHERS',\n",
       " 'EV6LDIB8 1': 'OTHERS',\n",
       " 'F98SUVJH 0': 'OTHERS',\n",
       " 'F98SUVJH 1': 'OTHERS',\n",
       " 'F98SUVJH 3': 'OTHERS',\n",
       " 'FF00TEZG 0': 'OTHERS',\n",
       " 'FF00TEZG 1': 'OTHERS',\n",
       " 'FVKKTA8O 1': 'OTHERS',\n",
       " 'FVKKTA8O 2': 'OTHERS',\n",
       " 'FVKKTA8O 3': 'OTHERS',\n",
       " 'FVKKTA8O 4': 'OTHERS',\n",
       " 'FWSGRDQ3 0': 'OTHERS',\n",
       " 'FWSGRDQ3 1': 'OTHERS',\n",
       " 'FWSGRDQ3 2': 'OTHERS',\n",
       " 'FWSGRDQ3 3': 'OTHERS',\n",
       " 'GINQPZQC 0': 'OTHERS',\n",
       " 'GINQPZQC 1': 'OTHERS',\n",
       " 'GNDO9OXJ 0': 'ORG',\n",
       " 'GNDO9OXJ 1': 'OTHERS',\n",
       " 'GNDO9OXJ 2': 'OTHERS',\n",
       " 'GNDO9OXJ 3': 'OTHERS',\n",
       " 'GNDO9OXJ 4': 'OTHERS',\n",
       " 'GNDO9OXJ 5': 'OTHERS',\n",
       " 'GY7KNULP 0': 'OTHERS',\n",
       " 'GY7KNULP 1': 'OTHERS',\n",
       " 'GY7KNULP 2': 'OTHERS',\n",
       " 'GY7KNULP 3': 'OTHERS',\n",
       " 'HB00DX4L 0': 'OTHERS',\n",
       " 'HB00DX4L 1': 'OTHERS',\n",
       " 'HBKGJQEY 1': 'OTHERS',\n",
       " 'HBKGJQEY 2': 'OTHERS',\n",
       " 'HCLZSPEJ 0': 'OTHERS',\n",
       " 'HCLZSPEJ 1': 'OTHERS',\n",
       " 'HGIUTSCG 0': 'OTHERS',\n",
       " 'HGIUTSCG 2': 'OTHERS',\n",
       " 'HGIUTSCG 3': 'LOC',\n",
       " 'HGIUTSCG 4': 'OTHERS',\n",
       " 'HGYG2DVU 0': 'OTHERS',\n",
       " 'HGYG2DVU 1': 'OTHERS',\n",
       " 'HLJ9HHEE 0': 'OTHERS',\n",
       " 'HLJ9HHEE 1': 'OTHERS',\n",
       " 'HLJ9HHEE 2': 'OTHERS',\n",
       " 'HLJ9HHEE 3': 'OTHERS',\n",
       " 'HQCEC5NO 0': 'OTHERS',\n",
       " 'HQCEC5NO 1': 'OTHERS',\n",
       " 'HQRATPBV 0': 'OTHERS',\n",
       " 'HTF6MVY9 0': 'OTHERS',\n",
       " 'HTF6MVY9 1': 'OTHERS',\n",
       " 'I0ENR9U5 0': 'OTHERS',\n",
       " 'I0ENR9U5 1': 'OTHERS',\n",
       " 'I0ENR9U5 2': 'OTHERS',\n",
       " 'I0ENR9U5 3': 'OTHERS',\n",
       " 'I6BBMPNU 0': 'OTHERS',\n",
       " 'I6BBMPNU 1': 'OTHERS',\n",
       " 'I6BBMPNU 2': 'OTHERS',\n",
       " 'I6BBMPNU 4': 'OTHERS',\n",
       " 'I847EKDH 0': 'OTHERS',\n",
       " 'I847EKDH 1': 'OTHERS',\n",
       " 'I847EKDH 3': 'OTHERS',\n",
       " 'IP6ZRIGH 0': 'OTHERS',\n",
       " 'IP6ZRIGH 1': 'LOC',\n",
       " 'IP6ZRIGH 2': 'OTHERS',\n",
       " 'IPTLCCCU 0': 'OTHERS',\n",
       " 'IPTLCCCU 1': 'OTHERS',\n",
       " 'IPTLCCCU 2': 'OTHERS',\n",
       " 'IPTLCCCU 3': 'OTHERS',\n",
       " 'IYZYF533 0': 'OTHERS',\n",
       " 'IYZYF533 1': 'OTHERS',\n",
       " 'IYZYF533 2': 'OTHERS',\n",
       " 'IYZYF533 3': 'OTHERS',\n",
       " 'IZF82AX9 0': 'OTHERS',\n",
       " 'IZF82AX9 1': 'OTHERS',\n",
       " 'J9EJV2S3 0': 'OTHERS',\n",
       " 'J9EJV2S3 1': 'OTHERS',\n",
       " 'JA4L7KWX 1': 'OTHERS',\n",
       " 'JA4L7KWX 3': 'OTHERS',\n",
       " 'JMNRCW4E 0': 'OTHERS',\n",
       " 'JMNRCW4E 1': 'OTHERS',\n",
       " 'JN0T2O5I 0': 'OTHERS',\n",
       " 'JN0T2O5I 1': 'OTHERS',\n",
       " 'JZ22O0DD 0': 'OTHERS',\n",
       " 'JZ22O0DD 2': 'ORG',\n",
       " 'JZLRN9PL 1': 'OTHERS',\n",
       " 'JZLRN9PL 2': 'OTHERS',\n",
       " 'JZLRN9PL 3': 'OTHERS',\n",
       " 'JZLRN9PL 4': 'OTHERS',\n",
       " 'K2VEUQT0 0': 'OTHERS',\n",
       " 'K2VEUQT0 1': 'OTHERS',\n",
       " 'K3X3L22Y 0': 'OTHERS',\n",
       " 'K3X3L22Y 1': 'OTHERS',\n",
       " 'K3X3L22Y 2': 'OTHERS',\n",
       " 'K3X3L22Y 3': 'OTHERS',\n",
       " 'K3X3L22Y 5': 'OTHERS',\n",
       " 'K71YYDXO 0': 'OTHERS',\n",
       " 'K71YYDXO 1': 'LOC',\n",
       " 'K71YYDXO 2': 'LOC',\n",
       " 'K71YYDXO 4': 'OTHERS',\n",
       " 'K71YYDXO 5': 'OTHERS',\n",
       " 'KOQM4YU9 0': 'OTHERS',\n",
       " 'KOQM4YU9 1': 'OTHERS',\n",
       " 'KOQM4YU9 3': 'OTHERS',\n",
       " 'KUGCH9I3 0': 'OTHERS',\n",
       " 'KUGCH9I3 2': 'LOC',\n",
       " 'KUGCH9I3 3': 'OTHERS',\n",
       " 'KUGCH9I3 4': 'OTHERS',\n",
       " 'L2MCKOWX 0': 'OTHERS',\n",
       " 'L2MCKOWX 1': 'OTHERS',\n",
       " 'L2MCKOWX 4': 'OTHERS',\n",
       " 'LV5N8XDB 0': 'OTHERS',\n",
       " 'LV5N8XDB 2': 'ORG',\n",
       " 'M56G7E6D 0': 'OTHERS',\n",
       " 'M56G7E6D 1': 'OTHERS',\n",
       " 'M56G7E6D 3': 'OTHERS',\n",
       " 'MANO2PKR 0': 'OTHERS',\n",
       " 'MANO2PKR 1': 'OTHERS',\n",
       " 'MANO2PKR 2': 'OTHERS',\n",
       " 'MANO2PKR 3': 'OTHERS',\n",
       " 'MLRJYJ5K 0': 'OTHERS',\n",
       " 'MLRJYJ5K 1': 'OTHERS',\n",
       " 'MLRJYJ5K 4': 'OTHERS',\n",
       " 'MM6ZALRS 0': 'OTHERS',\n",
       " 'MM6ZALRS 1': 'OTHERS',\n",
       " 'MSCT8MJD 0': 'OTHERS',\n",
       " 'MSCT8MJD 1': 'OTHERS',\n",
       " 'MSCT8MJD 2': 'OTHERS',\n",
       " 'MZ0BI8NN 0': 'ORG',\n",
       " 'MZ0BI8NN 1': 'LOC',\n",
       " 'MZ0BI8NN 2': 'LOC',\n",
       " 'N6QAC84T 1': 'OTHERS',\n",
       " 'N6QAC84T 2': 'OTHERS',\n",
       " 'N6QAC84T 6': 'OTHERS',\n",
       " 'N9SJK368 0': 'OTHERS',\n",
       " 'ND6V1CBE 0': 'OTHERS',\n",
       " 'ND6V1CBE 1': 'OTHERS',\n",
       " 'NDSTZH1I 0': 'OTHERS',\n",
       " 'NDSTZH1I 1': 'OTHERS',\n",
       " 'NPNYM0BA 0': 'OTHERS',\n",
       " 'NPNYM0BA 1': 'OTHERS',\n",
       " 'NPNYM0BA 2': 'OTHERS',\n",
       " 'NPNYM0BA 3': 'OTHERS',\n",
       " 'NV4GY44T 0': 'OTHERS',\n",
       " 'NV4GY44T 3': 'OTHERS',\n",
       " 'NYFX4M8T 0': 'OTHERS',\n",
       " 'NYFX4M8T 1': 'LOC',\n",
       " 'NYFX4M8T 2': 'LOC',\n",
       " 'NYFX4M8T 4': 'OTHERS',\n",
       " 'NYFX4M8T 5': 'OTHERS',\n",
       " 'O0BECB72 0': 'OTHERS',\n",
       " 'O0BECB72 1': 'OTHERS',\n",
       " 'O0BECB72 2': 'OTHERS',\n",
       " 'O0BECB72 3': 'OTHERS',\n",
       " 'O0UJJVBR 0': 'OTHERS',\n",
       " 'O0UJJVBR 2': 'OTHERS',\n",
       " 'O0UJJVBR 3': 'OTHERS',\n",
       " 'O0UJJVBR 4': 'OTHERS',\n",
       " 'OD9USH5H 0': 'OTHERS',\n",
       " 'OD9USH5H 2': 'OTHERS',\n",
       " 'OD9USH5H 3': 'OTHERS',\n",
       " 'OHGI1JNY 1': 'OTHERS',\n",
       " 'OHGI1JNY 2': 'OTHERS',\n",
       " 'OIDIXPNZ 0': 'OTHERS',\n",
       " 'OIDIXPNZ 1': 'LOC',\n",
       " 'OIDIXPNZ 2': 'OTHERS',\n",
       " 'OKW6UUW5 0': 'OTHERS',\n",
       " 'OKW6UUW5 1': 'OTHERS',\n",
       " 'OKW6UUW5 2': 'OTHERS',\n",
       " 'OP0PZIXY 1': 'OTHERS',\n",
       " 'OP0PZIXY 2': 'OTHERS',\n",
       " 'OP0PZIXY 3': 'OTHERS',\n",
       " 'ORPKCFRL 0': 'OTHERS',\n",
       " 'ORPKCFRL 1': 'OTHERS',\n",
       " 'ORPKCFRL 2': 'OTHERS',\n",
       " 'ORPKCFRL 3': 'OTHERS',\n",
       " 'ORPKCFRL 5': 'OTHERS',\n",
       " 'OUL0NHS5 1': 'OTHERS',\n",
       " 'OUL0NHS5 2': 'OTHERS',\n",
       " 'OZYCQ769 1': 'OTHERS',\n",
       " 'OZYCQ769 2': 'OTHERS',\n",
       " 'P86ZJFXK 0': 'OTHERS',\n",
       " 'P86ZJFXK 1': 'OTHERS',\n",
       " 'P86ZJFXK 2': 'OTHERS',\n",
       " 'P86ZJFXK 3': 'OTHERS',\n",
       " 'P8B3IAOY 0': 'OTHERS',\n",
       " 'P8B3IAOY 1': 'OTHERS',\n",
       " 'P8B3IAOY 2': 'OTHERS',\n",
       " 'P8JQV93S 1': 'OTHERS',\n",
       " 'PWNRGOJ5 0': 'OTHERS',\n",
       " 'PWNRGOJ5 1': 'OTHERS',\n",
       " 'PWNRGOJ5 4': 'OTHERS',\n",
       " 'PZXDACJ9 0': 'OTHERS',\n",
       " 'PZXDACJ9 1': 'OTHERS',\n",
       " 'PZXDACJ9 2': 'OTHERS',\n",
       " 'QIW5EC2H 0': 'OTHERS',\n",
       " 'QIW5EC2H 1': 'OTHERS',\n",
       " 'QNP7O8L5 0': 'OTHERS',\n",
       " 'QNP7O8L5 1': 'OTHERS',\n",
       " 'QNP7O8L5 2': 'OTHERS',\n",
       " 'QNP7O8L5 3': 'ORG',\n",
       " 'QOAVEFGY 1': 'OTHERS',\n",
       " 'QOAVEFGY 2': 'OTHERS',\n",
       " 'QOAVEFGY 6': 'OTHERS',\n",
       " 'QOL4ZIHL 0': 'ORG',\n",
       " 'QOL4ZIHL 1': 'OTHERS',\n",
       " 'QOL4ZIHL 4': 'OTHERS',\n",
       " 'QPFX5Z8J 0': 'OTHERS',\n",
       " 'QPFX5Z8J 5': 'OTHERS',\n",
       " 'QW492LGU 0': 'OTHERS',\n",
       " 'QW492LGU 1': 'OTHERS',\n",
       " 'QW492LGU 2': 'OTHERS',\n",
       " 'RYTFLT5K 0': 'OTHERS',\n",
       " 'RYTFLT5K 1': 'OTHERS',\n",
       " 'RYTFLT5K 2': 'OTHERS',\n",
       " 'RYTFLT5K 4': 'OTHERS',\n",
       " 'S7DQFOD4 0': 'OTHERS',\n",
       " 'S7DQFOD4 1': 'OTHERS',\n",
       " 'S7DQFOD4 3': 'OTHERS',\n",
       " 'S8UOQYBG 0': 'OTHERS',\n",
       " 'S8UOQYBG 1': 'OTHERS',\n",
       " 'S8UOQYBG 2': 'OTHERS',\n",
       " 'S8UOQYBG 4': 'OTHERS',\n",
       " 'SFGT3EDA 0': 'OTHERS',\n",
       " 'SFGT3EDA 1': 'OTHERS',\n",
       " 'SHV3ZSSV 0': 'OTHERS',\n",
       " 'SHV3ZSSV 1': 'OTHERS',\n",
       " 'SHV3ZSSV 3': 'OTHERS',\n",
       " 'SNUO09BH 0': 'OTHERS',\n",
       " 'SNUO09BH 1': 'OTHERS',\n",
       " 'SNUO09BH 4': 'ORG',\n",
       " 'SNUO09BH 5': 'OTHERS',\n",
       " 'SNUO09BH 6': 'OTHERS',\n",
       " 'SPPNJXB2 0': 'OTHERS',\n",
       " 'SPPNJXB2 1': 'OTHERS',\n",
       " 'SRVLBA90 0': 'OTHERS',\n",
       " 'SRVLBA90 1': 'OTHERS',\n",
       " 'SRVLBA90 3': 'OTHERS',\n",
       " 'T7RPWH6N 1': 'OTHERS',\n",
       " 'T7RPWH6N 2': 'OTHERS',\n",
       " 'T7RPWH6N 6': 'OTHERS',\n",
       " 'TNI3UKH2 0': 'OTHERS',\n",
       " 'TNI3UKH2 2': 'OTHERS',\n",
       " 'TNI3UKH2 3': 'OTHERS',\n",
       " 'TNI3UKH2 4': 'OTHERS',\n",
       " 'TNI3UKH2 5': 'OTHERS',\n",
       " 'TNI3UKH2 6': 'OTHERS',\n",
       " 'TW1HGRFI 1': 'OTHERS',\n",
       " 'TW1HGRFI 2': 'OTHERS',\n",
       " 'U1FDHL7N 0': 'LOC',\n",
       " 'U1FDHL7N 3': 'OTHERS',\n",
       " 'U1FDHL7N 4': 'OTHERS',\n",
       " 'U30NUSS7 1': 'OTHERS',\n",
       " 'U5L6L4Q4 0': 'OTHERS',\n",
       " 'U5L6L4Q4 5': 'OTHERS',\n",
       " 'UEJEB27H 0': 'OTHERS',\n",
       " 'UEJEB27H 2': 'LOC',\n",
       " 'UEJEB27H 3': 'OTHERS',\n",
       " 'UEJEB27H 4': 'OTHERS',\n",
       " 'UN0JPGPF 1': 'OTHERS',\n",
       " 'UN0JPGPF 2': 'OTHERS',\n",
       " 'UN0JPGPF 3': 'OTHERS',\n",
       " 'UURPYBGQ 0': 'OTHERS',\n",
       " 'UURPYBGQ 1': 'OTHERS',\n",
       " 'VGUZX5R3 0': 'OTHERS',\n",
       " 'VGUZX5R3 1': 'OTHERS',\n",
       " 'VGUZX5R3 3': 'OTHERS',\n",
       " 'VKWTT7F7 0': 'ORG',\n",
       " 'VKWTT7F7 1': 'LOC',\n",
       " 'VKWTT7F7 2': 'LOC',\n",
       " 'VLUAWPBO 0': 'OTHERS',\n",
       " 'VLUAWPBO 1': 'OTHERS',\n",
       " 'VLUAWPBO 4': 'OTHERS',\n",
       " 'WH6JINCM 0': 'ORG',\n",
       " 'WH6JINCM 1': 'OTHERS',\n",
       " 'WH6JINCM 4': 'OTHERS',\n",
       " 'WI5LYQ0H 0': 'OTHERS',\n",
       " 'WI5LYQ0H 1': 'OTHERS',\n",
       " 'WI5LYQ0H 2': 'OTHERS',\n",
       " 'WRGS0WCX 0': 'ORG',\n",
       " 'WRGS0WCX 1': 'OTHERS',\n",
       " 'WRGS0WCX 2': 'OTHERS',\n",
       " 'WRGS0WCX 3': 'OTHERS',\n",
       " 'WRGS0WCX 4': 'OTHERS',\n",
       " 'WRGS0WCX 5': 'OTHERS',\n",
       " 'WRPAQPNC 0': 'OTHERS',\n",
       " 'WRPAQPNC 1': 'OTHERS',\n",
       " 'WWBIR8H6 0': 'OTHERS',\n",
       " 'WWBIR8H6 1': 'ORG',\n",
       " 'WWBIR8H6 2': 'OTHERS',\n",
       " 'WZL61XYS 0': 'OTHERS',\n",
       " 'WZL61XYS 1': 'OTHERS',\n",
       " 'X1LRVWSR 0': 'OTHERS',\n",
       " 'X1LRVWSR 1': 'OTHERS',\n",
       " 'X1LRVWSR 2': 'OTHERS',\n",
       " 'X23TMJ3R 0': 'OTHERS',\n",
       " 'X23TMJ3R 1': 'OTHERS',\n",
       " 'X23TMJ3R 2': 'OTHERS',\n",
       " 'X23TMJ3R 3': 'ORG',\n",
       " 'X3RACWMT 0': 'ORG',\n",
       " 'X3RACWMT 1': 'OTHERS',\n",
       " 'X3RACWMT 4': 'OTHERS',\n",
       " 'XBHX2VRT 0': 'OTHERS',\n",
       " 'XZRRL5AE 0': 'OTHERS',\n",
       " 'Y85GTOSS 0': 'OTHERS',\n",
       " 'Y85GTOSS 1': 'OTHERS',\n",
       " 'Y85GTOSS 2': 'OTHERS',\n",
       " 'YCOUS57M 0': 'OTHERS',\n",
       " 'Z4M8AT89 0': 'OTHERS',\n",
       " 'ZZNW93IV 0': 'OTHERS',\n",
       " 'ZZNW93IV 1': 'OTHERS'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_to_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668/668 [01:53<00:00,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cea_file = './data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"].split(\" \")[0] in q_ids.values():\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"./data/Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_sorted_mentions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_sorted_mentions[:q1_idx]\n",
    "q2 = R4_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = int(len(R4_sorted_mentions)/40) \n",
    "R4_sample_keys = []\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q1, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q2, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q3, sample_size)\n",
    "R4_sample_keys = R4_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SPARQL queries: 100%|██████████| 31922/31922 [10:06<00:00, 52.67it/s]\n",
      " 63%|██████▎   | 13994/22207 [1:06:49<5:38:02,  2.47s/it]"
     ]
    }
   ],
   "source": [
    "# find the mention in the table\n",
    "tables = \"./data/Dataset/Dataset/Round4_2020/tables/\"\n",
    "cta_file = './data/Dataset/Dataset/Round4_2020/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "mapping = {\n",
    "    \"LOC\": [\n",
    "        \"Q17334923\",  # Place\n",
    "        \"Q486972\",    # PopulatedPlace\n",
    "        \"Q618123\",    # Geographical Feature\n",
    "        \"Q515\",       # City\n",
    "        \"Q6256\",      # Country\n",
    "        \"Q82794\",     # Region\n",
    "        \"Q8502\",      # Mountain\n",
    "        \"Q23442\",     # Island\n",
    "        \"Q23397\",     # Lake\n",
    "        \"Q4022\",      # River\n",
    "        \"Q22698\",     # Park\n",
    "        \"Q41176\",     # Building\n",
    "        \"Q839954\",    # HistoricPlace\n",
    "        \"Q4989906\",   # Monument\n",
    "        \"Q12280\",     # Bridge\n",
    "        \"Q34442\",     # Road\n",
    "        \"Q1248784\",   # Airport\n",
    "        \"Q13226383\",  # Residential area\n",
    "        \"Q13221722\",  # Industrial area\n",
    "        \"Q164238\",    # Forest\n",
    "        \"Q55488\",     # Desert\n",
    "        \"Q3065569\",   # Archaeological site\n",
    "        \"Q14752696\",  # Campus\n",
    "        \"Q1022083\",   # Tourist attraction\n",
    "        \"Q207694\",    # World Heritage Site\n",
    "        \"Q4989906\",   # Monument\n",
    "        \"Q2065736\",   # Memorial\n",
    "        \"Q10864048\",  # Castle\n",
    "        \"Q23413\",     # Palace\n",
    "        \"Q838948\",    # Estate\n",
    "    ],\n",
    "    \"PER\": [\n",
    "        \"Q215627\",    # Person\n",
    "        \"Q5\",         # Human\n",
    "        \"Q483501\",    # Artist\n",
    "        \"Q2066131\",   # Athlete\n",
    "        \"Q82955\",     # Politician\n",
    "        \"Q901\",       # Scientist\n",
    "        \"Q36180\",     # Writer\n",
    "        \"Q33999\",     # Actor\n",
    "        \"Q639669\",    # Musician\n",
    "        \"Q6581097\",   # MilitaryPerson\n",
    "        \"Q947873\",    # Religious\n",
    "        \"Q12078\",     # Royalty\n",
    "        \"Q1456951\",   # Criminal\n",
    "        \"Q36834\",     # Philosopher\n",
    "        \"Q49757\",     # Director\n",
    "        \"Q622425\",    # Producer\n",
    "        \"Q333634\",    # Businessperson\n",
    "        \"Q937857\",    # Entrepreneur\n",
    "        \"Q3282637\",   # Inventor\n",
    "        \"Q10871364\",  # Fashion designer\n",
    "        \"Q4834541\",   # Journalist\n",
    "        \"Q36180\",     # Poet\n",
    "        \"Q170790\",    # Diplomat\n",
    "        \"Q376799\",    # Chef\n",
    "        \"Q24238356\",  # Influencer\n",
    "        \"Q207694\",    # Celebrity\n",
    "        \"Q10864048\",  # Explorer\n",
    "    ],\n",
    "    \"ORG\": [\n",
    "        \"Q43229\",     # Organisation\n",
    "        \"Q783794\",    # Company\n",
    "        \"Q3918\",      # EducationalInstitution\n",
    "        \"Q7278\",      # PoliticalParty\n",
    "        \"Q12973014\",  # SportsTeam\n",
    "        \"Q163740\",    # Non-ProfitOrganisation\n",
    "        \"Q327333\",    # GovernmentAgency\n",
    "        \"Q1969448\",   # ReligiousOrganisation\n",
    "        \"Q215380\",    # Band\n",
    "        \"Q7075\",      # Library\n",
    "        \"Q33506\",     # Museum\n",
    "        \"Q16917\",     # Hospital\n",
    "        \"Q216052\",    # TradeUnion\n",
    "        \"Q6501447\",   # Research Institute\n",
    "        \"Q163740\",    # Foundation\n",
    "        \"Q200695\",    # Bank\n",
    "        \"Q19317\",     # Corporation\n",
    "        \"Q875538\",    # Law firm\n",
    "        \"Q2234766\",   # Professional association\n",
    "        \"Q2088357\",   # Charity\n",
    "        \"Q163740\",    # NGO (Non-Governmental Organization)\n",
    "        \"Q484652\",    # Think tank\n",
    "        \"Q507619\",    # Multinational corporation\n",
    "        \"Q4438121\",   # Media company\n",
    "        \"Q156537\",    # Publishing company\n",
    "        \"Q11426\",     # Airline\n",
    "        \"Q980447\",    # Hotel chain\n",
    "        \"Q1131088\",   # Technology company\n",
    "    ]\n",
    "}\n",
    "\n",
    "def get_item_root(mapping, id_list):\n",
    "    # Flatten the list of target IDs\n",
    "    target_ids = [item for sublist in mapping.values() for item in sublist]\n",
    "    \n",
    "    # Define the SPARQL endpoint\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    \n",
    "    # Define the SPARQL query\n",
    "    id_list_str = \" \".join(\"wd:\" + id_ for id_ in id_list)\n",
    "    target_ids_str = \" \".join(\"wd:\" + tid for tid in target_ids)\n",
    "    query = \"\"\"\n",
    "    SELECT ?item ?class WHERE {\n",
    "      VALUES ?item { %s }\n",
    "      ?item wdt:P279* ?class.\n",
    "      VALUES ?class { %s }\n",
    "    }\n",
    "    \"\"\" % (id_list_str, target_ids_str)\n",
    "    \n",
    "    # Set the query and request JSON response\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    \n",
    "    # Execute the query and convert the result to JSON\n",
    "    results = sparql.query().convert()\n",
    "    \n",
    "    # Extract the classes from the results\n",
    "    id_to_root_class = {}\n",
    "    for result in results[\"results\"][\"bindings\"]:\n",
    "        item_id = result[\"item\"][\"value\"].split('/')[-1]\n",
    "        root_class = result[\"class\"][\"value\"].split('/')[-1]\n",
    "        id_to_root_class[item_id] = root_class\n",
    "    \n",
    "    return id_to_root_class\n",
    "\n",
    "def get_item_root_batch(mapping, ids, batch_size=50):\n",
    "    root_classes = {}\n",
    "    with tqdm(total=len(ids), desc=\"SPARQL queries\") as pbar:\n",
    "        for i in range(0, len(ids), batch_size):\n",
    "            batch_ids = ids[i:i+batch_size]\n",
    "            batch_root_classes = get_item_root(mapping, batch_ids)\n",
    "            root_classes.update(batch_root_classes)\n",
    "            pbar.update(len(batch_ids))\n",
    "    return [root_classes.get(id_, None) for id_ in ids]\n",
    "\n",
    "# Create reverse mapping\n",
    "reverse_mapping = {v: k for k, values in mapping.items() for v in values}\n",
    "\n",
    "# Define function to map df[2] values to their categories\n",
    "def map_class_to_category(class_name):\n",
    "    return reverse_mapping.get(class_name, \"OTHERS\")\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "ids = [url.split('/')[-1] for url in df[2]]\n",
    "\n",
    "root_classes = get_item_root_batch(mapping, ids)\n",
    "\n",
    "# Map root classes to categories\n",
    "root_categories = [map_class_to_category(root_id) for root_id in root_classes]\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "key_to_cell = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                key_to_cell[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_from_value(d, value):\n",
    "    keys = [key for key, val in d.items() if val == value]\n",
    "    return keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cea_file = './data/Dataset/Dataset/Round4_2020/gt/cea.csv'\n",
    "mentions = {}\n",
    "chunk_size = 1000\n",
    "column_names = [\"table_name\", \"row\", \"col\", \"url\"] \n",
    "\n",
    "total_rows = sum(1 for line in open(cea_file)) - 1  # Exclude header\n",
    "total_iterations = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division to include last chunk\n",
    "\n",
    "for chunk_cea in tqdm(pd.read_csv(cea_file, chunksize=chunk_size), total=total_iterations):\n",
    "    chunk_cea.columns = column_names\n",
    "    for _, row in chunk_cea.iterrows():\n",
    "        key = f\"{row['table_name']} {row['col']}\"\n",
    "        if key in key_to_cell.keys() and row[\"url\"] in q_ids.values():\n",
    "            print(\"found\")\n",
    "            data = key_to_cell[key]\n",
    "            mentions[get_keys_from_value(q_ids, row[\"url\"])] = (row[\"url\"], data)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(mentions))\n\u001b[0;32m--> 109\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:  \u001b[38;5;66;03m# For environments like Jupyter\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[33], line 101\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mentions, url, pbar)\u001b[0m\n\u001b[1;32m     97\u001b[0m         cont_el \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m count\n\u001b[1;32m     99\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()  \u001b[38;5;66;03m# No need to await here\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoverage of R1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcont_el\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmentions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeasure Reciprocal Rank of R1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm_mrr\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(mentions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except Exception as e:\n",
    "                return []\n",
    "\n",
    "async def process_item(session, name, value, url, headers, semaphore, pbar):\n",
    "    params = {\n",
    "        'name': name,\n",
    "        'token': 'lamapi_demo_2023',\n",
    "        'kg': 'wikidata',\n",
    "        'limit': 1000,\n",
    "        'query': f'''\n",
    "        {{\n",
    "            \"query\": {{\n",
    "                \"bool\": {{\n",
    "                    \"must\": [\n",
    "                        {{\n",
    "                            \"match\": {{\n",
    "                                \"name\": {{\n",
    "                                    \"query\": \"{name}\",\n",
    "                                    \"boost\": 2.0\n",
    "                                }}\n",
    "                            }}\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"term\": {{\n",
    "                                \"NERtype\": \"{value[1]}\"\n",
    "                            }}\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        '''\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{name}'\")\n",
    "            pbar.update(1)  # No need to await here\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            GT_id_match = re.search(r'Q(\\d+)$', value[0])\n",
    "            if GT_id_match:\n",
    "                GT_id = GT_id_match[0]\n",
    "                if GT_id == item.get('id'):\n",
    "                    pbar.update(1)  # No need to await here\n",
    "                   # print(f\"{el}-->t{item}\")\n",
    "                    #print(\"__________________________\")\n",
    "                    pos_score = item.get('pos_score', 0)\n",
    "                    if pos_score:\n",
    "                        mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                    else:\n",
    "                        mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                    return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(mentions, url, pbar):\n",
    "    string_name_list = mentions\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for name, type in string_name_list.items():\n",
    "            tasks.append(process_item(session, name, type, url, headers, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for mrr_increment, count in results:\n",
    "            m_mrr += mrr_increment\n",
    "            cont_el += count\n",
    "\n",
    "        pbar.close()  # No need to await here\n",
    "\n",
    "    print(f\"Coverage of R4: {cont_el / len(mentions)}\")\n",
    "    print(f\"Measure Reciprocal Rank of R4: {m_mrr / len(mentions)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(mentions))\n",
    "        asyncio.run(main(mentions, url, pbar))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(mentions, url, pbar))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
