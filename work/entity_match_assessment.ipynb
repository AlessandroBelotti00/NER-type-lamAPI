{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e343dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import requests\n",
    "from aiohttp import ClientResponseError\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd060b-eb7e-460f-a1d7-63b8b15cad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)\n",
    "\n",
    "\n",
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc803f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(R4_2T_sorted_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0385ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) | set(edInst_subclass) | set(govAgency_subclass) | set(intOrg_subclass))\n",
    "    geolocation_subclass = list(set(geolocation_subclass) | set(country_subclass) | set(city_subclass) | set(capitals_subclass) | set(admTerr_subclass))\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "   #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tables_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "cta_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c68919",
   "metadata": {},
   "outputs": [],
   "source": [
    "### in case you want HT2\n",
    "\n",
    "tables_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "pattern = r'^\\.'\n",
    "\n",
    "# Create a list of file paths, excluding files that start with a dot\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n",
    "\n",
    "\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/HT2_ner_type.json', 'r') as f:\n",
    "    ner_type = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "### in case you want R4\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json', 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7bf1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from JSON file:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = \"./R4_ner_type_new.json\"\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('./R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)\n",
    "\n",
    "# Now key_to_cell contains the dictionary loaded from the JSON file\n",
    "print(\"Dictionary loaded from JSON file:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf3ae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTHERS: 13306\n",
      "PERS: 1607\n",
      "LOC: 260\n",
      "ORG: 211\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "category_counts = Counter(ner_type.values())\n",
    "\n",
    "# Display the counts for each category\n",
    "for category, count in category_counts.items():\n",
    "    print(f'{category}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import backoff\n",
    "import asyncio\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "rows = []\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the async function to fetch data with retries\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch_data(session, url, data):\n",
    "    async with session.post(url, headers=headers, json=data) as response:\n",
    "        # Check the content type of the response\n",
    "        content_type = response.headers.get('Content-Type', '').lower()\n",
    "        \n",
    "        if 'application/json' not in content_type:\n",
    "            print(f\"Unexpected content type: {content_type}. URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        return await response.json()\n",
    "\n",
    "# Main async function to process mentions\n",
    "async def process_mentions():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for key, (text, id) in tqdm(key_to_cell_sample.items(), desc='Processing mentions', unit='item'):\n",
    "            match = re.search(r'Q(\\d+)$', id)\n",
    "            \n",
    "            if not match:\n",
    "                continue\n",
    "            \n",
    "            data = {'json': [match[0]]}\n",
    "\n",
    "            try:\n",
    "                response_json = await fetch_data(session, url, data)\n",
    "                \n",
    "                if response_json is None or len(response_json) == 0:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                desc = response_json[match[0]]['description']\n",
    "                if desc == None:\n",
    "                    continue\n",
    "            \n",
    "                label = response_json[match[0]]['NERtype']\n",
    "                new_row = {'text': text, 'label': label, 'desc': desc}\n",
    "\n",
    "                rows.append(new_row)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "# Run the async function\n",
    "await (process_mentions())\n",
    "\n",
    "# Create DataFrame from the list of rows\n",
    "df = pd.DataFrame(rows)\n",
    "category_counts = Counter(df['label'])\n",
    "\n",
    "# Display the counts for each category\n",
    "for category, count in category_counts.items():\n",
    "    print(f'{category}: {count}')\n",
    "    \n",
    "#df.to_csv('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/embedding_training_data/Round3_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60563a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/embedding_training_data/HT2_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f27731",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb3fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8411/475897 [22:05<20:27:49,  6.35it/s]  \n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "    if value is not None:\n",
    "        # Soft filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}}\n",
    "                    ],\n",
    "                    \"should\": [\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "queries = []\n",
    "key_to_cell_sample = {}\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        key_to_cell_sample[key] = key_to_cell[key]\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        \n",
    "        matched_results = []\n",
    "        for q_id in q_ids:            \n",
    "            match = re.search(r'Q(\\d+)$', q_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "\n",
    "            json_data = json.dumps(data)\n",
    "            response = requests.post(url, headers=headers, data=json_data)\n",
    "            if len(response.json()) == 0:\n",
    "                continue\n",
    "            \n",
    "            true_ner = response.json()[match[0]]['NERtype']\n",
    "            break\n",
    "\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['should'][0]['term']['NERtype']\n",
    "            # ner_type_list is the ner column\n",
    "            queries.append((query, match[0],ner_type_list, true_ner))\n",
    "            if len(queries) == 4000:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4c37db0-3904-4e6c-acc6-d7ec7eb3f55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(key_to_cell_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0215ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_cell_sample = {key: value for key, value in key_to_cell.items() if value[1].split('/')[-1] in missing_values}\n",
    "# just for 2T missing values comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8afa8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3999/4000 [13:28<00:00,  4.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "\n",
    "    if value is not None:\n",
    "        # Hard filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}},\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }    \n",
    "\n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell_sample):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        if NER_type is None:\n",
    "            print(f\"q_ids: {q_ids}, ner_type key: {new_key}\")\n",
    "        query = get_query(name, NER_type)\n",
    "        \n",
    "        matched_results = []\n",
    "        for q_id in q_ids:\n",
    "            match = re.search(r'Q(\\d+)$', q_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "\n",
    "            json_data = json.dumps(data)\n",
    "            response = requests.post(url, headers=headers, data=json_data)\n",
    "            if len(response.json()) == 0:\n",
    "                continue\n",
    "            \n",
    "            true_ner = response.json()[match[0]]['NERtype']\n",
    "            break\n",
    "\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "            # ner_type_list is the ner column\n",
    "            queries.append((query, match[0],ner_type_list,true_ner))\n",
    "            if len(queries) == 4000:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0400192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 417/4000 [02:15<12:28,  4.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request timed out for params: {'name': 'Regional Centre for Biotechnology Act, 2016', 'token': 'lamapi_demo_2023', 'kg': 'wikidata', 'limit': 1000, 'query': '{\"query\": {\"bool\": {\"must\": [{\"match\": {\"name\": {\"query\": \"Regional Centre for Biotechnology Act, 2016\", \"boost\": 2.0}}}, {\"term\": {\"NERtype\": \"OTHERS\"}}]}}}', 'sort': ['{\"popularity\": {\"order\": \"desc\"}}']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2905/4000 [14:15<05:22,  3.40it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of 2T: 0.72625\n",
      "Measure Reciprocal Rank of 2T: 0.6855012499999815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assume queries is a list of tuples [(param1, id1), (param2, id2), ...]\n",
    "\n",
    "failed_queries = {}\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=10, \n",
    "    max_time=400\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        # Convert all params to str, int, or float\n",
    "        #params = {k: (int(v) if isinstance(v, np.integer) else str(v)) for k, v in params.items()}\n",
    "        async with session.get(url, params=params, headers=headers, timeout=50) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except asyncio.TimeoutError:\n",
    "                print(f\"Request timed out for params: {params}\")\n",
    "                return []  # Return an empty list to handle the timeout gracefully\n",
    "            except aiohttp.ClientError as e:\n",
    "                print(f\"ClientError for params {params}: {str(e)}\")\n",
    "                return []\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for params {params}: {str(e)}\")\n",
    "                return []\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{id}'\")\n",
    "            asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar, failed_queries):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id, _, _ in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id, col_NERtype, item_NERtype) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                failed_queries[id] = (col_NERtype, item_NERtype)\n",
    "            else:\n",
    "                m_mrr += mrr_increment\n",
    "                cont_el += count\n",
    "\n",
    "        asyncio.get_event_loop().call_soon_threadsafe(pbar.close)\n",
    "\n",
    "    print(f\"Coverage of 2T: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar, failed_queries))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar, failed_queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002caacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d162709",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Round4_failed_queries_HARD.json', 'w') as json_file:\n",
    "    json.dump(failed_queries, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48472390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Round4_failed_queries_HARD.json', 'r') as f:\n",
    "    failed_queries_hard = json.load(f)\n",
    "\n",
    "with open('./data/Round4_failed_queries_SOFT.json', 'r') as f:\n",
    "    failed_queries_soft = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1043957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed_queries_hard: 1092 vs failed_queries_soft: 268\n"
     ]
    }
   ],
   "source": [
    "print(f\"failed_queries_hard: {len(failed_queries_hard)} vs failed_queries_soft: {len(failed_queries_soft)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7983ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 475897/475897 [00:07<00:00, 62398.35it/s] \n"
     ]
    }
   ],
   "source": [
    "column_type = []\n",
    "\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        if NER_type is not None:\n",
    "            column_type.append((q_ids, NER_type))\n",
    "            #print(f\"q_ids: {q_ids}, ner_type column: {NER_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "128a8e2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate through the queries\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, \u001b[38;5;28mid\u001b[39m, ner, ner_col \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Check if the ID is in the missing values list\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmissing_values\u001b[49m :\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Parse the query JSON\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Extract the NER type list\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'missing_values' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Initialize a dictionary to count mismatches by category\n",
    "mismatch_categories = defaultdict(int)\n",
    "tmp = []\n",
    "\n",
    "# Iterate through the queries\n",
    "for p, id, ner, ner_col in queries:\n",
    "    # Check if the ID is in the missing values list\n",
    "    if id in missing_values :\n",
    "        # Parse the query JSON\n",
    "        data = json.loads(p['query'])\n",
    "        # Extract the NER type list\n",
    "        ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "        # Print the NERtype and column NER for this ID\n",
    "        print(f\"id: {id} has item NERtype: {ner}, column NER: {ner_col}\")\n",
    "        # Compare item NERtype with column NER and count mismatches\n",
    "        if ner != ner_col:\n",
    "            # Create a category key for the mismatch\n",
    "            category = f\"{ner} != {ner_col}\"\n",
    "            # Increment the count for this mismatch category\n",
    "            mismatch_categories[category] += 1\n",
    "\n",
    "# Print the counts for each mismatch category\n",
    "for category, count in mismatch_categories.items():\n",
    "    print(f\"Mismatch category '{category}': {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4237c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for p, id, ner, ner_col in queries:\n",
    "    if id in missing_values:\n",
    "        data = json.loads(p['query'])\n",
    "        ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "        print(f\"id: {id} has item NERtype: {ner}, column NER: {ner_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4545c",
   "metadata": {},
   "source": [
    "# 2T\n",
    "id: Q328446 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q741830 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q2276193 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q988934 has item NERtype: LOC, column NER: OTHERS\n",
    "\n",
    "id: Q153195 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q2415851 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q2387130 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q233129 has item NERtype: LOC, column NER: OTHERS\n",
    "\n",
    "id: Q732342 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q3241019 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q1998298 has item NERtype: LOC, column NER: OTHERS\n",
    "\n",
    "id: Q755226 has item NERtype: LOC, column NER: ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53875a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in missing_values:\n",
    "    print(f\"id: {'http://wikidata.org/entity/'+el} has true NERtype: {failed_queries_hard[el]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e35413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch category 'OTHERS != LOC': 557 occurrences\n",
      "Mismatch category 'OTHERS != ORG': 261 occurrences\n",
      "Mismatch category 'ORG != LOC': 7 occurrences\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Initialize a dictionary to count mismatches by category\n",
    "mismatch_categories = defaultdict(int)\n",
    "\n",
    "\n",
    "\n",
    "for el in missing_values:\n",
    "    if failed_queries_hard[el][0] != failed_queries_hard[el][1]:\n",
    "        # Create a category key for the mismatch\n",
    "        category = f\"{failed_queries_hard[el][0]} != {failed_queries_hard[el][1]}\"\n",
    "        #print(category)\n",
    "        # Increment the count for this mismatch category\n",
    "        mismatch_categories[category] += 1\n",
    "\n",
    "# Print the counts for each mismatch category\n",
    "for category, count in mismatch_categories.items():\n",
    "    print(f\"Mismatch category '{category}': {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aa362e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "826\n"
     ]
    }
   ],
   "source": [
    "# 19 entities doesn’t match due to the Hard filtering constraint\n",
    "\n",
    "missing_values = set(failed_queries_hard) - set(failed_queries_soft)\n",
    "print(len(missing_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
