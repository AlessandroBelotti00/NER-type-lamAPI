{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e343dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from requests import get\n",
    "import numpy as np\n",
    "import requests\n",
    "from aiohttp import ClientResponseError\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cd060b-eb7e-460f-a1d7-63b8b15cad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# READ THE JSON\n",
    "#####################\n",
    "\n",
    "json_file_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2T_Round4_sorted_mentions.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    R4_2T_sorted_mentions = json.load(file)\n",
    "\n",
    "\n",
    "## Sample extraction\n",
    "# SPLIT OVER THE QUARTILES\n",
    "\n",
    "n = len(R4_2T_sorted_mentions)\n",
    "q1_idx = n // 4\n",
    "q2_idx = n // 2\n",
    "q3_idx = 3 * n // 4\n",
    "\n",
    "# Step 3: Split the list into quartiles\n",
    "q1 = R4_2T_sorted_mentions[:q1_idx]\n",
    "q2 = R4_2T_sorted_mentions[q1_idx:q2_idx]\n",
    "q3 = R4_2T_sorted_mentions[q2_idx:q3_idx]\n",
    "q4 = R4_2T_sorted_mentions[q3_idx:]\n",
    "\n",
    "sample_size = 1000\n",
    "R4_2T_sample_keys = []\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q1, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q2, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q3, sample_size)\n",
    "R4_2T_sample_keys = R4_2T_sample_keys + random.sample(q4, sample_size)\n",
    "\n",
    "q_ids = {item[1]['name']: item[1]['id'] for item in R4_2T_sample_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc803f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65297"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(R4_2T_sorted_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0385ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids\n",
    "\n",
    "\n",
    "try:\n",
    "    organization_subclass = get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    country_subclass = get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    country_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    city_subclass = get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    city_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    capitals_subclass = get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    capitals_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    admTerr_subclass = get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    admTerr_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    family_subclass = get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    family_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    sportLeague_subclass = get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    sportLeague_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    venue_subclass = get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    venue_subclass = set()\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    organization_subclass = list(set(organization_subclass) - set(country_subclass) - set(city_subclass) - set(capitals_subclass) - set(admTerr_subclass) - set(family_subclass) - set(sportLeague_subclass) - set(venue_subclass))\n",
    "    #print(len(organization_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    food_subclass = get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    food_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    edInst_subclass = get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    edInst_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    govAgency_subclass = get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    govAgency_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    intOrg_subclass = get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    intOrg_subclass = set()\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    timeZone_subclass = get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    timeZone_subclass = set()\n",
    "    pass\n",
    "   \n",
    "try:\n",
    "    geolocation_subclass = list(set(geolocation_subclass) - set(food_subclass) - set(edInst_subclass) - set(govAgency_subclass) - set(intOrg_subclass) - set(timeZone_subclass))\n",
    "    #print(len(geolocation_subclass))\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    human_subclass = get_wikidata_item_tree_item_idsSPARQL([5], backward_properties=[279])\n",
    "except json.decoder.JSONDecodeError:\n",
    "    human_subclass = set()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d6b59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100%|██████████| 180/180 [00:10<00:00, 16.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "tables_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1609b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:41<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "tables = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/gt/cea.csv'\n",
    "cta_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/2T_Round4/gt/cta.csv'\n",
    "os.listdir(tables)\n",
    "\n",
    "def get_item_root(id_list):     \n",
    "    id_to_root_class = {}\n",
    "    for el in id_list:\n",
    "        inst_item = int(re.search(r'(\\d+)$', el)[0])\n",
    "        if inst_item in geolocation_subclass:\n",
    "            #id_to_root_class[el] = \"LOC\"\n",
    "            return \"LOC\"\n",
    "        elif inst_item in organization_subclass:\n",
    "            #id_to_root_class[el] = \"ORG\"\n",
    "            return \"ORG\"\n",
    "        elif inst_item in human_subclass:\n",
    "            #id_to_root_class[el] = \"PERS\"\n",
    "            return \"PERS\"      \n",
    "    \n",
    "    return \"OTHERS\"\n",
    "\n",
    "# Apply the function and create the 'key' column\n",
    "root_classes = []\n",
    "df = pd.read_csv(cta_file, header=None)\n",
    "root_categories = []\n",
    "for urls in df[2]:\n",
    "    tmp = [url.split('/')[-1] for url in urls.split(\" \")]\n",
    "    root_categories.append(get_item_root(tmp))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"category\"] = root_categories\n",
    "cta_keys = {}\n",
    "cta_keys[\"key\"] = (df[0] + \" \" + df[1].astype('str'), df[\"category\"])\n",
    "\n",
    "ner_type = {}\n",
    "for table in tqdm(os.listdir(tables)):\n",
    "    table_file = os.path.join(tables, table)\n",
    "    table_name = table.split(\".\")[0]\n",
    "    df = pd.read_csv(table_file)\n",
    "    \n",
    "    for row in range(df.shape[0]):\n",
    "        for col in range(df.shape[1]):\n",
    "            key = f\"{table_name} {col}\"\n",
    "            if key in set(cta_keys[\"key\"][0].values):\n",
    "                tmp_index = cta_keys[\"key\"][0].values.tolist().index(key)\n",
    "                tmp_value = cta_keys[\"key\"][1].iloc[tmp_index]\n",
    "                ner_type[key] = tmp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c68919",
   "metadata": {},
   "outputs": [],
   "source": [
    "### in case you want HT2\n",
    "\n",
    "tables_path = \"C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/HardTablesR2/tables/\"\n",
    "cea_file = 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/Dataset/Dataset/HardTablesR2/gt/cea.csv'\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df = pd.read_csv(cea_file, header=None)\n",
    "df[\"key\"] = df[0] + \" \" + df[1].astype(str) + \" \" + df[2].astype(str)\n",
    "cea_values_dict = dict(zip(df[\"key\"].values, df[3].values))\n",
    "cea_keys_set = set(df[\"key\"].values)\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        local_key_to_cell = {}\n",
    "        \n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    local_key_to_cell[key] = (cell_value, cea_values_dict[key])\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "        \n",
    "        return local_key_to_cell\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "pattern = r'^\\.'\n",
    "\n",
    "# Create a list of file paths, excluding files that start with a dot\n",
    "table_files = [os.path.join(tables_path, table) for table in os.listdir(tables_path) if not re.match(pattern, table)]\n",
    "\n",
    "# Process tables sequentially\n",
    "key_to_cell = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    key_to_cell.update(local_key_to_cell)\n",
    "\n",
    "\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/HT2_ner_type.json', 'r') as f:\n",
    "    ner_type = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958e2370",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### in case you want R4\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     ner_type \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/R4_key_to_cell.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json'"
     ]
    }
   ],
   "source": [
    "### in case you want R4\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/data/R4_ner_type_new.json', 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/work/R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7bf1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from JSON file:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "file_path = \"./R4_ner_type_new.json\"\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r') as f:\n",
    "    ner_type = json.load(f)\n",
    "\n",
    "with open('./R4_key_to_cell.json', 'r') as f:\n",
    "    key_to_cell = json.load(f)\n",
    "\n",
    "# Now key_to_cell contains the dictionary loaded from the JSON file\n",
    "print(\"Dictionary loaded from JSON file:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf3ae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERS: 96\n",
      "OTHERS: 212\n",
      "ORG: 184\n",
      "LOC: 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "category_counts = Counter(ner_type.values())\n",
    "\n",
    "# Display the counts for each category\n",
    "for category, count in category_counts.items():\n",
    "    print(f'{category}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff7f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import backoff\n",
    "import asyncio\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "rows = []\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the async function to fetch data with retries\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=5, \n",
    "    max_time=300\n",
    ")\n",
    "async def fetch_data(session, url, data):\n",
    "    async with session.post(url, headers=headers, json=data) as response:\n",
    "        # Check the content type of the response\n",
    "        content_type = response.headers.get('Content-Type', '').lower()\n",
    "        \n",
    "        if 'application/json' not in content_type:\n",
    "            print(f\"Unexpected content type: {content_type}. URL: {url}\")\n",
    "            return None\n",
    "        \n",
    "        return await response.json()\n",
    "\n",
    "# Main async function to process mentions\n",
    "async def process_mentions():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for key, (text, id) in tqdm(key_to_cell_sample.items(), desc='Processing mentions', unit='item'):\n",
    "            match = re.search(r'Q(\\d+)$', id)\n",
    "            \n",
    "            if not match:\n",
    "                continue\n",
    "            \n",
    "            data = {'json': [match[0]]}\n",
    "\n",
    "            try:\n",
    "                response_json = await fetch_data(session, url, data)\n",
    "                \n",
    "                if response_json is None or len(response_json) == 0:\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                desc = response_json[match[0]]['description']\n",
    "                if desc == None:\n",
    "                    continue\n",
    "            \n",
    "                label = response_json[match[0]]['NERtype']\n",
    "                new_row = {'text': text, 'label': label, 'desc': desc}\n",
    "\n",
    "                rows.append(new_row)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "# Run the async function\n",
    "await (process_mentions())\n",
    "\n",
    "# Create DataFrame from the list of rows\n",
    "df = pd.DataFrame(rows)\n",
    "category_counts = Counter(df['label'])\n",
    "\n",
    "# Display the counts for each category\n",
    "for category, count in category_counts.items():\n",
    "    print(f'{category}: {count}')\n",
    "    \n",
    "#df.to_csv('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/embedding_training_data/Round3_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60563a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/embedding_training_data/HT2_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f27731",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 4000\n",
    "key_to_cell_sample = dict(random.sample(list(key_to_cell.items()), sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbb3fb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3999/194438 [19:43<15:39:15,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "    if value is not None:\n",
    "        # Soft filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}}\n",
    "                    ],\n",
    "                    \"should\": [\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "queries = []\n",
    "key_to_cell_sample = {}\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        key_to_cell_sample[key] = key_to_cell[key]\n",
    "        NER_type = ner_type[new_key]\n",
    "        query = get_query(name, NER_type)\n",
    "        \n",
    "        matched_results = []\n",
    "        for q_id in q_ids:            \n",
    "            match = re.search(r'Q(\\d+)$', q_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "\n",
    "            json_data = json.dumps(data)\n",
    "            response = requests.post(url, headers=headers, data=json_data)\n",
    "            if len(response.json()) == 0:\n",
    "                continue\n",
    "            \n",
    "            true_ner = response.json()[match[0]]['NERtype']\n",
    "            break\n",
    "\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['should'][0]['term']['NERtype']\n",
    "            # ner_type_list is the ner column\n",
    "            queries.append((query, match[0],ner_type_list, true_ner))\n",
    "            if len(queries) == 4000:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0215ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m key_to_cell_sample \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m key_to_cell\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m missing_values}\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m key_to_cell_sample \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m key_to_cell\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m value[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmissing_values\u001b[49m}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'missing_values' is not defined"
     ]
    }
   ],
   "source": [
    "key_to_cell_sample = {key: value for key, value in key_to_cell.items() if value[1].split('/')[-1] in missing_values}\n",
    "# just for 2T missing values comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8afa8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 3999/4000 [18:39<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "\n",
    "    if value is not None:\n",
    "        # Hard filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}},\n",
    "                        {\"term\": {\"NERtype\": value}}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 1000,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }    \n",
    "\n",
    "    return params\n",
    "\n",
    "url = 'https://lamapi.hel.sintef.cloud/entity/labels?token=lamapi_demo_2023'\n",
    "\n",
    "# Define the headers\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "queries = []\n",
    "for key in tqdm(key_to_cell_sample):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        if NER_type is None:\n",
    "            print(f\"q_ids: {q_ids}, ner_type key: {new_key}\")\n",
    "        query = get_query(name, NER_type)\n",
    "        \n",
    "        matched_results = []\n",
    "        for q_id in q_ids:\n",
    "            match = re.search(r'Q(\\d+)$', q_id)\n",
    "            if not match:\n",
    "                continue\n",
    "            data = {\n",
    "                'json': [match[0]]\n",
    "            }\n",
    "\n",
    "            json_data = json.dumps(data)\n",
    "            response = requests.post(url, headers=headers, data=json_data)\n",
    "            if len(response.json()) == 0:\n",
    "                continue\n",
    "            \n",
    "            true_ner = response.json()[match[0]]['NERtype']\n",
    "            break\n",
    "\n",
    "        if match:\n",
    "            data = json.loads(query['query'])\n",
    "            ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "            # ner_type_list is the ner column\n",
    "            queries.append((query, match[0],ner_type_list,true_ner))\n",
    "            if len(queries) == 4000:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0400192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 3155/4000 [04:04<01:05, 12.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of 2T: 0.78875\n",
      "Measure Reciprocal Rank of 2T: 0.7699557499999518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Assume queries is a list of tuples [(param1, id1), (param2, id2), ...]\n",
    "\n",
    "failed_queries = {}\n",
    "url = 'https://lamapi.hel.sintef.cloud/lookup/entity-retrieval'\n",
    "\n",
    "# Backoff decorator for handling retries with exponential backoff\n",
    "@backoff.on_exception(\n",
    "    backoff.expo, \n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError), \n",
    "    max_tries=10, \n",
    "    max_time=400\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        # Convert all params to str, int, or float\n",
    "        #params = {k: (int(v) if isinstance(v, np.integer) else str(v)) for k, v in params.items()}\n",
    "        async with session.get(url, params=params, headers=headers, timeout=50) as response:\n",
    "            try:\n",
    "                response.raise_for_status()  # Raises an exception for 4XX/5XX status codes\n",
    "                return await response.json()\n",
    "            except asyncio.TimeoutError:\n",
    "                print(f\"Request timed out for params: {params}\")\n",
    "                return []  # Return an empty list to handle the timeout gracefully\n",
    "            except aiohttp.ClientError as e:\n",
    "                print(f\"ClientError for params {params}: {str(e)}\")\n",
    "                return []\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for params {params}: {str(e)}\")\n",
    "                return []\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{id}'\")\n",
    "            asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise  # Re-raise the exception for other status codes\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                asyncio.get_event_loop().call_soon_threadsafe(pbar.update, 1)\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result  # Assume worst case for MRR if pos_score is 0\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, pbar, failed_queries):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)  # Limit to 50 concurrent requests\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for param, id, _, _ in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for (mrr_increment, count), (param, id, col_NERtype, item_NERtype) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                failed_queries[id] = (col_NERtype, item_NERtype)\n",
    "            else:\n",
    "                m_mrr += mrr_increment\n",
    "                cont_el += count\n",
    "\n",
    "        asyncio.get_event_loop().call_soon_threadsafe(pbar.close)\n",
    "\n",
    "    print(f\"Coverage of 2T: {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T: {m_mrr / len(queries)}\")\n",
    "\n",
    "# Check if there's already a running event loop\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()  # Apply nest_asyncio\n",
    "    try:\n",
    "        pbar = tqdm(total=len(queries))\n",
    "        asyncio.run(main(queries, url, pbar, failed_queries))\n",
    "    except RuntimeError:  # For environments like Jupyter\n",
    "        loop = asyncio.get_event_loop()\n",
    "        loop.run_until_complete(main(queries, url, pbar, failed_queries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "002caacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'failed_queries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mfailed_queries\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'failed_queries' is not defined"
     ]
    }
   ],
   "source": [
    "len(failed_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d162709",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2Tbis_failed_queries_SOFT.json', 'w') as json_file:\n",
    "    json.dump(failed_queries, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48472390",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2T_failed_queries_HARD.json', 'r') as f:\n",
    "    failed_queries_hard = json.load(f)\n",
    "\n",
    "with open('C:/ALESSANDRO/Università/MAGISTRALE/SINTEF_thesis/lamAPI/data/2T_failed_queries_SOFT.json', 'r') as f:\n",
    "    failed_queries_soft = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1043957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed_queries_hard: 320 vs failed_queries_soft: 302\n"
     ]
    }
   ],
   "source": [
    "print(f\"failed_queries_hard: {len(failed_queries_hard)} vs failed_queries_soft: {len(failed_queries_soft)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7983ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 475897/475897 [00:07<00:00, 62398.35it/s] \n"
     ]
    }
   ],
   "source": [
    "column_type = []\n",
    "\n",
    "for key in tqdm(key_to_cell):\n",
    "    id_table, _, id_col = key.split(\" \")\n",
    "    name = key_to_cell[key][0]\n",
    "    q_ids = key_to_cell[key][1].split(' ')\n",
    "    new_key = f\"{id_table} {id_col}\"\n",
    "    if new_key in ner_type:\n",
    "        NER_type = ner_type[new_key]\n",
    "        if NER_type is not None:\n",
    "            column_type.append((q_ids, NER_type))\n",
    "            #print(f\"q_ids: {q_ids}, ner_type column: {NER_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "128a8e2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'missing_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate through the queries\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, \u001b[38;5;28mid\u001b[39m, ner, ner_col \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Check if the ID is in the missing values list\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmissing_values\u001b[49m :\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Parse the query JSON\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# Extract the NER type list\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'missing_values' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Initialize a dictionary to count mismatches by category\n",
    "mismatch_categories = defaultdict(int)\n",
    "tmp = []\n",
    "\n",
    "# Iterate through the queries\n",
    "for p, id, ner, ner_col in queries:\n",
    "    # Check if the ID is in the missing values list\n",
    "    if id in missing_values :\n",
    "        # Parse the query JSON\n",
    "        data = json.loads(p['query'])\n",
    "        # Extract the NER type list\n",
    "        ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "        # Print the NERtype and column NER for this ID\n",
    "        print(f\"id: {id} has item NERtype: {ner}, column NER: {ner_col}\")\n",
    "        # Compare item NERtype with column NER and count mismatches\n",
    "        if ner != ner_col:\n",
    "            # Create a category key for the mismatch\n",
    "            category = f\"{ner} != {ner_col}\"\n",
    "            # Increment the count for this mismatch category\n",
    "            mismatch_categories[category] += 1\n",
    "\n",
    "# Print the counts for each mismatch category\n",
    "for category, count in mismatch_categories.items():\n",
    "    print(f\"Mismatch category '{category}': {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4237c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for p, id, ner, ner_col in queries:\n",
    "    if id in missing_values:\n",
    "        data = json.loads(p['query'])\n",
    "        ner_type_list = data['query']['bool']['must'][1]['term']['NERtype']\n",
    "        print(f\"id: {id} has item NERtype: {ner}, column NER: {ner_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4545c",
   "metadata": {},
   "source": [
    "# 2T\n",
    "id: Q328446 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q741830 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q2276193 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q988934 has item NERtype: LOC, column NER: OTHERS\n",
    "\n",
    "id: Q153195 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q2415851 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q2387130 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q233129 has item NERtype: LOC, column NER: OTHERS\n",
    "\n",
    "id: Q732342 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q3241019 has item NERtype: LOC, column NER: ORG\n",
    "\n",
    "id: Q1998298 has item NERtype: LOC, column NER: OTHERS\n",
    "\n",
    "id: Q755226 has item NERtype: LOC, column NER: ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53875a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in missing_values:\n",
    "    print(f\"id: {'http://wikidata.org/entity/'+el} has true NERtype: {failed_queries_hard[el]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e35413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch category 'OTHERS != LOC': 557 occurrences\n",
      "Mismatch category 'OTHERS != ORG': 261 occurrences\n",
      "Mismatch category 'ORG != LOC': 7 occurrences\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Initialize a dictionary to count mismatches by category\n",
    "mismatch_categories = defaultdict(int)\n",
    "\n",
    "\n",
    "\n",
    "for el in missing_values:\n",
    "    if failed_queries_hard[el][0] != failed_queries_hard[el][1]:\n",
    "        # Create a category key for the mismatch\n",
    "        category = f\"{failed_queries_hard[el][0]} != {failed_queries_hard[el][1]}\"\n",
    "        #print(category)\n",
    "        # Increment the count for this mismatch category\n",
    "        mismatch_categories[category] += 1\n",
    "\n",
    "# Print the counts for each mismatch category\n",
    "for category, count in mismatch_categories.items():\n",
    "    print(f\"Mismatch category '{category}': {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa362e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# 19 entities doesn’t match due to the Hard filtering constraint\n",
    "\n",
    "missing_values = set(failed_queries_hard.keys()) - set(failed_queries_soft.keys())\n",
    "print(len(missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c916507f-ed11-46e5-b447-1d1397d4f484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Q1613070', 'Q224650', 'Q1215575', 'Q991871', 'Q397905', 'Q16213808', 'Q5115712', 'Q653358', 'Q5605401', 'Q18216', 'Q4911138', 'Q462481', 'Q6235502', 'Q7861794', 'Q14950144', 'Q2443556', 'Q1110876', 'Q49266', 'Q1174042', 'Q4954365', 'Q1184769', 'Q782', 'Q3664160', 'Q29445', 'Q517878', 'Q913543', 'Q28993', 'Q5538385', 'Q18207395', 'Q1762871', 'Q36008', 'Q10424752', 'Q122741', 'Q320729', 'Q577450', 'Q10379507', 'Q489255', 'Q233129', 'Q7803613', 'Q988721', 'Q4766718', 'Q2570337', 'Q852704', 'Q755397', 'Q49179', 'Q1371', 'Q485176', 'Q483551', 'Q10858068', 'Q703908', 'Q12053713', 'Q5544366', 'Q3181344', 'Q12232446', 'Q17708839', 'Q3110949', 'Q4714253', 'Q3147338', 'Q25871', 'Q36159', 'Q963', 'Q432152', 'Q2306425', 'Q55174762', 'Q2021327', 'Q2325739', 'Q4704061', 'Q951440', 'Q8011060', 'Q45700', 'Q6210307', 'Q1689292', 'Q761885', 'Q970527', 'Q2702313', 'Q7147290', 'Q1033188', 'Q5987624', 'Q49195', 'Q79329', 'Q575122', 'Q7822396', 'Q2748009', 'Q15303252', 'Q137450', 'Q647103', 'Q2668651', 'Q329018', 'Q984377', 'Q6218163', 'Q16234884', 'Q82816', 'Q27048845', 'Q613842', 'Q3237210', 'Q732342', 'Q20709222', 'Q780330', 'Q145480', 'Q6209153', 'Q28260', 'Q4935700', 'Q2415851', 'Q2054988', 'Q1745862', 'Q4678811', 'Q275889', 'Q1509', 'Q988940', 'Q950934', 'Q959761', 'Q1719914', 'Q3852419', 'Q7609620', 'Q3101663', 'Q331104', 'Q7364995', 'Q174224', 'Q16016234', 'Q6182356', 'Q730430', 'Q7815592', 'Q1086913', 'Q25369', 'Q892401', 'Q1876560', 'Q2876725', 'Q668546', 'Q94274', 'Q326295', 'Q1986211', 'Q2003150', 'Q14428', 'Q4703927', 'Q6346', 'Q186969', 'Q20676265', 'Q1913953', 'Q79546', 'Q2246550', 'Q16553', 'Q15993292', 'Q125934', 'Q1017603', 'Q975843', 'Q5205317', 'Q467664', 'Q2418056', 'Q33486', 'Q15438261', 'Q1046188', 'Q174093', 'Q2056518', 'Q79867', 'Q975627', 'Q1676471', 'Q297032', 'Q12312', 'Q994656', 'Q754078', 'Q1204', 'Q114', 'Q408', 'Q852647', 'Q26844407', 'Q18921483', 'Q977', 'Q18746588', 'Q4017999', 'Q1687869', 'Q2306', 'Q6846533', 'Q6746', 'Q847869', 'Q5540791', 'Q4980374', 'Q25395', 'Q37', 'Q49276', 'Q1075852', 'Q2308188', 'Q230203', 'Q956665', 'Q483618', 'Q1013261', 'Q816', 'Q3379006', 'Q439601', 'Q43981', 'Q881', 'Q21872602', 'Q16826993', 'Q49221', 'Q1062', 'Q499189', 'Q7860157', 'Q79592', 'Q6260650', 'Q322788', 'Q876841', 'Q159232', 'Q313389', 'Q2639042', 'Q718752', 'Q495870', 'Q232', 'Q16739662', 'Q5273122', 'Q5274627', 'Q229018', 'Q219526', 'Q6408928', 'Q5107885', 'Q3241019', 'Q33405', 'Q1291929', 'Q6831682', 'Q6766634', 'Q48370', 'Q3998036', 'Q3528612', 'Q463635', 'Q6949637', 'Q726296', 'Q4704799', 'Q2563549', 'Q214164', 'Q303046', 'Q10101', 'Q2574546', 'Q1701201', 'Q3362982', 'Q6490842', 'Q52468', 'Q5234347', 'Q1342', 'Q352110', 'Q5231062', 'Q5609767', 'Q4962958', 'Q3939172', 'Q433726', 'Q8093', 'Q5541134', 'Q5386020', 'Q678', 'Q79580', 'Q962', 'Q162667', 'Q5673502', 'Q7341845', 'Q5585553', 'Q538634', 'Q2029466', 'Q1935584', 'Q913209', 'Q1001316', 'Q672', 'Q3876468', 'Q4933644', 'Q16151898', 'Q83813', 'Q222338', 'Q5386494', 'Q53569', 'Q1153175', 'Q3014174', 'Q1456', 'Q5545207', 'Q3491402', 'Q468648', 'Q16148409', 'Q200491', 'Q16194403', 'Q54220', 'Q6222819', 'Q2503300', 'Q5541037', 'Q21078109', 'Q1603', 'Q11813', 'Q5194501', 'Q6221558', 'Q983761', 'Q7026963', 'Q1279784', 'Q1025016', 'Q1313504', 'Q1168929', 'Q499169', 'Q902651', 'Q29364', 'Q984609', 'Q2831', 'Q17859802', 'Q3930685', 'Q712', 'Q16214385', 'Q653512', 'Q547420', 'Q199797', 'Q11637492', 'Q314427', 'Q4712165', 'Q965256', 'Q1969316', 'Q5488295', 'Q560436', 'Q553497', 'Q7299064', 'Q5105858', 'Q7309829', 'Q5107100', 'Q19430147', 'Q1046185', 'Q6221698', 'Q44989', 'Q16186373', 'Q2560090', 'Q41819'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_queries_hard.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
