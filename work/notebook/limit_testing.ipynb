{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "json_file_path = \"/home/lamapi/lamAPI/data/Downloads/_HTR2/HTR2_ext_WD_query_type.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    HTR2_type = json.load(file)\n",
    "\n",
    "\n",
    "tables_path = \"/home/lamapi/lamAPI/data/Downloads/HardTablesR2/tables/\"\n",
    "cea_file = '/home/lamapi/lamAPI/data/Downloads/HardTablesR2/gt/cea.csv'\n",
    "cta_file = '/home/lamapi/lamAPI/data/Downloads/HardTablesR2/gt/cta.csv'\n",
    "\n",
    "\n",
    "os.listdir(tables_path)\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Read the cea_file and create a key-value dictionary\n",
    "df_cea = pd.read_csv(cea_file, header=None)\n",
    "df_cea[\"key\"] = df_cea[0] + \" \" + df_cea[1].astype(str) + \" \" + df_cea[2].astype(str)\n",
    "df_cea[\"key_col\"] = df_cea[0] + \" \" + df_cea[2].astype(str)\n",
    "cea_values_dict = dict(zip(df_cea[\"key_col\"].values, df_cea[3].values))\n",
    "\n",
    "cea_keys_set = set(df_cea[\"key\"].values)\n",
    "cea_values_dict_cell = dict(zip(df_cea[\"key\"].values, df_cea[3].values))\n",
    "\n",
    "# Function to process a single table file\n",
    "def process_table_file(table_file):\n",
    "    try:\n",
    "        table_name = os.path.splitext(os.path.basename(table_file))[0]\n",
    "        df = pd.read_csv(table_file)\n",
    "        qid_to_value = {}\n",
    "\n",
    "        for row in range(df.shape[0]):\n",
    "            for col in range(df.shape[1]):\n",
    "                key = f\"{table_name} {row+1} {col}\"\n",
    "                if key in cea_keys_set:\n",
    "                    cell_value = df.iloc[row, col]\n",
    "                    qid = cea_values_dict_cell[key].split('/')[-1]  # Extract the QID from the URL\n",
    "                    qid_to_value[cell_value] = qid\n",
    "                    break  # Exit inner loop early as only one match per row/col is needed\n",
    "\n",
    "        return qid_to_value\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {table_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# List of table files\n",
    "table_files = [\n",
    "    os.path.join(tables_path, table)\n",
    "    for table in os.listdir(tables_path)\n",
    "    if not table.startswith('.')\n",
    "]\n",
    "\n",
    "# Process tables sequentially\n",
    "HTR2_id_to_name = {}\n",
    "for table_file in tqdm(table_files, desc=\"Processing tables\"):\n",
    "    local_key_to_cell = process_table_file(table_file)\n",
    "    HTR2_id_to_name.update(local_key_to_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(name, value):\n",
    "    name = str(name).replace('\"', ' ')\n",
    "    if value is not None:\n",
    "        # hard filtering constraint\n",
    "        query_dict = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        {\"match\": {\"name\": {\"query\": name, \"boost\": 2.0}}},\n",
    "                        {\"terms\": {\"extended_WDtypes\": value}}  # Ensures `value` matches at least one in the array\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        params = {\n",
    "            'name': name,\n",
    "            'token': 'lamapi_demo_2023',\n",
    "            'kg': 'wikidata',\n",
    "            'limit': 20,\n",
    "            'query': json.dumps(query_dict),  # Convert the query dictionary to a JSON string\n",
    "            'sort': [\n",
    "                '{\"popularity\": {\"order\": \"desc\"}}'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "queries = []\n",
    "for name, id  in tqdm(HTR2_id_to_name.items()):\n",
    "    if id in HTR2_type:\n",
    "        types_list = HTR2_type[id]\n",
    "\n",
    "        ########################################################\n",
    "        ##  modificare se types_list Ã¨ una lista di tipi\n",
    "        ########################################################\n",
    "    \n",
    "        query = get_query(name, types_list)\n",
    "\n",
    "        queries.append((query, id, types_list))\n",
    "        if len(queries) == 4000:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import backoff\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Assume queries is a list of tuples [(param1, id1), (param2, id2), ...]\n",
    "\n",
    "failed_queries = {}\n",
    "url = 'http://localhost:5000/lookup/entity-retrieval'\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    (aiohttp.ClientError, aiohttp.http_exceptions.HttpProcessingError, asyncio.TimeoutError),\n",
    "    max_tries=10,\n",
    "    max_time=400\n",
    ")\n",
    "async def fetch(session, url, params, headers, semaphore):\n",
    "    async with semaphore:\n",
    "        async with session.get(url, params=params, headers=headers, timeout=50) as response:\n",
    "            try:\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "            except asyncio.TimeoutError:\n",
    "                print(f\"Request timed out for params: {params}\")\n",
    "                return []\n",
    "            except aiohttp.ClientError as e:\n",
    "                print(f\"ClientError for params : {str(e)}\")\n",
    "                return []\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error for params {params}: {str(e)}\")\n",
    "                return []\n",
    "\n",
    "async def process_item(session, url, id, headers, params, semaphore, pbar):\n",
    "    try:\n",
    "        data = await fetch(session, url, params, headers, semaphore)\n",
    "    except aiohttp.ClientResponseError as e:\n",
    "        if e.status == 404:\n",
    "            print(f\"404 Error: Resource not found for '{id}'\")\n",
    "            pbar.update(1)\n",
    "            return 0, 0\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    num_result = len(data) if data else 0\n",
    "\n",
    "    if data:\n",
    "        for item in data:\n",
    "            if id == item.get('id'):\n",
    "                pbar.update(1)\n",
    "                pos_score = item.get('pos_score', 0)\n",
    "                if pos_score:\n",
    "                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                else:\n",
    "                    mrr_increment = 1 / num_result\n",
    "                return mrr_increment, 1\n",
    "\n",
    "    return 0, 0\n",
    "\n",
    "async def main(queries, url, failed_queries, l):\n",
    "    headers = {'accept': 'application/json'}\n",
    "    semaphore = asyncio.Semaphore(50)\n",
    "    m_mrr = 0\n",
    "    cont_el = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        pbar = tqdm_asyncio(total=len(queries), desc=f\"Limit {l}\")\n",
    "        for param, id, _ in queries:\n",
    "            tasks.append(process_item(session, url, id, headers, param, semaphore, pbar))\n",
    "\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        for (mrr_increment, count), (param, id, item_NERtype) in zip(results, queries):\n",
    "            if mrr_increment == 0 and count == 0:\n",
    "                failed_queries[id] = (id, item_NERtype)\n",
    "                \n",
    "                param['limit'] = l\n",
    "                \n",
    "                query_dict = json.loads(param['query'])\n",
    "\n",
    "                if \"query\" in query_dict and \"bool\" in query_dict[\"query\"] and \"must\" in query_dict[\"query\"][\"bool\"]:\n",
    "                    for condition in query_dict[\"query\"][\"bool\"][\"must\"]:\n",
    "                        if \"match\" in condition and \"name\" in condition[\"match\"]:\n",
    "                            condition[\"match\"][\"name\"][\"fuzziness\"] = \"AUTO\"\n",
    "\n",
    "                param['query'] = json.dumps(query_dict)\n",
    "\n",
    "                response = requests.get(url, params=param)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    num_result = len(data) if data else 0\n",
    "                    if data:\n",
    "                        for item in data:\n",
    "                            if id == item.get('id'):\n",
    "                                pbar.update(1)\n",
    "                                pos_score = item.get('pos_score', 0)\n",
    "                                if pos_score:\n",
    "                                    mrr_increment = (num_result - (pos_score * num_result)) / num_result\n",
    "                                else:\n",
    "                                    mrr_increment = 1 / num_result\n",
    "\n",
    "                m_mrr += mrr_increment\n",
    "                cont_el += count\n",
    "            else:\n",
    "                m_mrr += mrr_increment\n",
    "                cont_el += count\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    print(f\"Coverage of 2T (l={l}): {cont_el / len(queries)}\")\n",
    "    print(f\"Measure Reciprocal Rank of 2T (l={l}): {m_mrr / len(queries)}\")\n",
    "\n",
    "    return cont_el / len(queries), m_mrr / len(queries)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    try:\n",
    "        trend_dict = {}\n",
    "        l_values = list(range(1, 100, 5))  # Increment `l` by 5\n",
    "        loop = asyncio.get_event_loop()\n",
    "\n",
    "        results = loop.run_until_complete(\n",
    "            asyncio.gather(\n",
    "                *(main(queries, url, failed_queries, l) for l in l_values)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for l, (coverage, mrr) in zip(l_values, results):\n",
    "            trend_dict[l] = (coverage, mrr)\n",
    "\n",
    "        print(\"Trend Dictionary:\", trend_dict)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Runtime error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
