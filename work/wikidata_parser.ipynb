{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242f0f9-c9e3-4f3b-8d15-96050eeeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from json.decoder import JSONDecodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef38472-3087-45f2-ae9b-fdd8757076a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection setup\n",
    "MONGO_ENDPOINT, MONGO_ENDPOINT_PORT = os.environ[\"MONGO_ENDPOINT\"].split(\":\")\n",
    "MONGO_ENDPOINT_PORT = int(MONGO_ENDPOINT_PORT)\n",
    "MONGO_ENDPOINT_USERNAME = os.environ[\"MONGO_INITDB_ROOT_USERNAME\"]\n",
    "MONGO_ENDPOINT_PASSWORD = os.environ[\"MONGO_INITDB_ROOT_PASSWORD\"]\n",
    "DB_NAME = f\"wikidata\"\n",
    "\n",
    "client = MongoClient(MONGO_ENDPOINT, MONGO_ENDPOINT_PORT, username=MONGO_ENDPOINT_USERNAME, password=MONGO_ENDPOINT_PASSWORD)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16daa576-49bf-4fd5-960f-a034684916a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "\n",
    "def get_wikidata_item_tree_item_idsSPARQL(root_items, forward_properties=None, backward_properties=None):\n",
    "    \"\"\"Return ids of WikiData items, which are in the tree spanned by the given root items and claims relating them\n",
    "        to other items.\n",
    "\n",
    "    :param root_items: iterable[int] One or multiple item entities that are the root elements of the tree\n",
    "    :param forward_properties: iterable[int] | None property-claims to follow forward; that is, if root item R has\n",
    "        a claim P:I, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :param backward_properties: iterable[int] | None property-claims to follow in reverse; that is, if (for a root\n",
    "        item R) an item I has a claim P:R, and P is in the list, the search will branch recursively to item I as well.\n",
    "    :return: iterable[int]: List with ids of WikiData items in the tree\n",
    "    \"\"\"\n",
    "\n",
    "    query = '''PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "            PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "            PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "            PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>'''\n",
    "    if forward_properties:\n",
    "        query +='''SELECT ?WD_id WHERE {\n",
    "                  ?tree0 (wdt:P%s)* ?WD_id .\n",
    "                  BIND (wd:%s AS ?tree0)\n",
    "                  }'''%( ','.join(map(str, forward_properties)),','.join(map(str, root_items)))\n",
    "    elif backward_properties:\n",
    "        query+='''SELECT ?WD_id WHERE {\n",
    "                    ?WD_id (wdt:P%s)* wd:Q%s .\n",
    "                    }'''%(','.join(map(str, backward_properties)), ','.join(map(str, root_items)))\n",
    "    #print(query)\n",
    "\n",
    "    url = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'\n",
    "    data = get(url, params={'query': query, 'format': 'json'}).json()\n",
    "    \n",
    "    ids = []\n",
    "    for item in data['results']['bindings']:\n",
    "        this_id=item[\"WD_id\"][\"value\"].split(\"/\")[-1].lstrip(\"Q\")\n",
    "        #print(item)\n",
    "        try:\n",
    "            this_id = int(this_id)\n",
    "            ids.append(this_id)\n",
    "            #print(this_id)\n",
    "        except ValueError:\n",
    "            #print(\"exception\")\n",
    "            continue\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b8114a-6765-4c4c-ab18-1a5c17c8aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_dump_path = './my-data/latest-all.json.bz2'\n",
    "\n",
    "DATATYPES_MAPPINGS = {\n",
    "    'external-id': 'STRING',\n",
    "    'quantity': 'NUMBER',\n",
    "    'globe-coordinate': 'STRING',\n",
    "    'string': 'STRING',\n",
    "    'monolingualtext': 'STRING',\n",
    "    'commonsMedia': 'STRING',\n",
    "    'time': 'DATETIME',\n",
    "    'url': 'STRING',\n",
    "    'geo-shape': 'GEOSHAPE',\n",
    "    'math': 'MATH',\n",
    "    'musical-notation': 'MUSICAL_NOTATION',\n",
    "    'tabular-data': 'TABULAR_DATA'\n",
    "}\n",
    "DATATYPES = list(set(DATATYPES_MAPPINGS.values()))\n",
    "\n",
    "def check_skip(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if \"datavalue\" not in temp:\n",
    "        return True\n",
    "\n",
    "    skip = {\n",
    "        \"wikibase-lexeme\",\n",
    "        \"wikibase-form\",\n",
    "        \"wikibase-sense\"\n",
    "    }\n",
    "\n",
    "    return datatype in skip\n",
    "\n",
    "\n",
    "def get_value(obj, datatype):\n",
    "    temp = obj.get(\"mainsnak\", obj)\n",
    "    if datatype == \"globe-coordinate\":\n",
    "        latitude = temp[\"datavalue\"][\"value\"][\"latitude\"]\n",
    "        longitude = temp[\"datavalue\"][\"value\"][\"longitude\"]\n",
    "        value = f\"{latitude},{longitude}\"\n",
    "    else:\n",
    "        keys = {\n",
    "            \"quantity\": \"amount\",\n",
    "            \"monolingualtext\": \"text\",\n",
    "            \"time\": \"time\",\n",
    "        }\n",
    "        if datatype in keys:\n",
    "            key = keys[datatype]\n",
    "            value = temp[\"datavalue\"][\"value\"][key]\n",
    "        else:\n",
    "            value = temp[\"datavalue\"][\"value\"]\n",
    "    return value\n",
    "\n",
    "\n",
    "try:\n",
    "    geolocation_subclass = get_wikidata_item_tree_item_idsSPARQL([2221906], backward_properties=[279])\n",
    "    #print(\"geolocation_subclass\")\n",
    "    food_subclass =  get_wikidata_item_tree_item_idsSPARQL([2095], backward_properties=[279])\n",
    "    #print(\"food_subclass\")\n",
    "    edInst_subclass =  get_wikidata_item_tree_item_idsSPARQL([2385804], backward_properties=[279])\n",
    "    #print(\"edInst_subclass\")\n",
    "    govAgency_subclass =  get_wikidata_item_tree_item_idsSPARQL([327333], backward_properties=[279])\n",
    "    #print(\"govAgency_subclass\")\n",
    "    intOrg_subclass =  get_wikidata_item_tree_item_idsSPARQL([484652], backward_properties=[279])\n",
    "    #print(\"intOrg_subclass\")\n",
    "    timeZone_subclass =  get_wikidata_item_tree_item_idsSPARQL([12143], backward_properties=[279])\n",
    "    #print(\"timeZone_subclass\")\n",
    "    \n",
    "    geolocation_subclass = list(set(geolocation_subclass)-set(food_subclass)-set(edInst_subclass)-set(govAgency_subclass)-\n",
    "                            set(intOrg_subclass)-set(timeZone_subclass))\n",
    "    \n",
    "    #print(f\"geolocation_subclass: {len(geolocation_subclass)}\")\n",
    "    \n",
    "    organization_subclass=get_wikidata_item_tree_item_idsSPARQL([43229], backward_properties=[279])\n",
    "    #print(\"organization_subclass\")\n",
    "    country_subclass =  get_wikidata_item_tree_item_idsSPARQL([6256], backward_properties=[279])\n",
    "    #print(\"country_subclass\")\n",
    "    city_subclass =  get_wikidata_item_tree_item_idsSPARQL([515], backward_properties=[279])\n",
    "    #print(\"city_subclass\")\n",
    "    capitals_subclass =  get_wikidata_item_tree_item_idsSPARQL([5119], backward_properties=[279])\n",
    "    #print(\"capitals_subclass\")\n",
    "    admTerr_subclass =  get_wikidata_item_tree_item_idsSPARQL([15916867], backward_properties=[279])\n",
    "    #print(\"admTerr_subclass\")\n",
    "    family_subclass =  get_wikidata_item_tree_item_idsSPARQL([17350442], backward_properties=[279])\n",
    "    #print(\"family_subclass\")\n",
    "    sportLeague_subclass =  get_wikidata_item_tree_item_idsSPARQL([623109], backward_properties=[279])\n",
    "    #print(\"sportLeague_subclass\")\n",
    "    venue_subclass =  get_wikidata_item_tree_item_idsSPARQL([8436], backward_properties=[279])\n",
    "    #print(\"venue_subclass\")\n",
    "    organization_subclass = list(set(organization_subclass)-set(country_subclass)-set(city_subclass)-\n",
    "                             set(capitals_subclass)-set(admTerr_subclass)-set(family_subclass) -\n",
    "                            set(sportLeague_subclass)-set(venue_subclass))\n",
    "    \n",
    "    #print(f\"organization_subclass: {len(organization_subclass)}\")\n",
    "except json.decoder.JSONDecodeError:\n",
    "    pass\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 1000\n",
    "    \n",
    "    ORG = []\n",
    "    PERS = []\n",
    "    LOC = []\n",
    "    OTHERS = []\n",
    "             \n",
    "    for i, line in tqdm(enumerate(f), total=1000):\n",
    "        if count == 0:\n",
    "            break\n",
    "        try:\n",
    "            count -= 1\n",
    "            # Parse JSON data from each line\n",
    "            data = json.loads(line[:-2])\n",
    "\n",
    "            entity = data['id']\n",
    "            labels = data.get(\"labels\", {})\n",
    "            english_label = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "            aliases = data.get(\"aliases\", {})\n",
    "            description = data.get('descriptions', {}).get('en', {}).get(\"value\", \"\")\n",
    "            category = \"entity\"\n",
    "            sitelinks = data.get(\"sitelinks\", {})\n",
    "            popularity = len(sitelinks) if len(sitelinks) > 0 else 1\n",
    "\n",
    "\n",
    "            ###############################################################\n",
    "            # ORGANIZATION EXTRACTION\n",
    "            # All items with the root class Organization (Q43229) excluding country (Q6256), city (Q515), capitals (Q5119), \n",
    "            # administrative territorial entity of a single country (Q15916867), venue (Q17350442), sports league (Q623109) \n",
    "            # and family (Q8436)\n",
    "            \n",
    "            # LOCATION EXTRACTION\n",
    "            # All items with the root class Geographic Location (Q2221906) excluding: food (Q2095), educational institution (Q2385804), \n",
    "            # government agency (Q327333), international organization (Q484652) and time zone (Q12143)\n",
    "            \n",
    "            # PERSON EXTRACTION\n",
    "            # All items with the statement is instance of (P31) human (Q5) are classiﬁed as person.\n",
    "\n",
    "            \n",
    "            if data.get(\"type\") == \"item\" and \"claims\" in data:\n",
    "                p31_claims = data[\"claims\"].get(\"P31\", [])\n",
    "                for claim in p31_claims:\n",
    "                    mainsnak = claim.get(\"mainsnak\", {})\n",
    "                    datavalue = mainsnak.get(\"datavalue\", {})\n",
    "                    numeric_id = datavalue.get(\"value\", {}).get(\"numeric-id\")\n",
    "                    if numeric_id in organization_subclass:\n",
    "                        ORG.append(numeric_id)\n",
    "                        #print(f\"ORG --> Entity ID: {entity}, label: {english_label}, category: {category}, description: {description}\")\n",
    "                    elif numeric_id == 5:\n",
    "                        PERS.append(numeric_id)\n",
    "                        #print(f\"PERS --> Entity ID: {entity}, label: {english_label}, category: {category}, description: {description}\")\n",
    "                    elif numeric_id in geolocation_subclass:\n",
    "                        LOC.append(numeric_id)\n",
    "                        #print(f\"LOC --> Entity ID: {entity}, label: {english_label}, category: {category}, description: {description}\")\n",
    "                    else:\n",
    "                        OTHERS.append(numeric_id)\n",
    "                        #print(f\"OTHERS --> Entity ID: {entity}, label: {english_label}, category: {category}, description: {description}\")\n",
    "                    \n",
    "                    \n",
    "            ################################################################    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "parser.print_aggregate(log_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6043e2-23dc-4a5d-8f49-9c8f6b598523",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a850f-8f23-4096-9a22-e594d6ece098",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length_PERS = len(PERS)\n",
    "total_length_ORG = len(ORG)\n",
    "total_length_LOC = len(LOC)\n",
    "total_length_OTHERS = len(OTHERS)\n",
    "\n",
    "# Print the total lengths\n",
    "print(\"Total lengths:\")\n",
    "print(f\"Length of PERS: {total_length_PERS}\")\n",
    "print(f\"Length of ORG: {total_length_ORG}\")\n",
    "print(f\"Length of LOC: {total_length_LOC}\")\n",
    "print(f\"Length of OTHERS: {total_length_OTHERS}\")\n",
    "\n",
    "# Calculate the sum of lengths\n",
    "total_length = total_length_PERS + total_length_ORG + total_length_LOC + total_length_OTHERS\n",
    "\n",
    "# Print the sum of lengths\n",
    "print(f\"Total length: {total_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a9fd3-190e-43d4-93a6-acb0d79af0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in OTHERS:\n",
    "    if el in PERS:\n",
    "        print(f\"PERS and ORG --> Entity ID: {PERS.index(el)}\")\n",
    "    if el in LOC:\n",
    "        print(f\"LOC and ORG --> Entity ID: {LOC.index(el)}\")\n",
    "    if el in ORG:\n",
    "        print(f\"OTHERS and ORG --> Entity ID: {ORG.index(el)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1273d4-71d2-4e5d-9100-48ede8cc4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to sets for faster intersection operation\n",
    "ORG_set = set(ORG)\n",
    "PERS_set = set(PERS)\n",
    "LOC_set = set(LOC)\n",
    "OTHERS_set = set(OTHERS)\n",
    "\n",
    "# Initialize counters for each set\n",
    "ORG_counter = 0\n",
    "PERS_counter = 0\n",
    "LOC_counter = 0\n",
    "OTHERS_counter = 0\n",
    "\n",
    "# Find the overlapping items and update the counters\n",
    "for item in ORG_set.union(PERS_set, LOC_set, OTHERS_set):\n",
    "    num_overlaps = 0\n",
    "    if item in ORG_set:\n",
    "        print(\"item\")\n",
    "        num_overlaps += 1\n",
    "    if item in PERS_set:\n",
    "        num_overlaps += 1\n",
    "    if item in LOC_set:\n",
    "        num_overlaps += 1\n",
    "    if item in OTHERS_set:\n",
    "        num_overlaps += 1\n",
    "    \n",
    "    # Update the corresponding counter based on the number of overlaps\n",
    "    if num_overlaps == 1:\n",
    "        ORG_counter += 1\n",
    "    elif num_overlaps == 2:\n",
    "        PERS_counter += 1\n",
    "    elif num_overlaps == 3:\n",
    "        LOC_counter += 1\n",
    "    elif num_overlaps == 4:\n",
    "        OTHERS_counter += 1\n",
    "\n",
    "# Print the counts for each set\n",
    "print(\"Number of overlaps for each set:\")\n",
    "print(f\"ORG: {ORG_counter}\")\n",
    "print(f\"PERS: {PERS_counter}\")\n",
    "print(f\"LOC: {LOC_counter}\")\n",
    "print(f\"OTHERS: {OTHERS_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c9245",
   "metadata": {},
   "source": [
    "## URL Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4029e46-84a4-4177-a31b-e228d4149814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "# This Python file uses the following encoding: utf-8\n",
    "\n",
    "__author__ = 'jgeiss'\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# authors: Johanna Geiß, Heidelberg University, Germany                     #\n",
    "# email: geiss@informatik.uni-heidelberg.de                                 #\n",
    "# Copyright (c) 2017 Database Research Group,                               #\n",
    "#               Institute of Computer Science,                              #\n",
    "#               University of Heidelberg                                    #\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");         #\n",
    "#   you may not use this file except in compliance with the License.        #\n",
    "#   You may obtain a copy of the License at                                 #\n",
    "#                                                                           #\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0                              #\n",
    "#                                                                           #\n",
    "#   Unless required by applicable law or agreed to in writing, software     #\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,       #\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#\n",
    "#   See the License for the specific language governing permissions and     #\n",
    "#   limitations under the License.                                          #\n",
    "#############################################################################\n",
    "# last updated 21.3.2017 by Johanna Geiß\n",
    "\n",
    "from pymongo import *\n",
    "from pymongo import errors\n",
    "import configparser\n",
    "\n",
    "\n",
    "\n",
    "wikidata_dump_path = './my-data/latest-all.json.bz2'\n",
    "\n",
    "with bz2.open(wikidata_dump_path, 'rt', encoding='utf-8') as f:\n",
    "    count = 0\n",
    "    \n",
    "             \n",
    "    for i, line in tqdm(enumerate(f), total=1000):\n",
    "        if count == 10000:\n",
    "            break\n",
    "        try:\n",
    "            count += 1\n",
    "            # Parse JSON data from each line\n",
    "            data = json.loads(line[:-2])\n",
    "         \n",
    "            labels = data.get(\"labels\", {})\n",
    "            lang = labels.get(\"en\", {}).get(\"language\", \"\")\n",
    "            entry={}\n",
    "            entry[\"WD_id\"] = data['id']\n",
    "            entry[\"WP_id\"] = labels.get(\"en\", {}).get(\"value\", \"\")\n",
    "\n",
    "            entry[\"WD_id_URL\"] = \"http://www.wikidata.org/wiki/\"+entry[\"WD_id\"]\n",
    "            entry[\"WP_id_URL\"] = \"http://\"+lang+\".wikipedia.org/wiki/\"+entry[\"WP_id\"].replace(\" \",\"_\")\n",
    "            entry[\"dbpedia_URL\"] = \"http://dbpedia.org/resource/\"+entry[\"WP_id\"].capitalize().replace(\" \",\"_\")\n",
    "            \n",
    "            print(\"------------------\")\n",
    "            print(entry[\"WD_id_URL\"])\n",
    "            print(entry[\"WP_id_URL\"])\n",
    "            print(entry[\"dbpedia_URL\"])\n",
    "            print(\"------------------\")\n",
    "    \n",
    "        except json.decoder.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4405d-3f61-4355-ac23-5e685e372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbontracker import parser\n",
    "\n",
    "logs = parser.parse_all_logs(log_dir=\"./\")\n",
    "print(logs)\n",
    "first_log = logs[0]\n",
    "\n",
    "print(f\"Output file name: {first_log['output_filename']}\")\n",
    "print(f\"Standard file name: {first_log['standard_filename']}\")\n",
    "print(f\"Stopped early: {first_log['early_stop']}\")\n",
    "print(f\"Measured consumption: {first_log['actual']}\")\n",
    "print(f\"Predicted consumption: {first_log['pred']}\")\n",
    "print(f\"Measured GPU devices: {first_log['components']['gpu']['devices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c39bc5-a679-46c2-8406-0726fc6737cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
